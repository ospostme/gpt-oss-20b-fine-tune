{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fba48ef-4ea0-4da7-983a-387ba9c1b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m11 packages\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m11 packages\u001b[0m \u001b[2min 0.26ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1️⃣ Env Prepare Install Required Packages\n",
    "# ================================================\n",
    "\n",
    "#%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 numpy==2.3.4 pandas==2.3.3 pyarrow==22.0.0 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21721e8-6762-422b-ba4e-c252a934580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install --upgrade --force-reinstall --no-cache-dir transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e8045-aa49-4060-bca4-99336327f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install --upgrade --force-reinstall --no-cache-dir numpy==2.3.4 scipy scikit-learn pandas numba  statsmodels  joblib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef80ea-f972-43e9-b86c-b837c3fa8304",
   "metadata": {},
   "source": [
    "# SFT Config Parameters\n",
    "\n",
    "Following the main parameters which affect the fine tune speed a lot.  The main affect is the memory usage like:\n",
    "\n",
    "*How long sequence* will be seen by the model which directly decide attention matrix manapulation. Standard attention need O(N^2), state of art flash attention may bring linear complex. \n",
    "\n",
    "*Adapter rank size*  LoRA/QLoRA add two small matrices  s (A and B) to existing weight matrices (W) where  `W' = W + BA` \n",
    "\n",
    "B` (dim: `d x r`) and `A` (dim: `r x k`) have rank `r << d,k`, drastically reducing parameters.\n",
    "\n",
    "The rank size directly affected new introduced parameters, how much compute needed during the training.\n",
    "\n",
    "## sequence length \n",
    "\n",
    "✅  MAX_SEQ_LEN = 4096\n",
    "\n",
    "Each sequence is padded/truncated to 4096 tokens (context length).\n",
    "\n",
    "## Batch size\n",
    "\n",
    "Two different batches exsits. When working with an LLM training pipeline, you’ll encounter: Dataloader batch and Training batch\n",
    "\n",
    "### Dataloader batch\n",
    "\n",
    "Examples:\n",
    "\n",
    "PyTorch DataLoader(batch_size=...)\n",
    "HuggingFace Dataset.map() → grouped into batches\n",
    "Tokenizers producing “batch of size X”\n",
    "These batches are usually used for:\n",
    "- faster preprocessing\n",
    "- efficient tokenization\n",
    "- efficient I/O\n",
    "\n",
    "dataset.map(preprocess, batched=True, batch_size=1000)  # CPU-side\n",
    "\n",
    "This “batch_size=1000” has nothing to do with training batch!\n",
    "\n",
    "But they do NOT necessarily equal the batch the model trains on.\n",
    "\n",
    "### Training batch (micro-batch / per-GPU batch)\n",
    "\n",
    "This is the batch size that:\n",
    "\n",
    "- fits in GPU memory\n",
    "- goes into model(input)\n",
    "- is used in each forward/backward pass\n",
    "- is included in the global batch calculation\n",
    "\n",
    "This is what frameworks mean when they say micro_batch_size or per_device_train_batch_size.\n",
    "\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "✅ Each GPU receives 4 sequences per forward/backward pass (this is the micro-batch size).\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "✅  model will NOT update weights every micro-batch, Run 8 forward passes, Run 8 backward passes, Accumulate (add together) all 8 gradients\n",
    "Then perform 1 optimizer update, This lets you simulate a larger batch size without needing more GPU memory.\n",
    "\n",
    "So the actual global batch size is: 4 (micro) × 8 (accumulation) × 8 (gpus) = 256\n",
    "\n",
    "# System Prompt as activation key for DPO\n",
    "\n",
    "The success of the alignment relies on using an identical system prompt during both the data generation phase and production inference. This prompt is the conditional signal that activates the model's professional persona.\n",
    "\n",
    "*Include the injecting system prompt in DPO custom training data  NOT in SFT stage.*\n",
    "\n",
    "Injecting the prompt into SFT breaks things:\n",
    "\n",
    "prompt is a persona + behavioral rule set (tone, structure, domain focus). SFT’s job is to teach the model skills (reasoning, instruction following) across many styles and tasks. If you prepend the persona to every training example you will:\n",
    "\n",
    "*Overwrite diversity* — the model learns to always speak in that persona, even where it’s inappropriate (stories, casual chat, step-by-step math explanations that need verbosity).\n",
    "\n",
    "*Create optimization conflicts* — some datasets (GSM8K, OpenThoughts) require long, explicit reasoning. A “be concise” rule fights that, reducing reasoning quality.\n",
    "\n",
    "*Remove the option to “activate” the persona at inference* — you lose modularity and controllability.\n",
    "\n",
    "*Break DPO effectiveness* — DPO needs contrasts (bad vs good); if SFT already forces the good persona everywhere, DPO has nothing to teach.\n",
    "\n",
    "This is why the standard sequence used by labs is:\n",
    "\n",
    "- SFT = learn how to answer (neutral instructions)\n",
    "- DPO/RLHF = learn which answers are preferred (persona & tone),\n",
    "- Runtime system prompt = activate persona when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c877cd-c24f-468c-828f-3f7cce04d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 2️⃣ Basic Fine Tune Config\n",
    "# ================================================\n",
    "\n",
    "# Define your custom system prompt\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a highly professional, concise technical expert across modern computing domains — \n",
    "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
    "\n",
    "Your task is to:\n",
    "- Answer the user’s question using the provided CONTEXT as your primary source.\n",
    "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
    "  but clearly distinguish between context-based and general reasoning.\n",
    "\n",
    "Your responses must be:\n",
    "- Structured — use clear formatting and logical reasoning.\n",
    "- Contextual — rely only on the information available.\n",
    "- Concise — eliminate filler words while preserving precision.\n",
    "- Aligned with industry best practices — modern, reproducible, and standards-based.\n",
    "\"\"\"\n",
    "\n",
    "# Retain the VRAM safety length\n",
    "MAX_SEQ_LEN = 4096    \n",
    "# MAX_SEQ_LEN = 1024\n",
    "\n",
    "LORA_RANK = 32\n",
    "\n",
    "# per_device_train_batch_size\n",
    "DEVICE_BATCH = 32\n",
    "\n",
    "# gradient_accumulation_steps\n",
    "GRADIENT_ACCUMULATION = 32\n",
    "\n",
    "LEARNING_RATE = 1.5e-4\n",
    "\n",
    "OUTPUT_DIR = \"gpt-oss-20b-sft-qlora-adapter\"\n",
    "\n",
    "#SFT_TEST_SIZE = 100 # Using 100 rows for a quick test run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58a022-8dff-42aa-843c-5ecc091d5de3",
   "metadata": {},
   "source": [
    "# Understand LLM training \n",
    "\n",
    "## 4-Layer Universal LLM Training Stack Conceptual\n",
    "\n",
    "- LAYER 1 — Model Definition/Architecture\n",
    "  Define the model’s structure: layers, attention mechanism, feedforward (dense) blocks, embeddings, hyperparameters. Abstract\n",
    "  representation of the neural network.\n",
    "  \n",
    "- LAYER 2 — Model Loading & Preparation\n",
    "  Load the model parameters into memory and prepare them for training. Includes choosing which parts are trainable, adjusting\n",
    "  precision, and integrating adapters if needed.\n",
    "\n",
    "  \n",
    "- LAYER 3 — Training Loop\n",
    "  Execute optimization: forward pass → compute loss → backward pass → optimizer step → update parameters. Handle batching,\n",
    "  gradient accumulation, evaluation, and logging.\n",
    "\n",
    "  \n",
    "- LAYER 4 — System & Distributed Backend\n",
    "\n",
    "  Efficiently manage hardware and scaling: memory optimization, multi-GPU/multi-node coordination, data /tensor/pipeline parallelism,\n",
    "  mixed precision, and offloading if necessary.\n",
    "\n",
    "## 4-Layer Universal LLM Training Stack Practical\n",
    "\n",
    "> Layer 2–4 are highly framework-dependent, but the conceptual responsibilities remain the same.\n",
    "\n",
    "- LAYER 1 — Model Definition/Architecture\n",
    "  - Choose model type: Transformer, RWKV, Mamba\n",
    "  - Decide hyperparameters: layers, vocab size, hidden size,attention structure, attention heads, context length, feedforward design\n",
    "\n",
    "  - Ensure the architecture matches the intended task (e.g., GPT-style for text generation)\n",
    "  - For research, consider memory efficiency vs expressivity\n",
    "\n",
    "  \n",
    "- LAYER 2 — Model Loading & Preparation\n",
    "  - Load pretrained weights (HF Transformers, Unsloth, raw PyTorch)\n",
    "  - Apply LoRA / adapters for parameter-efficient tuning\n",
    "  - Set dtype / quantization (fp16, bf16, 4-bit, 8-bit)\n",
    "  - Freeze layers if using adapters\n",
    "\n",
    "  - Ensure checkpoint matches architecture\n",
    "  - Choose precision & device mapping based on GPU memory\n",
    "  - Decide which parameters are trainable now vs later\n",
    "  \n",
    "\n",
    "- LAYER 3 — Training Loop\n",
    "  - Use Trainer frameworks: HF Trainer, TRL (for RLHF / LoRA), Lightning, or raw PyTorch loops\n",
    "  - Implement forward → loss → backward → optimizer step\n",
    "  - Handle gradient accumulation, logging, evaluation\n",
    "    \n",
    "  - Choose trainer based on flexibility vs simplicity\n",
    "  - Use mixed precision and gradient checkpointing if memory-limited\n",
    "  - For RLHF, specialized trainers like TRL or trlx are recommended\n",
    "\n",
    " \n",
    "  \n",
    "- LAYER 4 — System & Distributed Backend\n",
    "  - Select framework for scaling: Accelerate, FSDP, DeepSpeed, Colossal-AI\n",
    "  - Configure memory optimization: ZeRO, offloading, sharding\n",
    "  - Choose parallelism strategy: data, tensor, pipeline\n",
    "\n",
    "  - Handles scaling and hardware orchestration, makes training efficient at large scale.\n",
    "  - Start with single-GPU / small scale before multi-GPU\n",
    "  - Understand how model weights are partitioned for large-scale training\n",
    "  - Plan for checkpoints and resuming training across devices\n",
    "  - Choice of framework depends on model size and hardware:\n",
    "      - Single GPU / small model: HF Accelerate or Lightning\n",
    "      - Medium multi-GPU model: FSDP / DeepSpeed Stage 1-2\n",
    "      - Huge multi-node model: DeepSpeed Stage 3, Colossal-AI 3D parallelism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b42e8d-333c-405f-b142-bb89f0acaf32",
   "metadata": {},
   "source": [
    "# Raw pytorch Code snippet\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1: Model Definition\n",
    "# -------------------------------\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size=1000, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 2: Model Loading & Preparation\n",
    "# -------------------------------\n",
    "model = SimpleDecoder()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy dataset\n",
    "data = torch.randint(0, 1000, (64, 20))\n",
    "target = torch.randint(0, 1000, (64, 20))\n",
    "dataset = TensorDataset(data, target)\n",
    "loader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 3: Training Loop\n",
    "# -------------------------------\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4: System / Distributed Backend\n",
    "# -------------------------------\n",
    "# Single GPU here; for multi-GPU, wrap with torch.nn.DataParallel or torch.distributed\n",
    "# Example:\n",
    "# model = nn.DataParallel(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04917e-d802-46c1-8397-6b444ec23580",
   "metadata": {},
   "source": [
    "# Hugging Face Transformer code Snippet\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1: Model Definition\n",
    "# -------------------------------\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 2: Model Loading & Preparation\n",
    "# -------------------------------\n",
    "model = model.to(\"cuda\")\n",
    "# Optionally, freeze layers for fine-tuning\n",
    "for param in model.transformer.h[:6].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 3: Training Loop (HF Trainer)\n",
    "# -------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4: System / Distributed Backend\n",
    "# -------------------------------\n",
    "# HF Trainer integrates Accelerate for multi-GPU automatically\n",
    "# Example: set environment variable CUDA_VISIBLE_DEVICES=0,1,2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b55bc-810b-4b42-bf66-4385396dc2a4",
   "metadata": {},
   "source": [
    "# Unsloth Code Snippet\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1: Model Definition\n",
    "# -------------------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 2: Model Loading & Preparation\n",
    "# -------------------------------\n",
    "# Prepare LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# Dummy dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "loader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 3: Training Loop\n",
    "# -------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4: System / Distributed Backend\n",
    "# -------------------------------\n",
    "# For larger models, integrate with DeepSpeed / FSDP\n",
    "# Unsloth / PEFT is memory efficient due to LoRA adapters\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d530fa9e-527c-4ccb-a015-a0a04c333337",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth cannot find any torch accelerator? You need a GPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 3️⃣  Load FastLanguageModel + Tokenizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ================================================\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, is_bfloat16_supported\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth/__init__.py:80\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth_zoo\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth_zoo/__init__.py:136\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m logging, torchao_logger, HideLoggingMessage\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Get device types and other variables\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    137\u001b[0m     is_hip,\n\u001b[1;32m    138\u001b[0m     get_device_type,\n\u001b[1;32m    139\u001b[0m     DEVICE_TYPE,\n\u001b[1;32m    140\u001b[0m     DEVICE_TYPE_TORCH,\n\u001b[1;32m    141\u001b[0m     DEVICE_COUNT,\n\u001b[1;32m    142\u001b[0m     ALLOW_PREQUANTIZED_MODELS,\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Torch 2.9 removed PYTORCH_HIP_ALLOC_CONF and PYTORCH_CUDA_ALLOC_CONF\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m major_torch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m minor_torch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth_zoo/device_type.py:56\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth currently only works on NVIDIA, AMD and Intel GPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# HIP fails for autocast and other torch functions. Use CUDA instead\u001b[39;00m\n\u001b[1;32m     58\u001b[0m DEVICE_TYPE_TORCH \u001b[38;5;241m=\u001b[39m DEVICE_TYPE\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth_zoo/device_type.py:46\u001b[0m, in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth cannot find any torch accelerator? You need a GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m     accelerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(torch\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mcurrent_accelerator())\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accelerator \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth cannot find any torch accelerator? You need a GPU."
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3️⃣  Load FastLanguageModel + Tokenizer\n",
    "# ================================================\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "print(is_bfloat16_supported())\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "dtype=None\n",
    "\n",
    "# Unsloth recommended: returns both model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    \n",
    "    # Hopper GPUs BF16 optimization, None for auto detection\n",
    "    dtype=dtype, \n",
    "    \n",
    "    # The model’s internal attention window – i.e. how many tokens it can actually process at once during forward/backward passes\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "\n",
    "    # 4 bit quantization to reduce memory\n",
    "    load_in_4bit = True,\n",
    "    \n",
    "    # False means with QLoRA/LoRA\n",
    "    # [NEW!] unsloth have full finetuning now!\n",
    "    full_finetuning = False,\n",
    "    \n",
    "    # token = \"hf_...\",              # use one if using gated models\n",
    ")\n",
    "\n",
    "print(\"\\n✅ FastLanguageModel + tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddefad95-8259-4d39-aeee-11f715124b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total samples: 29692\n",
      "\n",
      "✅ Inspect the first entry of the data:\n",
      "\n",
      " {'instruction': 'Compile a visually appealing list of at least ten distinctive dips and gravies that can be paired with a range of dishes prepared on your electric griddle. Make sure to include both sweet and savory options, as well as dips that cater to different dietary restrictions such as vegan, gluten-free, or low-fat. Additionally, provide brief descriptions of each dip or gravy highlighting its key ingredients, flavor profile, and suggested griddle dishes to accompany it.', 'response': '1. Caramelized Onion and Garlic Dip - This savory dip pairs perfectly with breakfast dishes such as eggs, bacon, and pancakes or can be used as a topping for burgers and sandwiches. It is made with caramelized onions, garlic, cream cheese, and sour cream, and has a sweet and tangy flavor.\\n \\n2. Spicy Avocado Dip - This vegan option is perfect for those looking for a healthy dip option. Made with ripe avocados, jalapenos, lime juice, and cilantro, it is a great accompaniment for veggie fajitas or as a topping for grilled chicken or fish.\\n\\n3. Sweet and Sour Dipping Sauce - This tangy sauce is great for dipping crispy chicken tenders or fried shrimp. Made with ketchup, brown sugar, apple cider vinegar, soy sauce, and garlic, it has a sweet and slightly sour flavor that complements fried foods well.\\n\\n4. Bacon and Cheddar Dip - This savory dip pairs well with baked potatoes and grilled meats. Made with chopped bacon, cheddar cheese, sour cream, and Worcestershire sauce, it has a rich and creamy texture with a smoky flavor.\\n\\n5. Chipotle Mayo Dip - This spicy dip is perfect for dipping sweet potato fries or as a topping for burgers or sandwiches. Made with mayonnaise, chipotle chilies, lime juice, and garlic, it has a smoky and spicy flavor.\\n\\n6. Cheesy Queso Dip - This gluten-free dip is great for dipping tortilla chips or as a topping for nachos. Made with Velveeta cheese, diced tomatoes, and green chilies, it has a creamy and cheesy flavor with a slight kick.\\n\\n7. Low-Fat Ranch Dip - This healthy dip is made with Greek yogurt, garlic, and fresh herbs, and is a great pairing for grilled veggies, chicken, or fish.\\n\\n8. Roasted Red Pepper Dip - This vegan dip is a great pairing for grilled vegetables, flatbreads, or grilled chicken. Made with roasted red peppers, garlic, and olive oil, it has a smoky and slightly sweet flavor.\\n\\n9. Cilantro Lime Aioli - This low-fat dip is great for dipping crunchy onion rings or as a topping for grilled chicken or fish. Made with Greek yogurt, lime juice, and fresh cilantro, it has a tangy and slightly spicy flavor.\\n\\n10. Chocolate Ganache - This sweet dip is perfect for dipping fresh fruit or as a topping for pancakes, waffles, or French toast. Made with dark chocolate and heavy cream, it has a rich and decadent chocolate flavor.', 'source': 'ultrachat'}\n",
      "\n",
      "✅ Train samples: 28207\n",
      "\n",
      "✅ Validation samples: 1485\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-10\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains — \n",
      "including software architecture, cloud inf\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-10\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains — \n",
      "including software architecture, cloud inf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4️⃣ Load Dataset, Split Dataset Train/Validation\n",
    "# ================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset_path = \"./train_sft_final.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files={\"train\": dataset_path})\n",
    "\n",
    "full_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "# for small dataset smoke test on T4 \n",
    "# full_dataset = full_dataset.select(range(100))\n",
    "\n",
    "print(f\"\\n✅ Total samples: {len(full_dataset)}\")\n",
    "print(f\"\\n✅ Inspect the first entry of the data:\\n\\n {full_dataset[0]}\")\n",
    "\n",
    "\n",
    "# 95% train, 5% validation\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"\\n✅ Train samples: {len(train_dataset)}\")\n",
    "print(f\"\\n✅ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "def inspect_message_with_chat_template(example, tokenizer):\n",
    "    messages = [\n",
    "        #{\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "    ]\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n✅ Inspect data after apply chat template\\n\")\n",
    "    print(formatted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "inspect_message_with_chat_template(train_dataset[0], tokenizer)\n",
    "inspect_message_with_chat_template(val_dataset[0], tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a44421-1d88-4430-bd64-10a8fde72997",
   "metadata": {},
   "source": [
    "#  How translate the message into tokens and fed to the model, Why tokenized data will be failed ???\n",
    "\n",
    "## Principle: Keep tokenization responsibilities consistent\n",
    "If you want the trainer to handle packing, batching, padding-free training etc — give it raw \"text\" strings and set dataset_text_field=\"text\" in your SFTConfig. This is the easiest path and avoids the shape errors you saw.\n",
    "\n",
    "If you pre-tokenize, return Python lists (list of input_ids lists and attention masks) and do not return PyTorch tensors from map. Also decide whether you pad or not: if you plan to use packing, do not pad during preprocessing — leave padding to trainer.\n",
    "\n",
    "## Option A — Let trainer tokenize (HIGHLY RECOMMENDED)\n",
    "Why this is safe: TRL/Unsloth will call the tokenizer inside the data collator in a consistent, batch-wise manner, and will manage packing/padding in the way expected by the model/attention implementation. No tensors leak into the dataset; no shape surprises.\n",
    "\n",
    "## Option B — Pre-tokenize correctly (ADVANCED)\n",
    "If you must pre-tokenize (e.g., offline processing, caching), do it this way:\n",
    "\n",
    "Use tokenize=False on apply_chat_template to get strings.\n",
    "\n",
    "*Tokenize the batch in a vectorized call: tokenizer(texts, truncation=True, padding=False, return_attention_mask=True, return_tensors=None) — return_tensors must be None so HF dataset gets Python lists.*\n",
    "\n",
    "*Do not use return_tensors=\"pt\" in map.*\n",
    "\n",
    "If you plan to use packing later, set padding=False and truncation=True (or False if you filtered earlier). Trainer can pack them.\n",
    "\n",
    "## Debugging checklist (TBD)\n",
    "\n",
    "### After map, inspect dataset sample types:\n",
    "\n",
    "```python\n",
    "sample = train_dataset[0]\n",
    "print(type(sample[\"input_ids\"]), isinstance(sample[\"input_ids\"][0], int), len(sample[\"input_ids\"]))\n",
    "print(type(sample[\"attention_mask\"]), isinstance(sample[\"attention_mask\"][0], int))\n",
    "```\n",
    "You should see list and first element is int (not torch.Tensor, not list-of-list nested weirdness).\n",
    "\n",
    "### Inspect collated batch shape used by trainer (simulate a collate):\n",
    "```python\n",
    "from transformers import default_data_collator\n",
    "batch = [train_dataset[i] for i in range(4)]\n",
    "collated = default_data_collator(batch)\n",
    "print({k: (type(v), getattr(v, \"shape\", None)) for k,v in collated.items()})\n",
    "```\n",
    "This should show input_ids/attention_mask as Torch tensors of shape (batch, seq_len).\n",
    "\n",
    "### Quick model forward sanity check (very small test):\n",
    "```python\n",
    "batch = default_data_collator([train_dataset[0], train_dataset[1]])\n",
    "# move to device if needed\n",
    "out = model(**{k: torch.tensor(v).to(model.device) for k,v in batch.items() if k in (\"input_ids\",\"attention_mask\")})\n",
    "print(\"last hidden:\", getattr(out, \"last_hidden_state\", None) and out.last_hidden_state.shape)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d65286-52ab-46df-bba5-6969ddef7549",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth cannot find any torch accelerator? You need a GPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     61\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     62\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     63\u001b[0m             [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m instr, resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     72\u001b[0m     ]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m : texts, }\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_sharegpt\n\u001b[1;32m     79\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_fn, batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_fn, batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth/__init__.py:80\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth_zoo\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth_zoo/__init__.py:136\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m logging, torchao_logger, HideLoggingMessage\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Get device types and other variables\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    137\u001b[0m     is_hip,\n\u001b[1;32m    138\u001b[0m     get_device_type,\n\u001b[1;32m    139\u001b[0m     DEVICE_TYPE,\n\u001b[1;32m    140\u001b[0m     DEVICE_TYPE_TORCH,\n\u001b[1;32m    141\u001b[0m     DEVICE_COUNT,\n\u001b[1;32m    142\u001b[0m     ALLOW_PREQUANTIZED_MODELS,\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Torch 2.9 removed PYTORCH_HIP_ALLOC_CONF and PYTORCH_CUDA_ALLOC_CONF\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m major_torch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m minor_torch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth_zoo/device_type.py:56\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth currently only works on NVIDIA, AMD and Intel GPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# HIP fails for autocast and other torch functions. Use CUDA instead\u001b[39;00m\n\u001b[1;32m     58\u001b[0m DEVICE_TYPE_TORCH \u001b[38;5;241m=\u001b[39m DEVICE_TYPE\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/unsloth_zoo/device_type.py:46\u001b[0m, in \u001b[0;36mget_device_type\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth cannot find any torch accelerator? You need a GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m     accelerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(torch\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mcurrent_accelerator())\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accelerator \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsloth cannot find any torch accelerator? You need a GPU."
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 5️⃣  Tokenize Datasets with chat template applyied\n",
    "# ================================================\n",
    "\n",
    "def tokenize_fn_old(example, tokenizer):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "\n",
    "    tokenized_chat_wrapped = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "    )\n",
    "\n",
    "    #return tokenized_chat_wrapped\n",
    "    # Return a dictionary so Hugging Face can build an Arrow table\n",
    "    return {\"input_ids\": tokenized_chat_wrapped, \n",
    "            \"attention_mask\": [1] * len(tokenized_chat_wrapped)}\n",
    "\n",
    "\n",
    "def tokenize_fn_problem(batch, tokenizer):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                # {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    # vectorized tokenizer call\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        #truncation=True,\n",
    "        #padding=\"max_length\",   # or padding=False to let Trainer handle dynamic padding\n",
    "        #padding_side = \"right\",\n",
    "        truncation=False,  # <--- CHANGED: Set to False\n",
    "        padding=False,     # <--- CHANGED: Set to False\n",
    "        #max_length=MAX_SEQ_LEN,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None,    # keep Python lists, HF Dataset friendly\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                # {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched = True)\n",
    "\n",
    "val_dataset = val_dataset.map(tokenize_fn, batched = True)\n",
    "\n",
    "# Apply the formatting using a lambda function to pass the tokenizer\n",
    "# map() can only pass the dataset batch, not extra arguments.\n",
    "#train_dataset = train_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=train_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT train\"\n",
    "#)\n",
    "#val_dataset = val_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=val_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT validation\"\n",
    "#)   \n",
    "\n",
    "print(\"\\n✅Tokenization complete\")\n",
    "\n",
    "#sample = val_dataset[0]\n",
    "#print(\"input_ids (first 1 tokens):\", sample[\"input_ids\"][:1])\n",
    "#print(\"attention_mask (first 1 tokens):\", sample[\"attention_mask\"][:1])\n",
    "val_dataset\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120f7d69-0674-4045-bbc6-7dc6cfe597ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "--- 1. Model and Adapter Check ---\n",
      "\n",
      "✅Base Model Parameters: 20930682432\n",
      " (Trainable: (15925248, 20930682432))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 6️⃣   PEFT settting\n",
    "# ================================================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 2*LORA_RANK,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "print(\"--- 1. Model and Adapter Check ---\")\n",
    "# This print statement now shows the doubled number of trainable parameters\n",
    "print(f\"\\n✅Base Model Parameters: {model.num_parameters()}\\n (Trainable: {model.get_nb_trainable_parameters()})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8d46b-6784-4d85-b419-e5435c05fa60",
   "metadata": {},
   "source": [
    "With Following unsloth trainer, make it run. **BUT, the batch size is not right as expected.**\n",
    "\n",
    "==((====))== Unsloth - 2x faster free finetuning | Num GPUs used = 1 \\\\ /| Num examples = 28,207 | Num Epochs = 3 | Total steps = 10,578 O^O/ \\_/ \\ Batch size per device = 4 | Gradient accumulation steps = 2 \\ / Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8 \"-____-\" Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained) Unsloth: Will smartly offload gradients to save VRAM!\n",
    "\n",
    "I set per_device_train_batch_size = 64 in your trainer_args, but Unsloth is still only using a batch of 4 per device.\n",
    "\n",
    "Batch size per device = 4 | Gradient accumulation steps = 2\n",
    "Total batch size (4 x 2 x 1) = 8\n",
    "\n",
    "```python\n",
    "from unsloth.trainer import SFTTrainer\n",
    "from unsloth.trainer import SFTTrainingArguments\n",
    "\n",
    "# set attention implementation **after loading**\n",
    "model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "# Create SFTTrainingArguments object\n",
    "training_args = SFTTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    per_device_train_batch_size=64,   # micro-batch\n",
    "    gradient_accumulation_steps=4,    # effective batch = 256\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    dataloader_num_workers=12,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=model.peft_config\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644740a-ca52-43ad-9ce4-a33215aa15ba",
   "metadata": {},
   "source": [
    "# explain Why from ChatGPT (NOT Verified)\n",
    "\n",
    "Short diagnosis — why “Batch size per device = 4” even though you set 64\n",
    "\n",
    "Unsloth will override/adjust the micro-batch in certain situations:\n",
    "\n",
    "It has internal safety/defaults (common default micro-batch = 1–4) and may clamp your requested micro-batch to something that fits the runtime/packaging/num_generations constraints. Unsloth docs and issues recommend small defaults (e.g. per_device_train_batch_size = 2 often recommended). \n",
    "Unsloth Docs\n",
    "+1\n",
    "\n",
    "per_device_train_batch_size might not actually be passed correctly into Unsloth’s trainer if you pass args with wrong calling style. The SFTTrainer wrapper expects an args object/dict (passed as args=), not **trainer_args into SFTTrainer init (that will raise unexpected-keyword errors). If you accidentally passed trainer args incorrectly earlier, the trainer used default values. (You already saw errors like unexpected output_dir when using **trainer_args.) \n",
    "GitHub\n",
    "\n",
    "num_generations constraint: Unsloth has code that forces per_device_train_batch_size to be a multiple of num_generations (see issue/commit wording, it will change the batch to num_generations if it doesn’t match). If your per_device_train_batch_size is not aligned it may change it to some lower multiple. (I found this referenced in Unsloth issue logs.) \n",
    "GitHub\n",
    "\n",
    "Memory / quantization / PEFT interactions: depending on full_finetuning vs QLoRA vs LoRA, bf16/fp16 settings, and use_gradient_checkpointing, the trainer may reduce micro-batch size to ensure no OOM. Unsloth prints “Will smartly offload gradients to save VRAM!”, meaning it’s actively making tradeoffs. \n",
    "Unsloth Docs\n",
    "\n",
    "Net: either your args were not passed in the correct API shape, or Unsloth intentionally lowered the micro-batch for safety/compatibility (or both).\n",
    "\n",
    "\n",
    "Evidence & sources\n",
    "\n",
    "Unsloth Fine-tuning guide / defaults: per_device_train_batch_size recommended small, use grad accumulation. \n",
    "Unsloth Docs\n",
    "\n",
    "GitHub issues referencing per_device_train_batch_size being changed by Unsloth/compat constraints and the num_generations behavior. \n",
    "GitHub\n",
    "+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ce1d8-65ef-46d3-9946-330b0b91a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[trl.trainer.sft_trainer|WARNING]Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "[trl.trainer.sft_trainer|WARNING]You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, model, args=None, data_collator=None, train_dataset=None, eval_dataset=None, processing_class=None, compute_loss_func=None, compute_metrics=None, callbacks=None, optimizer_cls_and_kwargs=None, preprocess_logits_for_metrics=None, peft_config=None, formatting_func=None, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 8,312 | Num Epochs = 3 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 32\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 32 x 1) = 1,024\n",
      " \"-____-\"     Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 7️⃣ Training Arguments\n",
    "# ================================================\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "# set attention implementation **after loading**\n",
    "#model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # TRL-Specific Args\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    packing=True,                  # 🚀 CRITICAL for Unsloth/Flash Attention efficiency\n",
    "    dataset_text_field=\"text\",     # The column containing the formatted data\n",
    "\n",
    "    # Core Training Args (Batching, Learning)\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=32, # Effective batch = 8\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optim=\"paged_adamw_32bit\",     # Recommended optimizer for QLoRA\n",
    "\n",
    "    # Logging and Saving\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Precision (auto-detects bfloat16 if hardware supports it)\n",
    "    bf16=is_bfloat16_supported(), \n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    \n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    \n",
    "    eval_dataset=val_dataset,\n",
    "    \n",
    "    peft_config=None,            # LoRA already applied\n",
    "\n",
    "    formatting_func=None         # Optional: custom formatting\n",
    ")\n",
    "\n",
    "import inspect\n",
    "print(inspect.signature(SFTTrainer.__init__))\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cde5a-14da-44c9-8f38-07ab453c1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 8️⃣ Save Fine-Tuned Model\n",
    "# ================================================\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"SFT model with validation saved to {output_dir}\")\n",
    "model.push_to_hub(\"ospost/gpt-oss-20b-sft-qlora-adapter\", token = \"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\") # Save to HF\n",
    "\n",
    "\n",
    "1️⃣\n",
    "2️⃣\n",
    "3️⃣\n",
    "4️⃣\n",
    "5️⃣\n",
    "6️⃣\n",
    "7️⃣\n",
    "8️⃣\n",
    "9️⃣\n",
    "🔟"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
