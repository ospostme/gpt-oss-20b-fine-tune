{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fba48ef-4ea0-4da7-983a-387ba9c1b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m11 packages\u001b[0m \u001b[2min 162ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m11 packages\u001b[0m \u001b[2min 0.78ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1Ô∏è‚É£ Env Prepare Install Required Packages\n",
    "# ================================================\n",
    "\n",
    "#%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 numpy==2.3.4 pandas==2.3.3 pyarrow==22.0.0 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21721e8-6762-422b-ba4e-c252a934580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install --upgrade --force-reinstall --no-cache-dir transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6e8045-aa49-4060-bca4-99336327f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install --upgrade --force-reinstall --no-cache-dir numpy==2.3.4 scipy scikit-learn pandas numba  statsmodels  joblib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef80ea-f972-43e9-b86c-b837c3fa8304",
   "metadata": {},
   "source": [
    "# SFT Config Parameters\n",
    "\n",
    "Following the main parameters which affect the fine tune speed a lot.  The main affect is the memory usage like:\n",
    "\n",
    "*How long sequence* will be seen by the model which directly decide attention matrix manapulation. Standard attention need O(N^2), state of art flash attention may bring linear complex. \n",
    "\n",
    "*Adapter rank size*  LoRA/QLoRA add two small matrices  s (A and B) to existing weight matrices (W) where  `W' = W + BA` \n",
    "\n",
    "B` (dim: `d x r`) and `A` (dim: `r x k`) have rank `r << d,k`, drastically reducing parameters.\n",
    "\n",
    "The rank size directly affected new introduced parameters, how much compute needed during the training.\n",
    "\n",
    "## sequence length \n",
    "\n",
    "‚úÖ  MAX_SEQ_LEN = 4096\n",
    "\n",
    "Each sequence is padded/truncated to 4096 tokens (context length).\n",
    "\n",
    "## Batch size\n",
    "\n",
    "Two different batches exsits. When working with an LLM training pipeline, you‚Äôll encounter: Dataloader batch and Training batch\n",
    "\n",
    "### Dataloader batch\n",
    "\n",
    "Examples:\n",
    "\n",
    "PyTorch DataLoader(batch_size=...)\n",
    "HuggingFace Dataset.map() ‚Üí grouped into batches\n",
    "Tokenizers producing ‚Äúbatch of size X‚Äù\n",
    "These batches are usually used for:\n",
    "- faster preprocessing\n",
    "- efficient tokenization\n",
    "- efficient I/O\n",
    "\n",
    "dataset.map(preprocess, batched=True, batch_size=1000)  # CPU-side\n",
    "\n",
    "This ‚Äúbatch_size=1000‚Äù has nothing to do with training batch!\n",
    "\n",
    "But they do NOT necessarily equal the batch the model trains on.\n",
    "\n",
    "### Training batch (micro-batch / per-GPU batch)\n",
    "\n",
    "This is the batch size that:\n",
    "\n",
    "- fits in GPU memory\n",
    "- goes into model(input)\n",
    "- is used in each forward/backward pass\n",
    "- is included in the global batch calculation\n",
    "\n",
    "This is what frameworks mean when they say micro_batch_size or per_device_train_batch_size.\n",
    "\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "‚úÖ Each GPU receives 4 sequences per forward/backward pass (this is the micro-batch size).\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "‚úÖ  model will NOT update weights every micro-batch, Run 8 forward passes, Run 8 backward passes, Accumulate (add together) all 8 gradients\n",
    "Then perform 1 optimizer update, This lets you simulate a larger batch size without needing more GPU memory.\n",
    "\n",
    "So the actual global batch size is: 4 (micro) √ó 8 (accumulation) √ó 8 (gpus) = 256\n",
    "\n",
    "# System Prompt as activation key for DPO\n",
    "\n",
    "The success of the alignment relies on using an identical system prompt during both the data generation phase and production inference. This prompt is the conditional signal that activates the model's professional persona.\n",
    "\n",
    "*Include the injecting system prompt in DPO custom training data  NOT in SFT stage.*\n",
    "\n",
    "Injecting the prompt into SFT breaks things:\n",
    "\n",
    "prompt is a persona + behavioral rule set (tone, structure, domain focus). SFT‚Äôs job is to teach the model skills (reasoning, instruction following) across many styles and tasks. If you prepend the persona to every training example you will:\n",
    "\n",
    "*Overwrite diversity* ‚Äî the model learns to always speak in that persona, even where it‚Äôs inappropriate (stories, casual chat, step-by-step math explanations that need verbosity).\n",
    "\n",
    "*Create optimization conflicts* ‚Äî some datasets (GSM8K, OpenThoughts) require long, explicit reasoning. A ‚Äúbe concise‚Äù rule fights that, reducing reasoning quality.\n",
    "\n",
    "*Remove the option to ‚Äúactivate‚Äù the persona at inference* ‚Äî you lose modularity and controllability.\n",
    "\n",
    "*Break DPO effectiveness* ‚Äî DPO needs contrasts (bad vs good); if SFT already forces the good persona everywhere, DPO has nothing to teach.\n",
    "\n",
    "This is why the standard sequence used by labs is:\n",
    "\n",
    "- SFT = learn how to answer (neutral instructions)\n",
    "- DPO/RLHF = learn which answers are preferred (persona & tone),\n",
    "- Runtime system prompt = activate persona when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c877cd-c24f-468c-828f-3f7cce04d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 2Ô∏è‚É£ Basic Fine Tune Config\n",
    "# ================================================\n",
    "\n",
    "# Define your custom system prompt\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
    "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
    "\n",
    "Your task is to:\n",
    "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
    "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
    "  but clearly distinguish between context-based and general reasoning.\n",
    "\n",
    "Your responses must be:\n",
    "- Structured ‚Äî use clear formatting and logical reasoning.\n",
    "- Contextual ‚Äî rely only on the information available.\n",
    "- Concise ‚Äî eliminate filler words while preserving precision.\n",
    "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
    "\"\"\"\n",
    "\n",
    "# Retain the VRAM safety length\n",
    "MAX_SEQ_LEN = 4096    \n",
    "# MAX_SEQ_LEN = 1024\n",
    "\n",
    "LORA_RANK = 32\n",
    "\n",
    "# per_device_train_batch_size\n",
    "DEVICE_BATCH = 32\n",
    "\n",
    "# gradient_accumulation_steps\n",
    "GRADIENT_ACCUMULATION = 32\n",
    "\n",
    "LEARNING_RATE = 1.5e-4\n",
    "\n",
    "OUTPUT_DIR = \"gpt-oss-20b-sft-qlora-adapter\"\n",
    "\n",
    "#SFT_TEST_SIZE = 100 # Using 100 rows for a quick test run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58a022-8dff-42aa-843c-5ecc091d5de3",
   "metadata": {},
   "source": [
    "# Understand LLM training \n",
    "\n",
    "## 4-Layer Universal LLM Training Stack Conceptual\n",
    "\n",
    "- LAYER 1 ‚Äî Model Definition/Architecture\n",
    "  Define the model‚Äôs structure: layers, attention mechanism, feedforward (dense) blocks, embeddings, hyperparameters. Abstract\n",
    "  representation of the neural network.\n",
    "  \n",
    "- LAYER 2 ‚Äî Model Loading & Preparation\n",
    "  Load the model parameters into memory and prepare them for training. Includes choosing which parts are trainable, adjusting\n",
    "  precision, and integrating adapters if needed.\n",
    "\n",
    "  \n",
    "- LAYER 3 ‚Äî Training Loop\n",
    "  Execute optimization: forward pass ‚Üí compute loss ‚Üí backward pass ‚Üí optimizer step ‚Üí update parameters. Handle batching,\n",
    "  gradient accumulation, evaluation, and logging.\n",
    "\n",
    "  \n",
    "- LAYER 4 ‚Äî System & Distributed Backend\n",
    "\n",
    "  Efficiently manage hardware and scaling: memory optimization, multi-GPU/multi-node coordination, data /tensor/pipeline parallelism,\n",
    "  mixed precision, and offloading if necessary.\n",
    "\n",
    "## 4-Layer Universal LLM Training Stack Practical\n",
    "\n",
    "> Layer 2‚Äì4 are highly framework-dependent, but the conceptual responsibilities remain the same.\n",
    "\n",
    "- LAYER 1 ‚Äî Model Definition/Architecture\n",
    "  - Choose model type: Transformer, RWKV, Mamba\n",
    "  - Decide hyperparameters: layers, vocab size, hidden size,attention structure, attention heads, context length, feedforward design\n",
    "\n",
    "  - Ensure the architecture matches the intended task (e.g., GPT-style for text generation)\n",
    "  - For research, consider memory efficiency vs expressivity\n",
    "\n",
    "  \n",
    "- LAYER 2 ‚Äî Model Loading & Preparation\n",
    "  - Load pretrained weights (HF Transformers, Unsloth, raw PyTorch)\n",
    "  - Apply LoRA / adapters for parameter-efficient tuning\n",
    "  - Set dtype / quantization (fp16, bf16, 4-bit, 8-bit)\n",
    "  - Freeze layers if using adapters\n",
    "\n",
    "  - Ensure checkpoint matches architecture\n",
    "  - Choose precision & device mapping based on GPU memory\n",
    "  - Decide which parameters are trainable now vs later\n",
    "  \n",
    "\n",
    "- LAYER 3 ‚Äî Training Loop\n",
    "  - Use Trainer frameworks: HF Trainer, TRL (for RLHF / LoRA), Lightning, or raw PyTorch loops\n",
    "  - Implement forward ‚Üí loss ‚Üí backward ‚Üí optimizer step\n",
    "  - Handle gradient accumulation, logging, evaluation\n",
    "    \n",
    "  - Choose trainer based on flexibility vs simplicity\n",
    "  - Use mixed precision and gradient checkpointing if memory-limited\n",
    "  - For RLHF, specialized trainers like TRL or trlx are recommended\n",
    "\n",
    " \n",
    "  \n",
    "- LAYER 4 ‚Äî System & Distributed Backend\n",
    "  - Select framework for scaling: Accelerate, FSDP, DeepSpeed, Colossal-AI\n",
    "  - Configure memory optimization: ZeRO, offloading, sharding\n",
    "  - Choose parallelism strategy: data, tensor, pipeline\n",
    "\n",
    "  - Handles scaling and hardware orchestration, makes training efficient at large scale.\n",
    "  - Start with single-GPU / small scale before multi-GPU\n",
    "  - Understand how model weights are partitioned for large-scale training\n",
    "  - Plan for checkpoints and resuming training across devices\n",
    "  - Choice of framework depends on model size and hardware:\n",
    "      - Single GPU / small model: HF Accelerate or Lightning\n",
    "      - Medium multi-GPU model: FSDP / DeepSpeed Stage 1-2\n",
    "      - Huge multi-node model: DeepSpeed Stage 3, Colossal-AI 3D parallelism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b42e8d-333c-405f-b142-bb89f0acaf32",
   "metadata": {},
   "source": [
    "# Raw pytorch Code snippet\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1: Model Definition\n",
    "# -------------------------------\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size=1000, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 2: Model Loading & Preparation\n",
    "# -------------------------------\n",
    "model = SimpleDecoder()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy dataset\n",
    "data = torch.randint(0, 1000, (64, 20))\n",
    "target = torch.randint(0, 1000, (64, 20))\n",
    "dataset = TensorDataset(data, target)\n",
    "loader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 3: Training Loop\n",
    "# -------------------------------\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4: System / Distributed Backend\n",
    "# -------------------------------\n",
    "# Single GPU here; for multi-GPU, wrap with torch.nn.DataParallel or torch.distributed\n",
    "# Example:\n",
    "# model = nn.DataParallel(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04917e-d802-46c1-8397-6b444ec23580",
   "metadata": {},
   "source": [
    "# Hugging Face Transformer code Snippet\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1: Model Definition\n",
    "# -------------------------------\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 2: Model Loading & Preparation\n",
    "# -------------------------------\n",
    "model = model.to(\"cuda\")\n",
    "# Optionally, freeze layers for fine-tuning\n",
    "for param in model.transformer.h[:6].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 3: Training Loop (HF Trainer)\n",
    "# -------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4: System / Distributed Backend\n",
    "# -------------------------------\n",
    "# HF Trainer integrates Accelerate for multi-GPU automatically\n",
    "# Example: set environment variable CUDA_VISIBLE_DEVICES=0,1,2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b55bc-810b-4b42-bf66-4385396dc2a4",
   "metadata": {},
   "source": [
    "# Unsloth Code Snippet\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 1: Model Definition\n",
    "# -------------------------------\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 2: Model Loading & Preparation\n",
    "# -------------------------------\n",
    "# Prepare LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "# Dummy dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "loader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 3: Training Loop\n",
    "# -------------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Layer 4: System / Distributed Backend\n",
    "# -------------------------------\n",
    "# For larger models, integrate with DeepSpeed / FSDP\n",
    "# Unsloth / PEFT is memory efficient due to LoRA adapters\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d530fa9e-527c-4ccb-a015-a0a04c333337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "True\n",
      "==((====))==  Unsloth 2025.12.10: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.811 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaa6b4ea11a4831a3b37f3192522c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6177e2bdf5df479d8ecfb1a9eb0c10c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297fcf10631c48149d611bfbdedd6e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de3ce30068b47dbbb0a133f640dd2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554740ea2302491b9dd920fa3b39d228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1c627e8cda48e9be480ffe75d76fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6912bb4cf7b644d98ec7ff30d1ef4513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/165 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b84b863d844301b8787fbc26297780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72685af45224cfe98f861ce1dfa926d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbbf37b2e9b45e4b15d8258b9e603f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf53b5bd0ff4eae9c5817e2407149a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FastLanguageModel + tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 3Ô∏è‚É£  Load FastLanguageModel + Tokenizer\n",
    "# ================================================\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "print(is_bfloat16_supported())\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "dtype=None\n",
    "\n",
    "# Unsloth recommended: returns both model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    \n",
    "    # Hopper GPUs BF16 optimization, None for auto detection\n",
    "    dtype=dtype, \n",
    "    \n",
    "    # The model‚Äôs internal attention window ‚Äì i.e. how many tokens it can actually process at once during forward/backward passes\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "\n",
    "    # 4 bit quantization to reduce memory\n",
    "    load_in_4bit = True,\n",
    "    \n",
    "    # False means with QLoRA/LoRA\n",
    "    # [NEW!] unsloth have full finetuning now!\n",
    "    full_finetuning = False,\n",
    "    \n",
    "    # token = \"hf_...\",              # use one if using gated models\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ FastLanguageModel + tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddefad95-8259-4d39-aeee-11f715124b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a076cd4509664e5f882e315241cfa642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total samples: 30118\n",
      "\n",
      "‚úÖ Inspect the first entry of the data:\n",
      "\n",
      " {'instruction': 'What is the least spoken language in the world?\\nA: kindergarten class\\nB: mimesis\\nC: sign language\\nD: rhetoric\\nE: late latin', 'response': 'Okay, let\\'s try to figure out the answer to this question. The question is asking for the least spoken language in the world, and the options are A through E: kindergarten class, mimesis, sign language, rhetoric, and Late Latin. Hmm, interesting.\\n\\nFirst, I need to understand each of these options properly. Let me go through them one by one.\\n\\nStarting with option A: kindergarten class. Wait, a kindergarten class isn\\'t a language. It\\'s a place or a group of young students. Maybe this is a trick question, but I don\\'t think \"kindergarten class\" refers to any language. Unless it\\'s a mistranslation or misunderstanding. Maybe in some contexts, but I don\\'t recall any language by that name. So probably, this option is a distractor.\\n\\nOption B: mimesis. Mimesis is a term I\\'ve heard in literature or art, referring to imitation or representation. Like how art mimics life. But is that a language? No, mimesis is a concept, not a language. Unless there\\'s a specific sign language or constructed language called mimesis, but I don\\'t think so. Maybe it\\'s a red herring.\\n\\nOption C: sign language. Sign languages are actual languages used by deaf communities. They have their own grammar and structure. There are many sign languages around the world, like American Sign Language, British Sign Language, etc. But the question is about the least spoken. However, sign languages are used by millions of people globally. So even though they\\'re not \"spoken\" in the traditional sense, they are widely used. So maybe not the least spoken, depending on interpretation. But the question says \"spoken,\" which might imply vocal language, but sign languages aren\\'t spoken. So perhaps this is a trick in the wording. If \"spoken\" is literal, then sign language isn\\'t spoken at all, so technically it\\'s the least? But that might be a play on words. But I need to check other options.\\n\\nOption D: rhetoric. Rhetoric is the art of persuasive speaking or writing. It\\'s a field of study, not a language itself. So again, not a language. Unless there\\'s a language named Rhetoric, but I don\\'t think that\\'s the case. So this is probably another distractor.\\n\\nOption E: Late Latin. Latin is a classical language that evolved into Late Latin, which was used in the Late Antiquity period. Eventually, it developed into the Romance languages. But Late Latin itself is considered extinct or no longer spoken as a native language. However, Ecclesiastical Latin is used by the Catholic Church. But in terms of native speakers, there are none. So Late Latin would be a dead language. If that\\'s the case, then it\\'s not spoken at all except maybe in limited contexts. But how does that compare to the other options?\\n\\nWait, but let\\'s think again. The question is about the \"least spoken\" language. If a language is extinct or has no native speakers, then it\\'s not spoken, making it the least. But among the options, which is a real language that\\'s not spoken? Let\\'s confirm:\\n\\nA: Not a language.\\n\\nB: Not a language.\\n\\nC: Sign language is a language but not spoken; it\\'s signed. However, many people use it. So if the question is about spoken (vocal) languages, sign language wouldn\\'t count, but the question might be using \"spoken\" more generally. But the options include it as a possible answer.\\n\\nD: Not a language.\\n\\nE: Late Latin is a historical form of Latin, not spoken natively anymore. So if the question is looking for a language with the fewest speakers, Late Latin might be the answer because it\\'s extinct. But how does that compare to sign language? For example, if a language has zero speakers, that\\'s the least. But sign languages have millions of users. So if Late Latin is considered a dead language, then it\\'s not spoken, making it the least. But maybe there\\'s a nuance here.\\n\\nAlternatively, could \"kindergarten class\" be a language? Maybe a constructed language or a joke answer. But I don\\'t think so. Similarly, \"rhetoric\" and \"mimesis\" are concepts, not languages. So the real contenders are C and E.\\n\\nBut the question is about the least spoken. If \"spoken\" refers to verbal communication, then sign language isn\\'t spoken, so technically it\\'s not spoken at all, making it the least. But that\\'s a bit of a wordplay. Alternatively, if the question is which has the fewest speakers, then Late Latin would have zero or very few (like scholars who can read/write it but don\\'t use it daily). So in that case, E would be the answer.\\n\\nWait, but some people do study Late Latin and might use it in academic or religious contexts. But as a living, daily spoken language, it\\'s extinct. So compared to sign language, which has active users, Late Latin has none. Therefore, the least spoken language among the options would be Late Latin. So answer E.\\n\\nBut I need to check if any other options are actual languages. Let me verify each term again.\\n\\n- Kindergarten class: Not a language.\\n\\n- Mimesis: A concept, not a language.\\n\\n- Sign language: A legitimate category of languages, but not spoken.\\n\\n- Rhetoric: A discipline, not a language.\\n\\n- Late Latin: A historical stage of Latin, no native speakers.\\n\\nTherefore, the answer is E: Late Latin, as it\\'s no longer spoken, making it the least spoken, whereas sign languages are actively used, even if not vocally.\\n\\n\\n\\n\\n\\nThe question asks for the least spoken language among the given options. Here‚Äôs the systematic breakdown:\\n\\n1. **Option A: Kindergarten Class**  \\n   Not a language. It refers to a group of students or an educational setting, not a linguistic system. **Eliminated.**\\n\\n2. **Option B: Mimesis**  \\n   A concept in art and literature (imitation of reality), not a language. **Eliminated.**\\n\\n3. **Option C: Sign Language**  \\n   A legitimate category of languages (e.g., ASL, BSL) used by deaf communities. While not \"spoken\" vocally, they are actively used by millions globally. **Not the least spoken.**\\n\\n4. **Option D: Rhetoric**  \\n   A field of study focused on persuasive communication, not a language. **Eliminated.**\\n\\n5. **Option E: Late Latin**  \\n   A historical form of Latin from Late Antiquity. It evolved into Romance languages and has no native speakers today. Though studied academically or used ceremonially (e.g., in the Catholic Church), it is extinct as a living, daily spoken language. **Zero active native speakers.**\\n\\n**Conclusion:**  \\nThe term \"spoken\" here likely refers to vocal communication. However, even if interpreted broadly, Late Latin has no living speakers, while sign languages are actively used. Thus, **Late Latin (E)** is the least spoken language among the options.\\n\\n**Answer:** E: Late Latin', 'source': 'OpenThoughts'}\n",
      "\n",
      "‚úÖ Train samples: 28612\n",
      "\n",
      "‚úÖ Validation samples: 1506\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2026-01-03\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Question: can a senior in high school play junior varsity\n",
      "\n",
      "Context:\n",
      "Members of a junior varsity team are underclassmen determined by the coaching staff to\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2026-01-03\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>user<|message|>Question: Small mammals have many adaptations that keep them warm in winter. Which would not help conserve heat?\n",
      "\n",
      "Choices:\n",
      "A. running\n",
      "B. hibernating\n",
      "C. hu\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 4Ô∏è‚É£ Load Dataset, Split Dataset Train/Validation\n",
    "# ================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset_path = \"./train_sft_final.json\"\n",
    "raw_dataset = load_dataset(\"json\", data_files={\"train\": dataset_path})\n",
    "\n",
    "full_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "# for small dataset smoke test on T4 \n",
    "# full_dataset = full_dataset.select(range(100))\n",
    "\n",
    "print(f\"\\n‚úÖ Total samples: {len(full_dataset)}\")\n",
    "print(f\"\\n‚úÖ Inspect the first entry of the data:\\n\\n {full_dataset[0]}\")\n",
    "\n",
    "\n",
    "# 95% train, 5% validation\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"\\n‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "def inspect_message_with_chat_template(example, tokenizer):\n",
    "    messages = [\n",
    "        #{\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "    ]\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n‚úÖ Inspect data after apply chat template\\n\")\n",
    "    print(formatted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "inspect_message_with_chat_template(train_dataset[0], tokenizer)\n",
    "inspect_message_with_chat_template(val_dataset[0], tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a44421-1d88-4430-bd64-10a8fde72997",
   "metadata": {},
   "source": [
    "#  How translate the message into tokens and fed to the model, Why tokenized data will be failed ???\n",
    "\n",
    "## Principle: Keep tokenization responsibilities consistent\n",
    "If you want the trainer to handle packing, batching, padding-free training etc ‚Äî give it raw \"text\" strings and set dataset_text_field=\"text\" in your SFTConfig. This is the easiest path and avoids the shape errors you saw.\n",
    "\n",
    "If you pre-tokenize, return Python lists (list of input_ids lists and attention masks) and do not return PyTorch tensors from map. Also decide whether you pad or not: if you plan to use packing, do not pad during preprocessing ‚Äî leave padding to trainer.\n",
    "\n",
    "## Option A ‚Äî Let trainer tokenize (HIGHLY RECOMMENDED)\n",
    "Why this is safe: TRL/Unsloth will call the tokenizer inside the data collator in a consistent, batch-wise manner, and will manage packing/padding in the way expected by the model/attention implementation. No tensors leak into the dataset; no shape surprises.\n",
    "\n",
    "## Option B ‚Äî Pre-tokenize correctly (ADVANCED)\n",
    "If you must pre-tokenize (e.g., offline processing, caching), do it this way:\n",
    "\n",
    "Use tokenize=False on apply_chat_template to get strings.\n",
    "\n",
    "*Tokenize the batch in a vectorized call: tokenizer(texts, truncation=True, padding=False, return_attention_mask=True, return_tensors=None) ‚Äî return_tensors must be None so HF dataset gets Python lists.*\n",
    "\n",
    "*Do not use return_tensors=\"pt\" in map.*\n",
    "\n",
    "If you plan to use packing later, set padding=False and truncation=True (or False if you filtered earlier). Trainer can pack them.\n",
    "\n",
    "## Debugging checklist (TBD)\n",
    "\n",
    "### After map, inspect dataset sample types:\n",
    "\n",
    "```python\n",
    "sample = train_dataset[0]\n",
    "print(type(sample[\"input_ids\"]), isinstance(sample[\"input_ids\"][0], int), len(sample[\"input_ids\"]))\n",
    "print(type(sample[\"attention_mask\"]), isinstance(sample[\"attention_mask\"][0], int))\n",
    "```\n",
    "You should see list and first element is int (not torch.Tensor, not list-of-list nested weirdness).\n",
    "\n",
    "### Inspect collated batch shape used by trainer (simulate a collate):\n",
    "```python\n",
    "from transformers import default_data_collator\n",
    "batch = [train_dataset[i] for i in range(4)]\n",
    "collated = default_data_collator(batch)\n",
    "print({k: (type(v), getattr(v, \"shape\", None)) for k,v in collated.items()})\n",
    "```\n",
    "This should show input_ids/attention_mask as Torch tensors of shape (batch, seq_len).\n",
    "\n",
    "### Quick model forward sanity check (very small test):\n",
    "```python\n",
    "batch = default_data_collator([train_dataset[0], train_dataset[1]])\n",
    "# move to device if needed\n",
    "out = model(**{k: torch.tensor(v).to(model.device) for k,v in batch.items() if k in (\"input_ids\",\"attention_mask\")})\n",
    "print(\"last hidden:\", getattr(out, \"last_hidden_state\", None) and out.last_hidden_state.shape)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d65286-52ab-46df-bba5-6969ddef7549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d8b2ceb41240cca12d8bf84299de7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42078c1a3e594669816440ef781c08e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖTokenization complete\n",
      "Dataset({\n",
      "    features: ['instruction', 'response', 'source', 'text'],\n",
      "    num_rows: 1506\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 5Ô∏è‚É£  Tokenize Datasets with chat template applyied\n",
    "# ================================================\n",
    "\n",
    "def tokenize_fn_old(example, tokenizer):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "\n",
    "    tokenized_chat_wrapped = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "    )\n",
    "\n",
    "    #return tokenized_chat_wrapped\n",
    "    # Return a dictionary so Hugging Face can build an Arrow table\n",
    "    return {\"input_ids\": tokenized_chat_wrapped, \n",
    "            \"attention_mask\": [1] * len(tokenized_chat_wrapped)}\n",
    "\n",
    "\n",
    "def tokenize_fn_problem(batch, tokenizer):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                # {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    # vectorized tokenizer call\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        #truncation=True,\n",
    "        #padding=\"max_length\",   # or padding=False to let Trainer handle dynamic padding\n",
    "        #padding_side = \"right\",\n",
    "        truncation=False,  # <--- CHANGED: Set to False\n",
    "        padding=False,     # <--- CHANGED: Set to False\n",
    "        #max_length=MAX_SEQ_LEN,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None,    # keep Python lists, HF Dataset friendly\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                # {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched = True)\n",
    "\n",
    "val_dataset = val_dataset.map(tokenize_fn, batched = True)\n",
    "\n",
    "# Apply the formatting using a lambda function to pass the tokenizer\n",
    "# map() can only pass the dataset batch, not extra arguments.\n",
    "#train_dataset = train_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=train_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT train\"\n",
    "#)\n",
    "#val_dataset = val_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=val_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT validation\"\n",
    "#)   \n",
    "\n",
    "print(\"\\n‚úÖTokenization complete\")\n",
    "\n",
    "#sample = val_dataset[0]\n",
    "#print(\"input_ids (first 1 tokens):\", sample[\"input_ids\"][:1])\n",
    "#print(\"attention_mask (first 1 tokens):\", sample[\"attention_mask\"][:1])\n",
    "val_dataset\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120f7d69-0674-4045-bbc6-7dc6cfe597ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "--- 1. Model and Adapter Check ---\n",
      "\n",
      "‚úÖBase Model Parameters: 20930682432\n",
      " (Trainable: (15925248, 20930682432))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 6Ô∏è‚É£   PEFT settting\n",
    "# ================================================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 2*LORA_RANK,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "print(\"--- 1. Model and Adapter Check ---\")\n",
    "# This print statement now shows the doubled number of trainable parameters\n",
    "print(f\"\\n‚úÖBase Model Parameters: {model.num_parameters()}\\n (Trainable: {model.get_nb_trainable_parameters()})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8d46b-6784-4d85-b419-e5435c05fa60",
   "metadata": {},
   "source": [
    "With Following unsloth trainer, make it run. **BUT, the batch size is not right as expected.**\n",
    "\n",
    "==((====))== Unsloth - 2x faster free finetuning | Num GPUs used = 1 \\\\ /| Num examples = 28,207 | Num Epochs = 3 | Total steps = 10,578 O^O/ \\_/ \\ Batch size per device = 4 | Gradient accumulation steps = 2 \\ / Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8 \"-____-\" Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained) Unsloth: Will smartly offload gradients to save VRAM!\n",
    "\n",
    "I set per_device_train_batch_size = 64 in your trainer_args, but Unsloth is still only using a batch of 4 per device.\n",
    "\n",
    "Batch size per device = 4 | Gradient accumulation steps = 2\n",
    "Total batch size (4 x 2 x 1) = 8\n",
    "\n",
    "```python\n",
    "from unsloth.trainer import SFTTrainer\n",
    "from unsloth.trainer import SFTTrainingArguments\n",
    "\n",
    "# set attention implementation **after loading**\n",
    "model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "# Create SFTTrainingArguments object\n",
    "training_args = SFTTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    per_device_train_batch_size=64,   # micro-batch\n",
    "    gradient_accumulation_steps=4,    # effective batch = 256\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    dataloader_num_workers=12,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=model.peft_config\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644740a-ca52-43ad-9ce4-a33215aa15ba",
   "metadata": {},
   "source": [
    "# explain Why from ChatGPT (NOT Verified)\n",
    "\n",
    "Short diagnosis ‚Äî why ‚ÄúBatch size per device = 4‚Äù even though you set 64\n",
    "\n",
    "Unsloth will override/adjust the micro-batch in certain situations:\n",
    "\n",
    "It has internal safety/defaults (common default micro-batch = 1‚Äì4) and may clamp your requested micro-batch to something that fits the runtime/packaging/num_generations constraints. Unsloth docs and issues recommend small defaults (e.g. per_device_train_batch_size = 2 often recommended). \n",
    "Unsloth Docs\n",
    "+1\n",
    "\n",
    "per_device_train_batch_size might not actually be passed correctly into Unsloth‚Äôs trainer if you pass args with wrong calling style. The SFTTrainer wrapper expects an args object/dict (passed as args=), not **trainer_args into SFTTrainer init (that will raise unexpected-keyword errors). If you accidentally passed trainer args incorrectly earlier, the trainer used default values. (You already saw errors like unexpected output_dir when using **trainer_args.) \n",
    "GitHub\n",
    "\n",
    "num_generations constraint: Unsloth has code that forces per_device_train_batch_size to be a multiple of num_generations (see issue/commit wording, it will change the batch to num_generations if it doesn‚Äôt match). If your per_device_train_batch_size is not aligned it may change it to some lower multiple. (I found this referenced in Unsloth issue logs.) \n",
    "GitHub\n",
    "\n",
    "Memory / quantization / PEFT interactions: depending on full_finetuning vs QLoRA vs LoRA, bf16/fp16 settings, and use_gradient_checkpointing, the trainer may reduce micro-batch size to ensure no OOM. Unsloth prints ‚ÄúWill smartly offload gradients to save VRAM!‚Äù, meaning it‚Äôs actively making tradeoffs. \n",
    "Unsloth Docs\n",
    "\n",
    "Net: either your args were not passed in the correct API shape, or Unsloth intentionally lowered the micro-batch for safety/compatibility (or both).\n",
    "\n",
    "\n",
    "Evidence & sources\n",
    "\n",
    "Unsloth Fine-tuning guide / defaults: per_device_train_batch_size recommended small, use grad accumulation. \n",
    "Unsloth Docs\n",
    "\n",
    "GitHub issues referencing per_device_train_batch_size being changed by Unsloth/compat constraints and the num_generations behavior. \n",
    "GitHub\n",
    "+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb49e7e-680f-493e-bf2c-66a91fec78b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Sample packing skipped (unsupported model type(s): gpt_oss detected).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837f4d7e36694076bcda35f87d63831e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=20):   0%|          | 0/28612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66765d8e20364f83be4b750c49e9bbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=20):   0%|          | 0/1506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, model, args=None, data_collator=None, train_dataset=None, eval_dataset=None, processing_class=None, compute_loss_func=None, compute_metrics=None, callbacks=None, optimizer_cls_and_kwargs=None, preprocess_logits_for_metrics=None, peft_config=None, formatting_func=None, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 7Ô∏è‚É£ Training Arguments\n",
    "# ================================================\n",
    "\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "# set attention implementation **after loading**\n",
    "#model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # TRL-Specific Args\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    packing=True,                  # üöÄ CRITICAL for Unsloth/Flash Attention efficiency\n",
    "    dataset_text_field=\"text\",     # The column containing the formatted data\n",
    "\n",
    "    # Core Training Args (Batching, Learning)\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=32, # Effective batch = 8\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optim=\"paged_adamw_32bit\",     # Recommended optimizer for QLoRA\n",
    "\n",
    "    # Logging and Saving\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Precision (auto-detects bfloat16 if hardware supports it)\n",
    "    bf16=is_bfloat16_supported(), \n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    \n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    \n",
    "    eval_dataset=val_dataset,\n",
    "    \n",
    "    peft_config=None,            # LoRA already applied\n",
    "\n",
    "    formatting_func=None         # Optional: custom formatting\n",
    ")\n",
    "\n",
    "import inspect\n",
    "print(inspect.signature(SFTTrainer.__init__))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4f8ec-831e-48b3-a523-8c3d83efcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c68af7-e588-4ebc-81c8-f4c3749de587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded checkpoint from gpt-oss-20b-sft-qlora-adapter/checkpoint-60\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# LOAD CHECKPOINT ONLY (NO TRAIN)\n",
    "# ================================\n",
    "\n",
    "checkpoint_path = f\"{OUTPUT_DIR}/checkpoint-60\"\n",
    "\n",
    "trainer._load_from_checkpoint(\n",
    "    resume_from_checkpoint=checkpoint_path\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded checkpoint from {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "413cde5a-14da-44c9-8f38-07ab453c1d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT model with validation saved to gpt-oss-20b-sft-qlora-adapter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9660698b6a3a470ab7bcf3b91c782237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027e32b2665e4582b78a5c1f249c5bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0399f541446040a9a1b4240526909b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3fbb78a41a4c08b3678880d9cd3366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...a-adapter/adapter_model.safetensors:   0%|          | 21.8kB / 63.7MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/ospost/gpt-oss-20b-sft-qlora-adapter\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 8Ô∏è‚É£ Save Fine-Tuned Model\n",
    "# ================================================\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"SFT model with validation saved to {OUTPUT_DIR}\")\n",
    "model.push_to_hub(\"ospost/gpt-oss-20b-sft-qlora-adapter\", token = \"\") # Save to HF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b914ce-5306-44d1-842d-bb8ceda8f440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
