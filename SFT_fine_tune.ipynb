{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fba48ef-4ea0-4da7-983a-387ba9c1b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2292abb-bf15-451f-a44d-0e5d0a78f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1Ô∏è‚É£ Install Required Packages\n",
    "# ==============================\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ Import Libraries\n",
    "# ==============================\n",
    "#import torch\n",
    "#from unsloth import FastLanguageModel \n",
    "#from unsloth.trainer import SFTTrainer\n",
    "#from datasets import load_dataset, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c877cd-c24f-468c-828f-3f7cce04d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3Ô∏è‚É£ Basic Fine Tune Config\n",
    "# ==============================\n",
    "\n",
    "# Define your custom system prompt\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
    "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
    "\n",
    "Your task is to:\n",
    "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
    "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
    "  but clearly distinguish between context-based and general reasoning.\n",
    "\n",
    "Your responses must be:\n",
    "- Structured ‚Äî use clear formatting and logical reasoning.\n",
    "- Contextual ‚Äî rely only on the information available.\n",
    "- Concise ‚Äî eliminate filler words while preserving precision.\n",
    "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
    "\"\"\"\n",
    "\n",
    "# --- Configuration ---\n",
    "#MAX_SEQ_LEN = 4096 # Retain the VRAM safety length\n",
    "MAX_SEQ_LEN = 1024\n",
    "DPO_TEST_SIZE = 100 # Using 100 rows for a quick test run\n",
    "LEARNING_RATE = 2e-5 # <--- INCREASED LEARNING RATE for DPO stability\n",
    "OUTPUT_DIR = \"sft_output\" # New output directory\n",
    "LORA_RANK = 16 # <--- INCREASED LORA RANK for better learning capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d530fa9e-527c-4ccb-a015-a0a04c333337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 11-08 05:38:10 [__init__.py:216] Automatically detected platform cuda.\n",
      "ERROR 11-08 05:38:11 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.2: Fast Gpt_Oss patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gpt_oss won't work! Using float32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d60d5319dcf4ec2b3c108b31fb83e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FastLanguageModel + tokenizer loaded successfully\n",
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "unsloth                           2025.11.2\n",
      "unsloth-zoo                       2025.11.3\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£  Load FastLanguageModel + Tokenizer\n",
    "# ==============================\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "dtype=None\n",
    "\n",
    "# Unsloth recommended: returns both model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n",
    "    \n",
    "    # Hopper GPUs BF16 optimization, None for auto detection\n",
    "    dtype=dtype, \n",
    "    \n",
    "    # The model‚Äôs internal attention window ‚Äì i.e. how many tokens it can actually process at once during forward/backward passes\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "\n",
    "    # 4 bit quantization to reduce memory\n",
    "    load_in_4bit = True,\n",
    "    \n",
    "    # False means with QLoRA/LoRA\n",
    "    # [NEW!] unsloth have full finetuning now!\n",
    "    full_finetuning = False,\n",
    "    \n",
    "    # token = \"hf_...\",              # use one if using gated models\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ FastLanguageModel + tokenizer loaded successfully\")\n",
    "\n",
    "!uv pip list | grep unsloth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f126d9-c348-457c-bccb-a8403a46dff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "unsloth                           2025.11.2\n",
      "unsloth-zoo                       2025.11.3\n"
     ]
    }
   ],
   "source": [
    "!uv pip list | grep unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddefad95-8259-4d39-aeee-11f715124b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total samples: 100\n",
      "\n",
      "‚úÖ Inspect the first entry of the data:\n",
      "\n",
      " {'instruction': 'Compile a visually appealing list of at least ten distinctive dips and gravies that can be paired with a range of dishes prepared on your electric griddle. Make sure to include both sweet and savory options, as well as dips that cater to different dietary restrictions such as vegan, gluten-free, or low-fat. Additionally, provide brief descriptions of each dip or gravy highlighting its key ingredients, flavor profile, and suggested griddle dishes to accompany it.', 'response': '1. Caramelized Onion and Garlic Dip - This savory dip pairs perfectly with breakfast dishes such as eggs, bacon, and pancakes or can be used as a topping for burgers and sandwiches. It is made with caramelized onions, garlic, cream cheese, and sour cream, and has a sweet and tangy flavor.\\n \\n2. Spicy Avocado Dip - This vegan option is perfect for those looking for a healthy dip option. Made with ripe avocados, jalapenos, lime juice, and cilantro, it is a great accompaniment for veggie fajitas or as a topping for grilled chicken or fish.\\n\\n3. Sweet and Sour Dipping Sauce - This tangy sauce is great for dipping crispy chicken tenders or fried shrimp. Made with ketchup, brown sugar, apple cider vinegar, soy sauce, and garlic, it has a sweet and slightly sour flavor that complements fried foods well.\\n\\n4. Bacon and Cheddar Dip - This savory dip pairs well with baked potatoes and grilled meats. Made with chopped bacon, cheddar cheese, sour cream, and Worcestershire sauce, it has a rich and creamy texture with a smoky flavor.\\n\\n5. Chipotle Mayo Dip - This spicy dip is perfect for dipping sweet potato fries or as a topping for burgers or sandwiches. Made with mayonnaise, chipotle chilies, lime juice, and garlic, it has a smoky and spicy flavor.\\n\\n6. Cheesy Queso Dip - This gluten-free dip is great for dipping tortilla chips or as a topping for nachos. Made with Velveeta cheese, diced tomatoes, and green chilies, it has a creamy and cheesy flavor with a slight kick.\\n\\n7. Low-Fat Ranch Dip - This healthy dip is made with Greek yogurt, garlic, and fresh herbs, and is a great pairing for grilled veggies, chicken, or fish.\\n\\n8. Roasted Red Pepper Dip - This vegan dip is a great pairing for grilled vegetables, flatbreads, or grilled chicken. Made with roasted red peppers, garlic, and olive oil, it has a smoky and slightly sweet flavor.\\n\\n9. Cilantro Lime Aioli - This low-fat dip is great for dipping crunchy onion rings or as a topping for grilled chicken or fish. Made with Greek yogurt, lime juice, and fresh cilantro, it has a tangy and slightly spicy flavor.\\n\\n10. Chocolate Ganache - This sweet dip is perfect for dipping fresh fruit or as a topping for pancakes, waffles, or French toast. Made with dark chocolate and heavy cream, it has a rich and decadent chocolate flavor.', 'source': 'ultrachat'}\n",
      "\n",
      "‚úÖ Train samples: 95\n",
      "\n",
      "‚úÖ Validation samples: 5\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
      "including software architecture, cloud inf\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
      "including software architecture, cloud inf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ Load Dataset, Split Dataset into Train / Validation\n",
    "# ==============================\n",
    "dataset_path = \"./train_sft_final.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files={\"train\": dataset_path})\n",
    "\n",
    "full_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "full_dataset = full_dataset.select(range(100))\n",
    "\n",
    "print(f\"\\n‚úÖ Total samples: {len(full_dataset)}\")\n",
    "print(f\"\\n‚úÖ Inspect the first entry of the data:\\n\\n {full_dataset[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "# 95% train, 5% validation\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"\\n‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "def inspect_message_with_chat_template(example, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "    ]\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n‚úÖ Inspect data after apply chat template\\n\")\n",
    "    print(formatted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "inspect_message_with_chat_template(train_dataset[0], tokenizer)\n",
    "inspect_message_with_chat_template(val_dataset[0], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d65286-52ab-46df-bba5-6969ddef7549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖTokenization complete\n",
      "Dataset({\n",
      "    features: ['instruction', 'response', 'source', 'text'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 5Ô∏è‚É£  Tokenize both Train & Validation Datasets with chat template\n",
    "# ==============================\n",
    "def tokenize_fn_old(example, tokenizer):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "\n",
    "    tokenized_chat_wrapped = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "    )\n",
    "\n",
    "    #return tokenized_chat_wrapped\n",
    "    # Return a dictionary so Hugging Face can build an Arrow table\n",
    "    return {\"input_ids\": tokenized_chat_wrapped, \n",
    "            \"attention_mask\": [1] * len(tokenized_chat_wrapped)}\n",
    "\n",
    "\n",
    "def tokenize_fn_problem(batch, tokenizer):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    # vectorized tokenizer call\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        #truncation=True,\n",
    "        #padding=\"max_length\",   # or padding=False to let Trainer handle dynamic padding\n",
    "        #padding_side = \"right\",\n",
    "        truncation=False,  # <--- CHANGED: Set to False\n",
    "        padding=False,     # <--- CHANGED: Set to False\n",
    "        #max_length=MAX_SEQ_LEN,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None,    # keep Python lists, HF Dataset friendly\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched = True)\n",
    "val_dataset = val_dataset.map(tokenize_fn, batched = True)\n",
    "\n",
    "# Apply the formatting using a lambda function to pass the tokenizer\n",
    "# map() can only pass the dataset batch, not extra arguments.\n",
    "#train_dataset = train_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=train_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT train\"\n",
    "#)\n",
    "#val_dataset = val_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=val_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT validation\"\n",
    "#)   \n",
    "\n",
    "print(\"\\n‚úÖTokenization complete\")\n",
    "\n",
    "#sample = val_dataset[0]\n",
    "#print(\"input_ids (first 1 tokens):\", sample[\"input_ids\"][:1])\n",
    "#print(\"attention_mask (first 1 tokens):\", sample[\"attention_mask\"][:1])\n",
    "val_dataset\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120f7d69-0674-4045-bbc6-7dc6cfe597ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "--- 1. Model and Adapter Check ---\n",
      "\n",
      "‚úÖBase Model Parameters: 20922719808\n",
      " (Trainable: (7962624, 20922719808))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "# TBD try unsloth later\n",
    "#from unsloth.trainer import SFTTrainer \n",
    "\n",
    "# ==============================\n",
    "# 6Ô∏è‚É£   PEFT settting\n",
    "# ==============================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 2*LORA_RANK,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "print(\"--- 1. Model and Adapter Check ---\")\n",
    "# This print statement now shows the doubled number of trainable parameters\n",
    "print(f\"\\n‚úÖBase Model Parameters: {model.num_parameters()}\\n (Trainable: {model.get_nb_trainable_parameters()})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60cd7997-e645-4e7f-8a76-1862b6b5f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 7Ô∏è‚É£ Training Arguments\n",
    "# ==============================\n",
    "\n",
    "trainer_args = {\n",
    "    # How many tokens per example are kept during dataset preprocessing (tokenization, padding, truncation).\n",
    "    \"max_seq_length\": MAX_SEQ_LEN,\n",
    "    \n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \n",
    "    # H100; 48 for H200\n",
    "    #\"per_device_train_batch_size\": 32,\n",
    "    \n",
    "    \"per_device_train_batch_size\": 2, \n",
    "    \n",
    "    # adjust to reach effective batch\n",
    "    #\"gradient_accumulation_steps\": 4,\n",
    "    \n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \n",
    "    \"num_train_epochs\": 1,\n",
    "    \n",
    "    \"logging_steps\": 5,\n",
    "    \n",
    "    \"save_strategy\": \"steps\",\n",
    "    \n",
    "    \"save_steps\": 50,\n",
    "    \n",
    "    \"save_total_limit\": 2,\n",
    "    \n",
    "    #\"bf16\": True,\n",
    "    \"bf16\": False,\n",
    "    \n",
    "    #\"fp16\": False,\n",
    "    \"fp16\": True,\n",
    "    \n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \n",
    "    \"report_to\": \"none\",\n",
    "    \n",
    "    # run validation during training\n",
    "    \"evaluation_strategy\": \"steps\",  \n",
    "    \n",
    "     # validation every 50 steps\n",
    "    \"eval_steps\": 50                \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe363e6d-640e-4648-a07d-73e284788ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[trl.trainer.sft_trainer|WARNING]Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "[trl.trainer.sft_trainer|WARNING]You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "[datasets.arrow_dataset|WARNING]num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'eval_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'torch_empty_cache_steps', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'restore_callback_states_from_checkpoint', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'parallelism_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'hub_revision', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'include_for_metrics', 'eval_do_concat_batches', 'fp16_backend', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', '_n_gpu', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules', 'batch_eval_metrics', 'eval_on_start', 'use_liger_kernel', 'liger_kernel_config', 'eval_use_gather_object', 'average_tokens_across_devices', 'model_init_kwargs', 'chat_template_path', 'dataset_text_field', 'dataset_kwargs', 'dataset_num_proc', 'eos_token', 'pad_token', 'max_length', 'packing', 'packing_strategy', 'padding_free', 'pad_to_multiple_of', 'eval_packing', 'completion_only_loss', 'assistant_only_loss', 'activation_offloading', 'vllm_sampling_params', 'unsloth_num_chunks', 'max_seq_length'])\n",
      "\n",
      "‚úÖ SFTConfig Parameters:\n",
      " dict_keys(['output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'eval_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'torch_empty_cache_steps', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'restore_callback_states_from_checkpoint', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'parallelism_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'hub_revision', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'include_for_metrics', 'eval_do_concat_batches', 'fp16_backend', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', '_n_gpu', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules', 'batch_eval_metrics', 'eval_on_start', 'use_liger_kernel', 'liger_kernel_config', 'eval_use_gather_object', 'average_tokens_across_devices', 'model_init_kwargs', 'chat_template_path', 'dataset_text_field', 'dataset_kwargs', 'dataset_num_proc', 'eos_token', 'pad_token', 'max_length', 'packing', 'packing_strategy', 'padding_free', 'pad_to_multiple_of', 'eval_packing', 'completion_only_loss', 'assistant_only_loss', 'activation_offloading', 'vllm_sampling_params', 'unsloth_num_chunks', 'max_seq_length'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 8Ô∏è‚É£ Initialize SFTTrainer with Validation\n",
    "# ==============================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    \n",
    "    args=trainer_args,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    \n",
    "    # validation dataset included\n",
    "    eval_dataset=val_dataset,\n",
    "    \n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    "    packing=True\n",
    "    \n",
    "    #packing=False\n",
    ")\n",
    "print(SFTConfig.__dataclass_fields__.keys())\n",
    "\n",
    "print(f\"\\n‚úÖ SFTConfig Parameters:\\n {SFTConfig.__dataclass_fields__.keys()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f99834f7-c9c3-464a-a4e8-b1fbb517418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 72 | Num Epochs = 3 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 7,962,624 of 20,922,719,808 (0.04% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 20:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.868700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.964100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.732500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.680900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.731800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.096300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.192500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.892500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27, training_loss=3.4462189232861555, metrics={'train_runtime': 1475.441, 'train_samples_per_second': 0.146, 'train_steps_per_second': 0.018, 'total_flos': 2.4545106573410304e+16, 'train_loss': 3.4462189232861555, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================\n",
    "# 9Ô∏è‚É£ Start Training\n",
    "# ==============================\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cde5a-14da-44c9-8f38-07ab453c1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üîü Save Fine-Tuned Model\n",
    "# ==============================\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"SFT model with validation saved to {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
