{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fba48ef-4ea0-4da7-983a-387ba9c1b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m11 packages\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m11 packages\u001b[0m \u001b[2min 0.26ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 numpy==2.3.4 pandas==2.3.3 pyarrow==22.0.0 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2292abb-bf15-451f-a44d-0e5d0a78f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1Ô∏è‚É£ Install Required Packages\n",
    "# ==============================\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ Import Libraries\n",
    "# ==============================\n",
    "#import torch\n",
    "#from unsloth import FastLanguageModel \n",
    "#from unsloth.trainer import SFTTrainer\n",
    "#from datasets import load_dataset, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21721e8-6762-422b-ba4e-c252a934580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --upgrade --force-reinstall --no-cache-dir transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e8045-aa49-4060-bca4-99336327f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --upgrade --force-reinstall --no-cache-dir numpy==2.3.4 scipy scikit-learn pandas numba  statsmodels  joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c877cd-c24f-468c-828f-3f7cce04d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3Ô∏è‚É£ Basic Fine Tune Config\n",
    "# ==============================\n",
    "\n",
    "# Define your custom system prompt\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
    "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
    "\n",
    "Your task is to:\n",
    "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
    "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
    "  but clearly distinguish between context-based and general reasoning.\n",
    "\n",
    "Your responses must be:\n",
    "- Structured ‚Äî use clear formatting and logical reasoning.\n",
    "- Contextual ‚Äî rely only on the information available.\n",
    "- Concise ‚Äî eliminate filler words while preserving precision.\n",
    "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
    "\"\"\"\n",
    "\n",
    "# --- Configuration ---\n",
    "MAX_SEQ_LEN = 4096 # Retain the VRAM safety length\n",
    "#MAX_SEQ_LEN = 1024\n",
    "#SFT_TEST_SIZE = 100 # Using 100 rows for a quick test run\n",
    "LEARNING_RATE = 1.5e-4 # <--- INCREASED LEARNING RATE for DPO stability\n",
    "OUTPUT_DIR = \"gpt-oss-20b-sft-qlora-adapter\" # New output directory\n",
    "LORA_RANK = 32 # <--- INCREASED LORA RANK for better learning capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d530fa9e-527c-4ccb-a015-a0a04c333337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.2: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.811 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54eb989bed7948afac08b90d158ca7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FastLanguageModel + tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£  Load FastLanguageModel + Tokenizer\n",
    "# ==============================\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "dtype=None\n",
    "\n",
    "# Unsloth recommended: returns both model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    \n",
    "    # Hopper GPUs BF16 optimization, None for auto detection\n",
    "    dtype=dtype, \n",
    "    \n",
    "    # The model‚Äôs internal attention window ‚Äì i.e. how many tokens it can actually process at once during forward/backward passes\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "\n",
    "    # 4 bit quantization to reduce memory\n",
    "    load_in_4bit = True,\n",
    "    \n",
    "    # False means with QLoRA/LoRA\n",
    "    # [NEW!] unsloth have full finetuning now!\n",
    "    full_finetuning = False,\n",
    "    \n",
    "    # token = \"hf_...\",              # use one if using gated models\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ FastLanguageModel + tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd97736-74dc-408e-9803-bb6bc9b199bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(is_bfloat16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddefad95-8259-4d39-aeee-11f715124b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total samples: 29692\n",
      "\n",
      "‚úÖ Inspect the first entry of the data:\n",
      "\n",
      " {'instruction': 'Compile a visually appealing list of at least ten distinctive dips and gravies that can be paired with a range of dishes prepared on your electric griddle. Make sure to include both sweet and savory options, as well as dips that cater to different dietary restrictions such as vegan, gluten-free, or low-fat. Additionally, provide brief descriptions of each dip or gravy highlighting its key ingredients, flavor profile, and suggested griddle dishes to accompany it.', 'response': '1. Caramelized Onion and Garlic Dip - This savory dip pairs perfectly with breakfast dishes such as eggs, bacon, and pancakes or can be used as a topping for burgers and sandwiches. It is made with caramelized onions, garlic, cream cheese, and sour cream, and has a sweet and tangy flavor.\\n \\n2. Spicy Avocado Dip - This vegan option is perfect for those looking for a healthy dip option. Made with ripe avocados, jalapenos, lime juice, and cilantro, it is a great accompaniment for veggie fajitas or as a topping for grilled chicken or fish.\\n\\n3. Sweet and Sour Dipping Sauce - This tangy sauce is great for dipping crispy chicken tenders or fried shrimp. Made with ketchup, brown sugar, apple cider vinegar, soy sauce, and garlic, it has a sweet and slightly sour flavor that complements fried foods well.\\n\\n4. Bacon and Cheddar Dip - This savory dip pairs well with baked potatoes and grilled meats. Made with chopped bacon, cheddar cheese, sour cream, and Worcestershire sauce, it has a rich and creamy texture with a smoky flavor.\\n\\n5. Chipotle Mayo Dip - This spicy dip is perfect for dipping sweet potato fries or as a topping for burgers or sandwiches. Made with mayonnaise, chipotle chilies, lime juice, and garlic, it has a smoky and spicy flavor.\\n\\n6. Cheesy Queso Dip - This gluten-free dip is great for dipping tortilla chips or as a topping for nachos. Made with Velveeta cheese, diced tomatoes, and green chilies, it has a creamy and cheesy flavor with a slight kick.\\n\\n7. Low-Fat Ranch Dip - This healthy dip is made with Greek yogurt, garlic, and fresh herbs, and is a great pairing for grilled veggies, chicken, or fish.\\n\\n8. Roasted Red Pepper Dip - This vegan dip is a great pairing for grilled vegetables, flatbreads, or grilled chicken. Made with roasted red peppers, garlic, and olive oil, it has a smoky and slightly sweet flavor.\\n\\n9. Cilantro Lime Aioli - This low-fat dip is great for dipping crunchy onion rings or as a topping for grilled chicken or fish. Made with Greek yogurt, lime juice, and fresh cilantro, it has a tangy and slightly spicy flavor.\\n\\n10. Chocolate Ganache - This sweet dip is perfect for dipping fresh fruit or as a topping for pancakes, waffles, or French toast. Made with dark chocolate and heavy cream, it has a rich and decadent chocolate flavor.', 'source': 'ultrachat'}\n",
      "\n",
      "‚úÖ Train samples: 28207\n",
      "\n",
      "‚úÖ Validation samples: 1485\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-10\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
      "including software architecture, cloud inf\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Inspect data after apply chat template\n",
      "\n",
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-10\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
      "including software architecture, cloud inf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ Load Dataset, Split Dataset into Train / Validation\n",
    "# ==============================\n",
    "dataset_path = \"./train_sft_final.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files={\"train\": dataset_path})\n",
    "\n",
    "full_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "# for small dataset smoke test on T4 \n",
    "# full_dataset = full_dataset.select(range(100))\n",
    "\n",
    "print(f\"\\n‚úÖ Total samples: {len(full_dataset)}\")\n",
    "print(f\"\\n‚úÖ Inspect the first entry of the data:\\n\\n {full_dataset[0]}\")\n",
    "\n",
    "\n",
    "# 95% train, 5% validation\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"\\n‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "def inspect_message_with_chat_template(example, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "    ]\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n‚úÖ Inspect data after apply chat template\\n\")\n",
    "    print(formatted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "inspect_message_with_chat_template(train_dataset[0], tokenizer)\n",
    "inspect_message_with_chat_template(val_dataset[0], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78d65286-52ab-46df-bba5-6969ddef7549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖTokenization complete\n",
      "Dataset({\n",
      "    features: ['instruction', 'response', 'source', 'text'],\n",
      "    num_rows: 1485\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 5Ô∏è‚É£  Tokenize both Train & Validation Datasets with chat template\n",
    "# ==============================\n",
    "def tokenize_fn_old(example, tokenizer):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "\n",
    "    tokenized_chat_wrapped = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "    )\n",
    "\n",
    "    #return tokenized_chat_wrapped\n",
    "    # Return a dictionary so Hugging Face can build an Arrow table\n",
    "    return {\"input_ids\": tokenized_chat_wrapped, \n",
    "            \"attention_mask\": [1] * len(tokenized_chat_wrapped)}\n",
    "\n",
    "\n",
    "def tokenize_fn_problem(batch, tokenizer):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    # vectorized tokenizer call\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        #truncation=True,\n",
    "        #padding=\"max_length\",   # or padding=False to let Trainer handle dynamic padding\n",
    "        #padding_side = \"right\",\n",
    "        truncation=False,  # <--- CHANGED: Set to False\n",
    "        padding=False,     # <--- CHANGED: Set to False\n",
    "        #max_length=MAX_SEQ_LEN,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None,    # keep Python lists, HF Dataset friendly\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched = True)\n",
    "val_dataset = val_dataset.map(tokenize_fn, batched = True)\n",
    "\n",
    "# Apply the formatting using a lambda function to pass the tokenizer\n",
    "# map() can only pass the dataset batch, not extra arguments.\n",
    "#train_dataset = train_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=train_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT train\"\n",
    "#)\n",
    "#val_dataset = val_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=val_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT validation\"\n",
    "#)   \n",
    "\n",
    "print(\"\\n‚úÖTokenization complete\")\n",
    "\n",
    "#sample = val_dataset[0]\n",
    "#print(\"input_ids (first 1 tokens):\", sample[\"input_ids\"][:1])\n",
    "#print(\"attention_mask (first 1 tokens):\", sample[\"attention_mask\"][:1])\n",
    "val_dataset\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120f7d69-0674-4045-bbc6-7dc6cfe597ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n",
      "--- 1. Model and Adapter Check ---\n",
      "\n",
      "‚úÖBase Model Parameters: 20930682432\n",
      " (Trainable: (15925248, 20930682432))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# 6Ô∏è‚É£   PEFT settting\n",
    "# ==============================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 2*LORA_RANK,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "print(\"--- 1. Model and Adapter Check ---\")\n",
    "# This print statement now shows the doubled number of trainable parameters\n",
    "print(f\"\\n‚úÖBase Model Parameters: {model.num_parameters()}\\n (Trainable: {model.get_nb_trainable_parameters()})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8d46b-6784-4d85-b419-e5435c05fa60",
   "metadata": {},
   "source": [
    "With Following unsloth trainer, make it run. **BUT, the batch size is not right as expected.**\n",
    "\n",
    "==((====))== Unsloth - 2x faster free finetuning | Num GPUs used = 1 \\\\ /| Num examples = 28,207 | Num Epochs = 3 | Total steps = 10,578 O^O/ \\_/ \\ Batch size per device = 4 | Gradient accumulation steps = 2 \\ / Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8 \"-____-\" Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained) Unsloth: Will smartly offload gradients to save VRAM!\n",
    "\n",
    "I set per_device_train_batch_size = 64 in your trainer_args, but Unsloth is still only using a batch of 4 per device.\n",
    "\n",
    "Batch size per device = 4 | Gradient accumulation steps = 2\n",
    "Total batch size (4 x 2 x 1) = 8\n",
    "\n",
    "```python\n",
    "from unsloth.trainer import SFTTrainer\n",
    "from unsloth.trainer import SFTTrainingArguments\n",
    "\n",
    "# set attention implementation **after loading**\n",
    "model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "# 1Ô∏è‚É£ Create SFTTrainingArguments object\n",
    "training_args = SFTTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    per_device_train_batch_size=64,   # micro-batch\n",
    "    gradient_accumulation_steps=4,    # effective batch = 256\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    dataloader_num_workers=12,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=model.peft_config\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Train\n",
    "trainer.train()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ce1d8-65ef-46d3-9946-330b0b91a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[trl.trainer.sft_trainer|WARNING]Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "[trl.trainer.sft_trainer|WARNING]You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, model, args=None, data_collator=None, train_dataset=None, eval_dataset=None, processing_class=None, compute_loss_func=None, compute_metrics=None, callbacks=None, optimizer_cls_and_kwargs=None, preprocess_logits_for_metrics=None, peft_config=None, formatting_func=None, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 8,312 | Num Epochs = 3 | Total steps = 27\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 32\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 32 x 1) = 1,024\n",
      " \"-____-\"     Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "# set attention implementation **after loading**\n",
    "#model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "# ==============================\n",
    "# 7Ô∏è‚É£ Training Arguments\n",
    "# ==============================\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # TRL-Specific Args\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    packing=True,                  # üöÄ CRITICAL for Unsloth/Flash Attention efficiency\n",
    "    dataset_text_field=\"text\",     # The column containing the formatted data\n",
    "\n",
    "    # Core Training Args (Batching, Learning)\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=32, # Effective batch = 8\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optim=\"paged_adamw_32bit\",     # Recommended optimizer for QLoRA\n",
    "\n",
    "    # Logging and Saving\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Precision (auto-detects bfloat16 if hardware supports it)\n",
    "    bf16=is_bfloat16_supported(), \n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    \n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    \n",
    "    eval_dataset=val_dataset,\n",
    "    \n",
    "    peft_config=None,            # LoRA already applied\n",
    "\n",
    "    formatting_func=None         # Optional: custom formatting\n",
    ")\n",
    "\n",
    "import inspect\n",
    "print(inspect.signature(SFTTrainer.__init__))\n",
    "\n",
    "# 3Ô∏è‚É£ Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217458b-e74a-48cf-95ca-f9b3f43ea3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cde5a-14da-44c9-8f38-07ab453c1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üîü Save Fine-Tuned Model\n",
    "# ==============================\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"SFT model with validation saved to {output_dir}\")\n",
    "model.push_to_hub(\"ospost/gpt-oss-20b-sft-qlora-adapter\", token = \"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\") # Save to HF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
