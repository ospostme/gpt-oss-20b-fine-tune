{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba48ef-4ea0-4da7-983a-387ba9c1b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 numpy==2.3.4 pandas==2.3.3 pyarrow==22.0.0 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2292abb-bf15-451f-a44d-0e5d0a78f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1Ô∏è‚É£ Install Required Packages\n",
    "# ==============================\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ Import Libraries\n",
    "# ==============================\n",
    "#import torch\n",
    "#from unsloth import FastLanguageModel \n",
    "#from unsloth.trainer import SFTTrainer\n",
    "#from datasets import load_dataset, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21721e8-6762-422b-ba4e-c252a934580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --upgrade --force-reinstall --no-cache-dir transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e8045-aa49-4060-bca4-99336327f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install --upgrade --force-reinstall --no-cache-dir numpy==2.3.4 scipy scikit-learn pandas numba  statsmodels  joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c877cd-c24f-468c-828f-3f7cce04d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3Ô∏è‚É£ Basic Fine Tune Config\n",
    "# ==============================\n",
    "\n",
    "# Define your custom system prompt\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
    "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
    "\n",
    "Your task is to:\n",
    "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
    "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
    "  but clearly distinguish between context-based and general reasoning.\n",
    "\n",
    "Your responses must be:\n",
    "- Structured ‚Äî use clear formatting and logical reasoning.\n",
    "- Contextual ‚Äî rely only on the information available.\n",
    "- Concise ‚Äî eliminate filler words while preserving precision.\n",
    "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
    "\"\"\"\n",
    "\n",
    "# --- Configuration ---\n",
    "MAX_SEQ_LEN = 4096 # Retain the VRAM safety length\n",
    "#MAX_SEQ_LEN = 1024\n",
    "#SFT_TEST_SIZE = 100 # Using 100 rows for a quick test run\n",
    "LEARNING_RATE = 1.5e-5 # <--- INCREASED LEARNING RATE for DPO stability\n",
    "OUTPUT_DIR = \"gpt-oss-20b-sft-qlora-adapter\" # New output directory\n",
    "LORA_RANK = 32 # <--- INCREASED LORA RANK for better learning capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530fa9e-527c-4ccb-a015-a0a04c333337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£  Load FastLanguageModel + Tokenizer\n",
    "# ==============================\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n",
    "    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n",
    "    \"unsloth/gpt-oss-120b\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "dtype=None\n",
    "\n",
    "# Unsloth recommended: returns both model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b\",\n",
    "    \n",
    "    # Hopper GPUs BF16 optimization, None for auto detection\n",
    "    dtype=dtype, \n",
    "    \n",
    "    # The model‚Äôs internal attention window ‚Äì i.e. how many tokens it can actually process at once during forward/backward passes\n",
    "    max_seq_length = MAX_SEQ_LEN,\n",
    "\n",
    "    # 4 bit quantization to reduce memory\n",
    "    load_in_4bit = True,\n",
    "    \n",
    "    # False means with QLoRA/LoRA\n",
    "    # [NEW!] unsloth have full finetuning now!\n",
    "    full_finetuning = False,\n",
    "    \n",
    "    # token = \"hf_...\",              # use one if using gated models\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ FastLanguageModel + tokenizer loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefad95-8259-4d39-aeee-11f715124b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ Load Dataset, Split Dataset into Train / Validation\n",
    "# ==============================\n",
    "dataset_path = \"./train_sft_final.jsonl\"\n",
    "raw_dataset = load_dataset(\"json\", data_files={\"train\": dataset_path})\n",
    "\n",
    "full_dataset = raw_dataset[\"train\"]\n",
    "\n",
    "# for small dataset smoke test on T4 \n",
    "# full_dataset = full_dataset.select(range(100))\n",
    "\n",
    "print(f\"\\n‚úÖ Total samples: {len(full_dataset)}\")\n",
    "print(f\"\\n‚úÖ Inspect the first entry of the data:\\n\\n {full_dataset[0]}\")\n",
    "\n",
    "\n",
    "# 95% train, 5% validation\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"\\n‚úÖ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "def inspect_message_with_chat_template(example, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "    ]\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\n‚úÖ Inspect data after apply chat template\\n\")\n",
    "    print(formatted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "inspect_message_with_chat_template(train_dataset[0], tokenizer)\n",
    "inspect_message_with_chat_template(val_dataset[0], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d65286-52ab-46df-bba5-6969ddef7549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5Ô∏è‚É£  Tokenize both Train & Validation Datasets with chat template\n",
    "# ==============================\n",
    "def tokenize_fn_old(example, tokenizer):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "\n",
    "    tokenized_chat_wrapped = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "    )\n",
    "\n",
    "    #return tokenized_chat_wrapped\n",
    "    # Return a dictionary so Hugging Face can build an Arrow table\n",
    "    return {\"input_ids\": tokenized_chat_wrapped, \n",
    "            \"attention_mask\": [1] * len(tokenized_chat_wrapped)}\n",
    "\n",
    "\n",
    "def tokenize_fn_problem(batch, tokenizer):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    # vectorized tokenizer call\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        #truncation=True,\n",
    "        #padding=\"max_length\",   # or padding=False to let Trainer handle dynamic padding\n",
    "        #padding_side = \"right\",\n",
    "        truncation=False,  # <--- CHANGED: Set to False\n",
    "        padding=False,     # <--- CHANGED: Set to False\n",
    "        #max_length=MAX_SEQ_LEN,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None,    # keep Python lists, HF Dataset friendly\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # build texts\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": instr},\n",
    "                {\"role\": \"assistant\", \"content\": resp},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        for instr, resp in zip(batch[\"instruction\"], batch[\"response\"])\n",
    "    ]\n",
    "\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched = True)\n",
    "val_dataset = val_dataset.map(tokenize_fn, batched = True)\n",
    "\n",
    "# Apply the formatting using a lambda function to pass the tokenizer\n",
    "# map() can only pass the dataset batch, not extra arguments.\n",
    "#train_dataset = train_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=train_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT train\"\n",
    "#)\n",
    "#val_dataset = val_dataset.map(\n",
    "#    lambda x: tokenize_fn(x, tokenizer),\n",
    "#    remove_columns=val_dataset.column_names,\n",
    "#    num_proc=4, # Use multiple cores for fast processing\n",
    "#    desc=\"Mapping self dataet for SFT validation\"\n",
    "#)   \n",
    "\n",
    "print(\"\\n‚úÖTokenization complete\")\n",
    "\n",
    "#sample = val_dataset[0]\n",
    "#print(\"input_ids (first 1 tokens):\", sample[\"input_ids\"][:1])\n",
    "#print(\"attention_mask (first 1 tokens):\", sample[\"attention_mask\"][:1])\n",
    "val_dataset\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f7d69-0674-4045-bbc6-7dc6cfe597ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================\n",
    "# 6Ô∏è‚É£   PEFT settting\n",
    "# ==============================\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_RANK, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 2*LORA_RANK,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "print(\"--- 1. Model and Adapter Check ---\")\n",
    "# This print statement now shows the doubled number of trainable parameters\n",
    "print(f\"\\n‚úÖBase Model Parameters: {model.num_parameters()}\\n (Trainable: {model.get_nb_trainable_parameters()})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8d46b-6784-4d85-b419-e5435c05fa60",
   "metadata": {},
   "source": [
    "With Following unsloth trainer, make it run. **BUT, the batch size is not right as expected.**\n",
    "\n",
    "==((====))== Unsloth - 2x faster free finetuning | Num GPUs used = 1 \\\\ /| Num examples = 28,207 | Num Epochs = 3 | Total steps = 10,578 O^O/ \\_/ \\ Batch size per device = 4 | Gradient accumulation steps = 2 \\ / Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8 \"-____-\" Trainable parameters = 15,925,248 of 20,930,682,432 (0.08% trained) Unsloth: Will smartly offload gradients to save VRAM!\n",
    "\n",
    "I set per_device_train_batch_size = 64 in your trainer_args, but Unsloth is still only using a batch of 4 per device.\n",
    "\n",
    "Batch size per device = 4 | Gradient accumulation steps = 2\n",
    "Total batch size (4 x 2 x 1) = 8\n",
    "\n",
    "```python\n",
    "from unsloth.trainer import SFTTrainer\n",
    "from unsloth.trainer import SFTTrainingArguments\n",
    "\n",
    "# set attention implementation **after loading**\n",
    "model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "# 1Ô∏è‚É£ Create SFTTrainingArguments object\n",
    "training_args = SFTTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    per_device_train_batch_size=64,   # micro-batch\n",
    "    gradient_accumulation_steps=4,    # effective batch = 256\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    dataloader_num_workers=12,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=model.peft_config\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Train\n",
    "trainer.train()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ce1d8-65ef-46d3-9946-330b0b91a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "# set attention implementation **after loading**\n",
    "#model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "# ==============================\n",
    "# 7Ô∏è‚É£ Training Arguments\n",
    "# ==============================\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    # TRL-Specific Args\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    packing=True,                  # üöÄ CRITICAL for Unsloth/Flash Attention efficiency\n",
    "    dataset_text_field=\"text\",     # The column containing the formatted data\n",
    "\n",
    "    # Core Training Args (Batching, Learning)\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2, # Effective batch = 8\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optim=\"paged_adamw_32bit\",     # Recommended optimizer for QLoRA\n",
    "\n",
    "    # Logging and Saving\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Precision (auto-detects bfloat16 if hardware supports it)\n",
    "    bf16=is_bfloat16_supported(), \n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    \n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=train_dataset,\n",
    "    \n",
    "    eval_dataset=val_dataset,\n",
    "    \n",
    "    peft_config=None,            # LoRA already applied\n",
    "\n",
    "    formatting_func=None         # Optional: custom formatting\n",
    ")\n",
    "\n",
    "import inspect\n",
    "print(inspect.signature(SFTTrainer.__init__))\n",
    "\n",
    "# 3Ô∏è‚É£ Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217458b-e74a-48cf-95ca-f9b3f43ea3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cde5a-14da-44c9-8f38-07ab453c1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üîü Save Fine-Tuned Model\n",
    "# ==============================\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"SFT model with validation saved to {output_dir}\")\n",
    "model.push_to_hub(\"ospost/gpt-oss-20b-sft-qlora-adapter\", token = \"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\") # Save to HF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
