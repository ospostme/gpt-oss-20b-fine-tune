{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed153c43-9c31-4051-95a1-8e1d2055f55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /home/ospost/micromamba/envs/ML\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 0.16ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================\n",
    "# 1ï¸âƒ£ Env Prepare Install Required Packages\n",
    "# ================================================\n",
    "\n",
    "#%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5da945-1efe-4050-8fbd-b35cab526c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ospost/micromamba/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.2: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    GRID A100D-40C. Num GPUs = 1. Max memory: 39.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1. CUDA: 8.0. CUDA Toolkit: 12.9. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB 4-bit layers found: 1632\n",
      "Sample BNB layer: model.layers.0.self_attn.q_proj\n",
      "Model compute dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from collections import Counter\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/gpt-oss-20b\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "bnb_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, bnb.nn.Linear4bit):\n",
    "        bnb_layers.append(name)\n",
    "\n",
    "print(f\"BNB 4-bit layers found: {len(bnb_layers)}\")\n",
    "print(\"Sample BNB layer:\", bnb_layers[0] if bnb_layers else \"âŒ NONE\")\n",
    "print(\"Model compute dtype:\", model.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74f0dea-da3a-477e-875f-b4483b817da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    checkpoint_path=\"ospost/gpt-oss-20b-sft-adapter\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7462d3c3-3ac6-465a-be7c-4690d33829dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA parameters found: 192\n",
      "Sample LoRA param: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "LoRA dtypes: Counter({torch.float32: 192})\n",
      "Trainable params count: 192\n",
      "Sample trainable param: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n"
     ]
    }
   ],
   "source": [
    "assert any(\"lora_\" in n for n,_ in model.named_parameters())\n",
    "\n",
    "lora_params = [\n",
    "    (n, p) for n, p in model.named_parameters() if \"lora_\" in n\n",
    "]\n",
    "\n",
    "print(f\"LoRA parameters found: {len(lora_params)}\")\n",
    "print(\"Sample LoRA param:\", lora_params[0][0] if lora_params else \"âŒ NONE\")\n",
    "\n",
    "print(\"LoRA dtypes:\", Counter(p.dtype for _, p in lora_params))\n",
    "\n",
    "trainable = [\n",
    "    n for n, p in model.named_parameters() if p.requires_grad\n",
    "]\n",
    "\n",
    "print(f\"Trainable params count: {len(trainable)}\")\n",
    "print(\"Sample trainable param:\", trainable[0] if trainable else \"âŒ NONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca51dbb-5595-43f5-a77f-6a9e595ef7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/ospost/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00000-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into mxfp4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/ospost/workspace/gpt-oss-20b-fine-tune/gpt-oss-20b-sft-mxfp4`\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    \"gpt-oss-20b-sft-mxfp4\",\n",
    "    tokenizer,\n",
    "    save_method=\"mxfp4\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9438f6ce-355c-46df-830b-0ef1a8ea3a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13G\n",
      "drwxrwxr-x 3 ospost ospost 4.0K Jan  9 14:07 .\n",
      "drwxrwxr-x 7 ospost ospost 4.0K Jan  9 14:07 ..\n",
      "drwxrwxr-x 3 ospost ospost 4.0K Jan  9 14:07 .cache\n",
      "-rw-rw-r-- 1 ospost ospost  15K Jan  9 14:07 chat_template.jinja\n",
      "-rw-rw-r-- 1 ospost ospost 1.8K Jan  9 14:07 config.json\n",
      "-rw-rw-r-- 1 ospost ospost 4.5G Jan  9 14:07 model-00000-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ospost ospost 4.5G Jan  9 14:07 model-00001-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ospost ospost 3.9G Jan  9 14:07 model-00002-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ospost ospost  36K Jan  9 14:07 model.safetensors.index.json\n",
      "-rw-rw-r-- 1 ospost ospost  446 Jan  9 14:07 special_tokens_map.json\n",
      "-rw-rw-r-- 1 ospost ospost  20K Jan  9 14:07 tokenizer_config.json\n",
      "-rw-rw-r-- 1 ospost ospost  27M Jan  9 14:07 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lah gpt-oss-20b-sft-mxfp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e322e27-1cb9-4bac-bdb9-d4f410383e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/ospost/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00000-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:20<00:00, 46.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Regenerating safetensors index for dequantized MXFP4 model...\n",
      "Unsloth: Merge process complete. Saved to `/home/ospost/workspace/gpt-oss-20b-fine-tune/gpt-oss-20b-sft-bf16`\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\n",
    "    \"gpt-oss-20b-sft-bf16\",\n",
    "    tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ff9738-8882-4d93-8325-1a5cad90783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39G\n",
      "drwxrwxr-x 3 ospost ospost 4.0K Jan  9 14:10 .\n",
      "drwxrwxr-x 8 ospost ospost 4.0K Jan  9 14:08 ..\n",
      "drwxrwxr-x 3 ospost ospost 4.0K Jan  9 14:07 .cache\n",
      "-rw-rw-r-- 1 ospost ospost  15K Jan  9 14:07 chat_template.jinja\n",
      "-rw-rw-r-- 1 ospost ospost 1.9K Jan  9 14:07 config.json\n",
      "-rw------- 1 ospost ospost  16G Jan  9 14:08 model-00000-of-00002.safetensors\n",
      "-rw------- 1 ospost ospost  16G Jan  9 14:09 model-00001-of-00002.safetensors\n",
      "-rw------- 1 ospost ospost 8.3G Jan  9 14:10 model-00002-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ospost ospost  35K Jan  9 14:10 model.safetensors.index.json\n",
      "-rw-rw-r-- 1 ospost ospost  446 Jan  9 14:07 special_tokens_map.json\n",
      "-rw-rw-r-- 1 ospost ospost  20K Jan  9 14:07 tokenizer_config.json\n",
      "-rw-rw-r-- 1 ospost ospost  27M Jan  9 14:07 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lah gpt-oss-20b-sft-bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e2c0c8-9e7b-4c8f-8466-358366aae15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ospost/micromamba/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.06s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt-oss-20b-sft-bf16\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"  # or \"auto\"\n",
    ")\n",
    "\n",
    "next(model.parameters()).dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e06d8-9473-4301-983f-7f9d0e7faa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"ospost/gpt-oss-20b-sft-mxfp4\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d8c3b-237e-4e53-b9b9-0553edcec494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/ggerganov/llama.cpp\n",
    "# cd llama.cpp\n",
    "\n",
    "# python convert_hf_to_gguf.py \\\n",
    "#  gpt-oss-20b-sft-mxfp4 \\\n",
    "#  --outtype q4_K_M \\\n",
    "#  --outfile gpt-oss-20b-sft-q4_K_M.gguf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
