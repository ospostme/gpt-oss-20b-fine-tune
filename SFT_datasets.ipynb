{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2cb024-d891-45b9-a022-fb05bc6cdf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/home/zeus/miniconda3/envs/cloudspace/bin/python'}\n",
      "datasets                  4.3.0\n",
      "huggingface-hub           0.34.4\n",
      "numpy                     2.3.4\n",
      "nvidia-cuda-cupti-cu12    12.8.90\n",
      "nvidia-cuda-nvrtc-cu12    12.8.93\n",
      "nvidia-cuda-runtime-cu12  12.8.90\n",
      "pandas                    2.3.3\n",
      "pyarrow                   22.0.0\n",
      "pytorch-lightning         2.5.5\n",
      "tokenizers                0.22.1\n",
      "torch                     2.8.0\n",
      "torchao                   0.14.1\n",
      "torchmetrics              1.7.4\n",
      "torchvision               0.23.0\n",
      "tqdm                      4.67.1\n",
      "transformers              4.56.2\n",
      "triton                    3.4.0\n",
      "triton_kernels            1.0.0\n",
      "trl                       0.22.2\n",
      "unsloth                   2025.11.2\n",
      "unsloth_zoo               2025.11.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print({sys.executable})\n",
    "!{sys.executable} -m pip list | grep -E \"torch|cuda|datasets|pandas|tqdm|numpy|pyarrow|transformers|tokenizers|trl|unsloth|unsloth_zoo|huggingface-hub|datasets|numpy|triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d63591-ba51-4f4b-95ba-1639aea094c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton                    3.4.0\n",
      "triton_kernels            1.0.0\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip list  | grep triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bfed80-3836-49cb-a511-236b74eb81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-cuda-cupti-cu12    12.8.90\n",
      "nvidia-cuda-nvrtc-cu12    12.8.93\n",
      "nvidia-cuda-runtime-cu12  12.8.90\n",
      "Python 3.12.11\n"
     ]
    }
   ],
   "source": [
    "#!uv pip install --upgrade --no-deps bitsandbytes transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 tqdm==4.67.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa02244-705c-4c45-b5f1-7d9dbf90e087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uv\n",
      "  Downloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.9.8\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade  uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760799f8-74c2-4499-895c-939336c2939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.19 environment at: /opt/jupyter/envs/main\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m97 packages\u001b[0m \u001b[2min 11.50s\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m51 packages\u001b[0m \u001b[2min 44.40s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/70] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m70 packages\u001b[0m \u001b[2min 47.14s\u001b[0m\u001b[0m                             \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1masync-timeout\u001b[0m\u001b[2m==5.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.48.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcut-cross-entropy\u001b[0m\u001b[2m==25.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiffusers\u001b[0m\u001b[2m==0.35.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-transfer\u001b[0m\u001b[2m==0.1.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1miniconfig\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpluggy\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytest\u001b[0m\u001b[2m==9.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentencepiece\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshtab\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchao\u001b[0m\u001b[2m==0.14.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.24.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.56.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton-kernels\u001b[0m\u001b[2m==1.0.0 (from git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.24.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtypeguard\u001b[0m\u001b[2m==4.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyro\u001b[0m\u001b[2m==0.9.35\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munsloth\u001b[0m\u001b[2m==2025.11.3 (from git+https://github.com/unslothai/unsloth@aa7cfa1d3cd8f3fa7c0423ee227a7748c4228630)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munsloth-zoo\u001b[0m\u001b[2m==2025.11.3 (from git+https://github.com/unslothai/unsloth-zoo@09a56df874bf24faf43d61137c1a3ddbf55b6287)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!uv pip install \"triton>=3.4.0\" bitsandbytes \"transformers==4.56.2\" \\\n",
    "#\"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "#\"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "#        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de872fe-713f-4107-bf23-9b6722f0771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-warmed imports\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pre-warmed imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4297b7a-1a52-4b3a-a664-c3f805b31ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-lightning         2.5.5\n",
      "torch                     2.8.0\n",
      "torchao                   0.14.1\n",
      "torchmetrics              1.7.4\n",
      "torchvision               0.23.0\n",
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "pytorch-lightning         2.5.5\n",
      "torch                     2.8.0\n",
      "torchao                   0.14.1\n",
      "torchmetrics              1.7.4\n",
      "torchvision               0.23.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list | grep torch\n",
    "!{sys.executable} -m uv pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f0164d-06d7-4586-9968-5e54bf22d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python executable:\n",
      "/home/zeus/miniconda3/envs/cloudspace/bin/python\n",
      "\n",
      "Active env name: cloudspace\n",
      "\n",
      "Kernel spec info:\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Available kernels:\n",
      "  python3    /home/zeus/miniconda3/envs/cloudspace/share/jupyter/kernels/python3\n"
     ]
    }
   ],
   "source": [
    "import sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Current Python executable:\")\n",
    "print(sys.executable)\n",
    "\n",
    "kernel_name = Path(sys.prefix).name\n",
    "print(\"\\nActive env name:\", kernel_name)\n",
    "\n",
    "print(\"\\nKernel spec info:\")\n",
    "!jupyter kernelspec list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f317a94-7a59-4990-b4e1-c94ac7a74d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "Package                   Version\n",
      "------------------------- ------------\n",
      "absl-py                   2.3.1\n",
      "accelerate                1.11.0\n",
      "aiohappyeyeballs          2.6.1\n",
      "aiohttp                   3.13.2\n",
      "aiosignal                 1.4.0\n",
      "annotated-doc             0.0.3\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.11.0\n",
      "argon2-cffi               25.1.0\n",
      "argon2-cffi-bindings      25.1.0\n",
      "arrow                     1.4.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.4.0\n",
      "babel                     2.17.0\n",
      "backoff                   2.2.1\n",
      "beautifulsoup4            4.14.2\n",
      "bitsandbytes              0.48.2\n",
      "bleach                    6.3.0\n",
      "boto3                     1.40.63\n",
      "botocore                  1.40.63\n",
      "cachetools                6.2.1\n",
      "certifi                   2025.10.5\n",
      "cffi                      2.0.0\n",
      "charset-normalizer        3.4.4\n",
      "click                     8.3.0\n",
      "comm                      0.2.3\n",
      "contourpy                 1.3.3\n",
      "cut-cross-entropy         25.1.1\n",
      "cycler                    0.12.1\n",
      "datasets                  4.3.0\n",
      "debugpy                   1.8.17\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "diffusers                 0.35.2\n",
      "dill                      0.4.0\n",
      "docker                    7.1.0\n",
      "docstring-parser          0.17.0\n",
      "executing                 2.2.1\n",
      "fastapi                   0.120.3\n",
      "fastjsonschema            2.21.2\n",
      "filelock                  3.20.0\n",
      "fonttools                 4.60.1\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.8.0\n",
      "fsspec                    2025.9.0\n",
      "google-auth               2.41.1\n",
      "google-auth-oauthlib      1.2.3\n",
      "grpcio                    1.76.0\n",
      "h11                       0.16.0\n",
      "hf-transfer               0.1.9\n",
      "hf-xet                    1.2.0\n",
      "httpcore                  1.0.9\n",
      "httptools                 0.7.1\n",
      "httpx                     0.28.1\n",
      "huggingface-hub           0.34.4\n",
      "idna                      3.11\n",
      "importlib-metadata        8.7.0\n",
      "iniconfig                 2.3.0\n",
      "ipykernel                 6.26.0\n",
      "ipython                   8.17.2\n",
      "ipywidgets                8.1.1\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "jinja2                    3.1.6\n",
      "jmespath                  1.0.1\n",
      "joblib                    1.5.2\n",
      "json5                     0.12.1\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.25.1\n",
      "jsonschema-specifications 2025.9.1\n",
      "jupyter-client            8.6.3\n",
      "jupyter-core              5.9.1\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.3.0\n",
      "jupyter-server            2.17.0\n",
      "jupyter-server-terminals  0.5.3\n",
      "jupyterlab                4.2.0\n",
      "jupyterlab-pygments       0.3.0\n",
      "jupyterlab-server         2.28.0\n",
      "jupyterlab-widgets        3.0.15\n",
      "kiwisolver                1.4.9\n",
      "lark                      1.3.1\n",
      "lightning                 2.5.5\n",
      "lightning-cloud           0.5.70\n",
      "lightning-sdk             2025.10.31\n",
      "lightning-utilities       0.15.2\n",
      "litai                     0.0.10\n",
      "litdata                   0.2.58\n",
      "litserve                  0.2.16\n",
      "llvmlite                  0.45.1\n",
      "markdown                  3.9\n",
      "markdown-it-py            4.0.0\n",
      "markupsafe                3.0.3\n",
      "matplotlib                3.8.2\n",
      "matplotlib-inline         0.2.1\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.1.4\n",
      "mpmath                    1.3.0\n",
      "msgspec                   0.19.0\n",
      "multidict                 6.7.0\n",
      "multiprocess              0.70.16\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.5\n",
      "notebook-shim             0.2.4\n",
      "numba                     0.62.1\n",
      "numpy                     2.3.4\n",
      "nvidia-cublas-cu12        12.8.4.1\n",
      "nvidia-cuda-cupti-cu12    12.8.90\n",
      "nvidia-cuda-nvrtc-cu12    12.8.93\n",
      "nvidia-cuda-runtime-cu12  12.8.90\n",
      "nvidia-cudnn-cu12         9.10.2.21\n",
      "nvidia-cufft-cu12         11.3.3.83\n",
      "nvidia-cufile-cu12        1.13.1.3\n",
      "nvidia-curand-cu12        10.3.9.90\n",
      "nvidia-cusolver-cu12      11.7.3.90\n",
      "nvidia-cusparse-cu12      12.5.8.93\n",
      "nvidia-cusparselt-cu12    0.7.1\n",
      "nvidia-nccl-cu12          2.27.3\n",
      "nvidia-nvjitlink-cu12     12.8.93\n",
      "nvidia-nvtx-cu12          12.8.90\n",
      "oauthlib                  3.3.1\n",
      "obstore                   0.8.2\n",
      "packaging                 25.0\n",
      "pandas                    2.3.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.5\n",
      "patsy                     1.0.2\n",
      "peft                      0.17.1\n",
      "pexpect                   4.9.0\n",
      "pillow                    12.0.0\n",
      "pip                       25.3\n",
      "platformdirs              4.5.0\n",
      "pluggy                    1.6.0\n",
      "prometheus-client         0.23.1\n",
      "prompt-toolkit            3.0.52\n",
      "propcache                 0.4.1\n",
      "protobuf                  6.33.0\n",
      "psutil                    7.1.3\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.3\n",
      "pyarrow                   22.0.0\n",
      "pyasn1                    0.6.1\n",
      "pyasn1-modules            0.4.2\n",
      "pycparser                 2.23\n",
      "pydantic                  2.12.3\n",
      "pydantic-core             2.41.4\n",
      "pygments                  2.19.2\n",
      "pyjwt                     2.10.1\n",
      "pyparsing                 3.2.5\n",
      "pytest                    9.0.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-dotenv             1.2.1\n",
      "python-json-logger        4.0.0\n",
      "python-multipart          0.0.20\n",
      "pytorch-lightning         2.5.5\n",
      "pytz                      2025.2\n",
      "pyyaml                    6.0.3\n",
      "pyzmq                     27.1.0\n",
      "referencing               0.37.0\n",
      "regex                     2025.11.3\n",
      "requests                  2.32.5\n",
      "requests-oauthlib         2.0.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rfc3987-syntax            1.1.0\n",
      "rich                      14.2.0\n",
      "rpds-py                   0.28.0\n",
      "rsa                       4.9.1\n",
      "s3transfer                0.14.0\n",
      "safetensors               0.6.2\n",
      "scikit-learn              1.7.2\n",
      "scipy                     1.16.3\n",
      "send2trash                1.8.3\n",
      "sentencepiece             0.2.1\n",
      "setuptools                80.9.0\n",
      "shtab                     1.7.2\n",
      "simple-term-menu          1.6.6\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.8\n",
      "stack-data                0.6.3\n",
      "starlette                 0.49.1\n",
      "statsmodels               0.14.5\n",
      "sympy                     1.14.0\n",
      "tensorboard               2.15.1\n",
      "tensorboard-data-server   0.7.2\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.6.0\n",
      "tifffile                  2025.10.16\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.22.1\n",
      "torch                     2.8.0\n",
      "torchao                   0.14.1\n",
      "torchmetrics              1.7.4\n",
      "torchvision               0.23.0\n",
      "tornado                   6.5.2\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.56.2\n",
      "triton                    3.4.0\n",
      "triton-kernels            1.0.0\n",
      "trl                       0.22.2\n",
      "typeguard                 4.4.4\n",
      "typing-extensions         4.15.0\n",
      "typing-inspection         0.4.2\n",
      "tyro                      0.9.35\n",
      "tzdata                    2025.2\n",
      "unsloth                   2025.11.2\n",
      "unsloth-zoo               2025.11.3\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.5.0\n",
      "uv                        0.9.8\n",
      "uvicorn                   0.38.0\n",
      "uvloop                    0.22.1\n",
      "watchfiles                1.1.1\n",
      "wcwidth                   0.2.14\n",
      "webcolors                 25.10.0\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.9.0\n",
      "websockets                15.0.1\n",
      "werkzeug                  3.1.3\n",
      "wget                      3.2\n",
      "wheel                     0.45.1\n",
      "widgetsnbextension        4.0.14\n",
      "xformers                  0.0.32.post2\n",
      "xxhash                    3.6.0\n",
      "yarl                      1.22.0\n",
      "zipp                      3.23.0\n"
     ]
    }
   ],
   "source": [
    "!uv pip list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2b1b2f-f831-4dc0-84b7-bce5fa694160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install --upgrade --no-deps \\\n",
    "#    transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f9577-1c75-49c1-9b38-047107be83c9",
   "metadata": {},
   "source": [
    "# Before everything we start,  preapre the envionment \n",
    "\n",
    "## Best Practice to follow : \n",
    "\n",
    "- Use Python 3.10 in your runtime\n",
    "- Install all heavy binary libs from conda\n",
    "- Install pip-only pure python packages AFTER NumPy/Torch are settled\n",
    "- Install optional Jupyter kernel inside the environment\n",
    "- (Optional but recommended) Create a reproducible environment file\n",
    "- Use Docker instead If your server supports Docker + NVIDIA Container Toolkit, this is the most reproducible\n",
    "\n",
    "‚úÖ NEVER mix uv/pip with conda for GPU ML frameworks (PyTorch, CUDA, Triton, bnb, xformers). \n",
    "You will see such kind of installation on jupyter notebook tutorial, that's a different story.\n",
    "\n",
    "‚úÖ !uv pip install only gets the installation faster due to the uv.  It deson't support --dry-run to check if there are any related dependency packages will be updated.\n",
    "\n",
    "‚úÖ How to lock/freeze the enviornment \n",
    "\n",
    "environment.yml (micromamba/conda) is the most solid choice\n",
    "\n",
    "| Method                  | System Packages | Python Packages |  Sub-dependencies | Reproducible? |\n",
    "|-------------------------|-------------------------|-------------------------|---------------------------|---------------|\n",
    "| pip freeze              | ‚ùå                       | ‚úÖ                       | ‚ùå                         | Partial       |\n",
    "| pip-tools / Poetry       | ‚ùå                       | ‚úÖ                       | ‚úÖ                         | ‚úÖ            |\n",
    "| environment.yml  | ‚úÖ           | ‚úÖ                       | ‚úÖ                         | ‚úÖ            |\n",
    "\n",
    "\n",
    "‚úÖ In Jupyter notebook,  pip/uv pip install actually not install packages into the connected kernel python environment. \n",
    "!pip list show different results from terminal (after  activate the env of the kernel) ??\n",
    "\n",
    "if you used !pip inside the notebook, it sometimes runs the system pip, not the pip from the kernel environment. \n",
    "\n",
    "!pip is a shell command. It resolves pip from the PATH in the Jupyter server process.\n",
    "\n",
    "That PATH might point to your system Python (or Jupyter‚Äôs default env) not your kernel‚Äôs env.\n",
    "\n",
    "That PATH might point to your system Python (or Jupyter‚Äôs default env) not your kernel‚Äôs env.\n",
    "\n",
    "!pip in notebooks is not fully reliable; always use {sys.executable} -m pip to be sure.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "!{sys.executable} -m pip list\n",
    "!{sys.executable} -m pip install package-name\n",
    "!{sys.executable} -m uv pip list\n",
    "!{sys.executable} -m uv pip install some-package\n",
    "```\n",
    "\n",
    "‚úÖ You will see a lot `!pip install !uv pip install` in the tutorial in jupyter notebook, if you are using the default system kernel, it's fine.  But not the customized envionment\n",
    "\n",
    "\n",
    "## For strick production ML enviornment\n",
    "\n",
    "### Packages contain large compiled C/CUDA binaries, should comes from Conda/micomamba \n",
    "\n",
    "> ABI compatibility  Application Binary Interface, how compiled components talk to each other.\n",
    "> PyTorch links to NumPy C-API\n",
    "> pandas links to NumPy C-API\n",
    "> PyArrow links to libarrow + NumPy C-API\n",
    "> PyTorch + TorchVision + TorchAudio + CUDA must be built with same compiler, same runtime, same dependencies\n",
    "> *If any one of these comes from pip, you may get a wheel compiled against a different compiler, different NumPy ABI,\n",
    "> different glibc, etc ‚Üí binary mismatch errors*\n",
    "\n",
    "### Conda/micomamba solves dependency versions together\n",
    "\n",
    "conda/mamba/micomamba solves installation as a set so they are fully compatible.\n",
    "\n",
    "pip cannot solve binary dependencies; it installs whatever wheels exist.\n",
    "\n",
    "### Conda binaries are built with compatible compilers\n",
    "\n",
    "conda-forge and pytorch channels use:\n",
    "\n",
    "same clang/gcc\n",
    "\n",
    "same GLIBC\n",
    "\n",
    "same CUDA runtime ABI\n",
    "\n",
    "same Arrow C++ libs\n",
    "\n",
    "same NumPy C API version\n",
    "\n",
    "pip wheels might:\n",
    "\n",
    "be compiled with gcc 9, 10, or 11\n",
    "\n",
    "assume NumPy 1.x ABI instead of 2.x\n",
    "\n",
    "bundle their own incompatible libarrow\n",
    "\n",
    "So mixing pip wheels for heavy packages is dangerous.\n",
    "\n",
    "### packages \n",
    "\n",
    "Which packages must ALWAYS come from conda?\n",
    "\n",
    "(üî• Critical ‚Äî do NOT install via pip/uv)\n",
    "\n",
    "‚úÖ Heavy Compiled Packages\n",
    "Package\tWhy it must be from conda\n",
    "pytorch\tCUDA + C++ + MKL linked libs\n",
    "torchvision, torchaudio\tMust match PyTorch ABI\n",
    "pytorch-cuda\tCUDA runtime libs; must match PyTorch\n",
    "numpy\tCore C-API; other libs depend on it\n",
    "pandas\tCompiled against NumPy C-API\n",
    "pyarrow\tCompiles against Arrow + NumPy C-API\n",
    "scipy (if any)\tHuge C++ build, must match NumPy\n",
    "opencv (cv2)\tMust match system libs / NumPy ABI\n",
    "tqdm (optional)\tOK in pip but stable in conda\n",
    "\n",
    "Which packages can come from pip/uv (safe)\n",
    "\n",
    "Pure Python packages (no compiled code):\n",
    "\n",
    "transformers\n",
    "trl\n",
    "huggingface-hub\n",
    "datasets\n",
    "unsloth\n",
    "unsloth_zoo\n",
    "tqdm\n",
    "tokenizers ‚ùó ‚Üê C++ but HuggingFace wheels are safe with both pip/conda\n",
    "\n",
    "General rule:\n",
    "\n",
    "‚úÖ If it does NOT contain heavy C++/CUDA ‚Üí pip is OK\n",
    "‚ùå If it DOES contain C++/CUDA ‚Üí use conda\n",
    "\n",
    "\n",
    "## commands \n",
    "\n",
    "```cmd\n",
    "\n",
    "\"${SHELL}\" <(curl -L micro.mamba.pm/install.sh)\n",
    "\n",
    "./bin/micromamba shell init -s bash -r ~/micromamba\n",
    "\n",
    "# better yet, restart your terminal!\n",
    "source ~/.bashrc\n",
    "\n",
    "micromamba create -n unsloth -c conda-forge -c nvidia -c pytorch python=3.10.12 pytorch=2.8.0 torchvision torchaudio pytorch-cuda numpy pillow pyarrow pandas  scipy scikit-learn triton==3.4.0\n",
    "\n",
    "This locks the channel sources for the environment. afterward will automatically use  conda-forge ‚Üí nvidia  ‚Üí pytorch  in that order.\n",
    "\n",
    "micromamba activate unsloth\n",
    "  \n",
    "pip install --dry-run --no-deps transformers==4.56.2 tokenizers torchmetrics trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 tqdm==4.67.1\n",
    "\n",
    "micromamba env export > environment.yml\n",
    "\n",
    "pip freeze > requirements.txt\n",
    "\n",
    "python -m ipykernel install --name ml --display-name \"ML (python 3.10)\"\n",
    "\n",
    "```\n",
    "> If you are working on a standard server,  your customized ML stack is done.\n",
    "> BUT in a cloud VM environment,  things will be much more complicated.  As the storage has a lot of underlying tricks.\n",
    "> - Some of the storage locations are overlay filesystem of container image.  A Writable layer on top a read only layer.\n",
    ">   what your saw is conbination results of these. Any changes will be lost after next reboot. It's pretty fair with cloud\n",
    ">   environment, reboot/restart bring a fresh new workable bench.\n",
    ">   \n",
    "> - Some of the persistency storages are on a networked overlay filesystem, files are stored on persistent network storage,\n",
    ">   not a local SSD. Take lighning studio as an exmaple:\n",
    ">\n",
    ">   a. Every I/O operation (e.g., imports, reads, writes, temporary cache creation) is slower ‚Äî sometimes by seconds per call.\n",
    ">\n",
    ">   b. When the Studio wakes from ‚Äúsleep,‚Äù it reloads your environment. Your micromamba env may live on a slower layer (overlay),\n",
    ">   and python and libraries are reloaded into memory the first time you import them ‚Äî causing the ‚Äúlong first cell execution.‚Äù\n",
    ">\n",
    ">   Fix: pre-warm import torch, transformers \n",
    ">\n",
    ">   c. IPC lag. In Lightning AI, your Jupyter UI and kernel may sometimes run in separate containers ‚Äî especially after a restart\n",
    ">   or upgrade. If your kernel‚Äôs python path is /teamspace/studios/this_studio/micromamba/envs/unsloth/bin/python but the JupyterLab\n",
    ">   process runs in /opt/jupyter/envs/main, IPC (inter-process communication) may traverse a socket bridge, introducing latency.\n",
    ">\n",
    ">   \n",
    "\n",
    "## Lightning AI extra setting for the customized micomamba env kernel\n",
    "\n",
    "\n",
    "```cmd\n",
    "#mkdir -p /teamspace/studios/this_studio/.jupyter_kernels\n",
    "#~ python -m ipykernel install --name ml --display-name \"ML (python 3.10)\" --prefix=/teamspace/studios/this_studio/.jupyter_kernels\n",
    "\n",
    "ln -s /teamspace/studios/this_studio/.jupyter_kernels/share/jupyter/kernels/ml  /opt/jupyter/envs/main/share/jupyter/kernels/ml\n",
    "sudo systemctl restart jupyterlab.service\n",
    "\n",
    "```\n",
    "> JUPYTER_PATH is an environment variable that tells Jupyter to look for kernels (and other data files) in extra directories.\n",
    "> But that doesn't work as that can't reach into application container. (maybe, not verified)\n",
    "> Install to /teamspace/studios/this_studio/.local/share/jupyter also doens't work, as reload the jupyterlab service will restore\n",
    "> .local/share/jupyter as well.\n",
    "\n",
    "> Lightning AI tips: Manually customized studio start/stop operation. Like start a app, or clean storage before sleeping...\n",
    "\n",
    "```cmd\n",
    "environment.yml  gpt-oss-20b-fine-tune  main.py  micromamba  requirements.txt\n",
    "‚ö° ~ cat  .lightning_studio/on_start.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# This script runs every time your Studio starts, from your home directory.\n",
    "\n",
    "# Logs from previous runs can be found in ~/.lightning_studio/logs/\n",
    "\n",
    "# List files under fast_load that need to load quickly on start (e.g. model checkpoints).\n",
    "#\n",
    "# ! fast_load\n",
    "# <your file here>\n",
    "\n",
    "# Add your startup commands below.\n",
    "#\n",
    "# Example: streamlit run my_app.py\n",
    "# Example: gradio my_app.py\n",
    "echo \"[on_start] starting at $(date)\"\n",
    "echo \"[on_start] done at $(date)\"\n",
    "‚ö° ~ cat  .lightning_studio/on_stop.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# This script runs every time your Studio sleeps, from your home directory.\n",
    "\n",
    "# Logs from previous runs can be found in ~/.lightning_studio/logs/\n",
    "\n",
    "# Add your shutdown commands below.\n",
    "#\n",
    "# Example: docker down my-container\n",
    "# Example: sudo service mysql stop\n",
    "\n",
    "echo \"[on_stop] cleaning...\"\n",
    "rm -rf /teamspace/studios/this_studio/.cache\n",
    "rm -rf /teamspace/studios/this_studio/tmp_checkpoints\n",
    "echo \"[on_stop] done.\"\n",
    "```\n",
    "\n",
    "\n",
    "## Understand the errors\n",
    "\n",
    "### Why cannot import name 'Inf' from 'numpy' ‚Äî what changed in NumPy?\n",
    "\n",
    "NumPy 2.0 removed several legacy aliases ‚Äî np.Inf / np.NINF / np.Infinity / np.NaN (aliases) ‚Äî in favor of the lowercase forms (np.inf, np.nan). If code (or older compiled extensions) still expects np.Inf, import-time errors or attribute errors appear.\n",
    "\n",
    "Actionable: If you see cannot import name 'Inf' it‚Äôs from code or a compiled extension expecting the old alias. Fix options:\n",
    "\n",
    "Downgrade to a NumPy 1.x-compatible version (or use the exact NumPy version the wheel was built for), or\n",
    "\n",
    "Reinstall the dependent extension compiled against the NumPy you intend to use.\n",
    "\n",
    "### Why ValueError: numpy.dtype size changed happens (ABI / binary mismatch)\n",
    "\n",
    "This is not a Python-level logic bug ‚Äî it‚Äôs a binary compatibility problem between NumPy C-API and compiled extensions (Cython, C/C++ extensions such as those inside pyarrow, pandas, tokenizers, triton kernels). The community docs and many troubleshooting threads explain this is the classic symptom when packages were built against a different NumPy header than currently installed.\n",
    "\n",
    "Actionable (how to fix):\n",
    "\n",
    "Reinstall NumPy first (or install it from conda), then reinstall packages with C-extensions so they compile/bind against that NumPy.\n",
    "\n",
    "Better: create a fresh conda env (Python 3.10/3.11) and install binary packages (numpy/pandas/pyarrow/torch) via conda channels before pip installs.\n",
    "\n",
    "### Which ML components have trouble on Python 3.12?\n",
    "\n",
    "Triton: long-standing issues/PRs show Triton did not support 3.12 for a while; many projects depending on Triton (custom GPU kernels) are fragile on 3.12. \n",
    "GitHub\n",
    "+1\n",
    "\n",
    "PyTorch: historically lagged on official wheel support for 3.12; you must check the official PyTorch wheel index for current support (the PyTorch community threads discuss this). If the runtime doesn‚Äôt have appropriate PyTorch wheels for 3.12, installs fail or fall back to incompatible builds. \n",
    "\n",
    "bitsandbytes / tokenizers / other wheels: bitsandbytes requires specific Python/PyTorch/CUDA combinations; while PyPI may list Python 3.9+, there are reported issues and source-build workarounds for 3.12. Tokenizers and some Hugging Face pieces also saw 3.12-related \n",
    "\n",
    "Important nuance: Some package maintainers do publish 3.12 wheels over time. But uptake is uneven: not guaranteed for every compiled dependency you depend on. That‚Äôs why the conservative recommendation is to use Python 3.10 or 3.11 for ML work today.\n",
    "\n",
    "\n",
    "## Understand package manager \n",
    "\n",
    "### What's the difference between mamba and conda\n",
    "\n",
    "mamba = conda, but much faster.\n",
    "It uses the same package repositories, same commands, same environment format, but solves dependencies using a much faster backend (libsolv, in C++). It is a drop-in replacement for conda.\n",
    "\n",
    "1. Speed (the biggest difference)\n",
    "\n",
    "conda uses a Python-based dependency solver ‚Üí slow, especially for large ML stacks.\n",
    "\n",
    "mamba uses libsolv (C++), the same solver used by Fedora/openSUSE ‚Üí much faster dependency resolution.\n",
    "\n",
    "‚úÖ For real ML projects:\n",
    "Creating large environments with PyTorch, numpy, cudatoolkit, transformers, pyarrow, etc. can take minutes with conda and seconds with mamba.\n",
    "\n",
    "2. Compatibility\n",
    "\n",
    "mamba and conda use the same package format, the same channels, and the same environment specs.\n",
    "\n",
    "Any conda command can be replaced with mamba\n",
    "\n",
    "3. Dependency Solving Behavior\n",
    "\n",
    "Both use the same rules, but:\n",
    "\n",
    "conda may occasionally get stuck in long solve loops.\n",
    "\n",
    "mamba handles big/complex dependency graphs quickly and reliably due to libsolv.\n",
    "\n",
    "4. Installation Footprint\n",
    "\n",
    "mamba comes from conda-forge as:\n",
    "\n",
    "mamba (standalone replacement for conda commands)\n",
    "\n",
    "micromamba (even smaller, can bootstrap whole envs without installing conda)\n",
    "\n",
    "For remote GPU servers or Docker images, people often prefer micromamba to keep containers small.\n",
    "\n",
    "\n",
    "## micromamba\n",
    "\n",
    "micromamba is better for fresh ML environments, cloud servers, and production\n",
    "\n",
    "‚≠ê Reason 1 ‚Äî Pure, zero-bloat installation\n",
    "\n",
    "Conda installs hundreds of MB of packages even before you create an environment.\n",
    "Micromamba installs nothing except itself (a 2‚Äì3 MB binary).\n",
    "Perfect for:\n",
    "- Cloud VMs\n",
    "- Docker images\n",
    "- Remote GPU servers\n",
    "- Clean reproducible ML environments\n",
    "\n",
    "‚≠ê Reason 2 ‚Äî Safer + more reproducible\n",
    "\n",
    "Micromamba:\n",
    "\n",
    "- Does not auto-update\n",
    "- Does not auto-install extra conda garbage\n",
    "- Does not pollute your system\n",
    "- Works entirely inside a single folder (purely isolated)\n",
    "- This makes your ML environment exactly reproducible, which is critical for:\n",
    "\n",
    "‚úÖ CUDA correctness\n",
    "‚úÖ Compiled libs\n",
    "‚úÖ PyTorch environments\n",
    "\n",
    "‚≠ê Reason 3 ‚Äî Faster than both conda and mamba\n",
    "\n",
    "Micromamba has:\n",
    "\n",
    "- Instant startup\n",
    "- Faster solver for environment creation\n",
    "- Minimal overhead\n",
    "\n",
    "| Task                                         | Best choice    | Why                               |\n",
    "| -------------------------------------------- | -------------- | --------------------------------- |\n",
    "| Existing conda setup                         | **mamba**      | Drop-in replacement, easy         |\n",
    "| Fresh minimal system (Ubuntu VM, GPU server) | **micromamba** | Fast, small, reproducible         |\n",
    "| Docker images                                | **micromamba** | The industry standard             |\n",
    "| Long-term ML env stability                   | **micromamba** | No surprise updates               |\n",
    "| Local development with GUI tools             | mamba          | Sometimes easier on Windows/macOS |\n",
    "\n",
    "\n",
    "\n",
    "### what is conda-forge ?\n",
    "\n",
    "conda-forge is a community-driven collection of conda packages, completely separate from the commercial ‚ÄúAnaconda, Inc.‚Äù channels.\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "‚úÖ A community-maintained, open-source package repository\n",
    "‚úÖ Much larger and more up-to-date than the default Anaconda channel\n",
    "‚úÖ The standard source for modern ML, scientific, and data engineering stacks\n",
    "\n",
    "It‚Äôs basically the PyPI of the conda world‚Äîbut with compiled (pre-built) binaries.\n",
    "\n",
    "#### Why it's important\n",
    "\n",
    "1. Most modern ML packages ship to conda-forge first\n",
    "\n",
    "Examples:\n",
    "\n",
    "PyTorch\n",
    "HuggingFace transformers\n",
    "NumPy\n",
    "SciPy\n",
    "PyArrow\n",
    "Pandas\n",
    "JAX\n",
    "FastAPI\n",
    "UV, ripgrep, and many system-level tools\n",
    "\n",
    "Many packages don‚Äôt even exist in the ‚Äúdefaults‚Äù channel.\n",
    "\n",
    "2. More up-to-date than Anaconda‚Äôs default channel\n",
    "\n",
    "conda-forge maintains fast release cycles.\n",
    "\n",
    "Example:\n",
    "\n",
    "PyArrow in conda-forge releases within days.\n",
    "SciPy, NumPy, JAX usually appear in conda-forge first.\n",
    "\n",
    "3. Better dependency solving\n",
    "\n",
    "conda-forge tries to keep its entire ecosystem compatible:\n",
    "\n",
    "All packages pinned to consistent versions\n",
    "All use modern compilers/libraries\n",
    "Strong automated build infrastructure\n",
    "This dramatically reduces ‚Äúbinary mismatch‚Äù errors.\n",
    "\n",
    "4. The preferred channel for mamba\n",
    "\n",
    "mamba itself is published and maintained via conda-forge.\n",
    "\n",
    "Most mamba users run their entire environment using:\n",
    "\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - pytorch\n",
    "\n",
    "### what is uv\n",
    "\n",
    "uv is essentially a Python tool/environment manager. Think of it as a modern hybrid between:\n",
    "\n",
    "- pip (for installing Python packages)\n",
    "- venv / virtualenv (for creating isolated environments)\n",
    "- pipx (for installing CLI tools in isolated venvs)\n",
    "\n",
    "Modern, high-performance package manager and installer for Python, written in Rust. It's designed to be a significantly faster, all-in-one replacement for multiple traditional Python tools, including pip, pip-tools, and virtualenv.\n",
    "\n",
    "-  Blazing Fast Speed: Due to its implementation in Rust, uv is dramatically faster than pip‚Äîbenchmarks show it can be 10-100x faster for package installation and dependency resolution, especially when leveraging its built-in global cache.\n",
    "\n",
    "-  Drop-in pip and pip-tools Replacement: It offers a compatibility layer, meaning you can often swap out commands like pip install with uv pip install and immediately benefit from the speed improvements.\n",
    "\n",
    "-  Integrated Environment Management: It eliminates the need for a separate virtualenv or venv tool. uv can create, manage, and synchronize virtual environments directly, which is also significantly faster.\n",
    "\n",
    "-  Advanced Dependency Resolution: uv is designed with a more robust dependency resolver to better handle complex dependency conflicts and provide clearer error messages\n",
    "\n",
    "-  Universal Tooling: It aims to replace the functionality of numerous other tools like pipx (for installing command-line tools) and even offers features for Python version management (similar to pyenv).\n",
    "\n",
    "-  Reproducibility: It natively supports the creation of lockfiles (uv.lock or a requirements.txt equivalent via uv pip compile), which pin the exact versions of all transitive dependencies to ensure consistent environments across all machines.\n",
    "\n",
    "\n",
    "\n",
    "#### Core features of uv\n",
    "\n",
    "\n",
    "Create a Project/Env\n",
    "    uv init <project-name>\n",
    "    \n",
    "    nitializes a new project with a virtual environment and a pyproject.toml file.\n",
    "    \n",
    "Create a Venv\n",
    "    uv venv\n",
    "    \n",
    "    Creates a virtual environment in the current directory (.venv).\n",
    "    \n",
    "Install Packages\n",
    "    uv pip install <package-name>\n",
    "    \n",
    "    Use the pip compatible interface for installing packages.\n",
    "    \n",
    "Add a Dependency\n",
    "    uv add <package-name>\n",
    "    \n",
    "    Adds a package to your pyproject.toml and updates the lockfile/environment.\n",
    "    \n",
    "Compile Dependencies\n",
    "    uv pip compile requirements.in -o requirements.txt\n",
    "    uv pip compile pyproject.toml -o uv.lock\n",
    "    \n",
    "    Generates a lockfile (requirements.txt) with pinned versions.\n",
    "    \n",
    "Sync Environment\n",
    "    uv pip sync requirements.txt \n",
    "    \n",
    "    Synchronizes the virtual environment with the exact versions in the lockfile.\n",
    "    replacement for pip install -r requirements.txt, but faster\n",
    "    \n",
    "Sync Environment\n",
    "    uv sync\n",
    "\n",
    "    The uv sync command (without specifying a file) assumes you are in a project directory and automatically looks for a generated lockfile (like uv.lock or requirements.txt) to synchronize the environment with the source of truth defined by the pyproject.toml file.\n",
    "    \n",
    "Run a Script/Command\n",
    "    uv run python script.py\n",
    "    \n",
    "    Executes a command within the project's virtual environment *(related with current dir)*.\n",
    "\n",
    "## Why Colab scirpts resinstall numpy and pillow ? \n",
    "\n",
    "Because Google Colab already ships preinstalled versions of NumPy & Pillow that are ABI-incompatible with newer PyTorch wheels.\n",
    "\n",
    "This causes runtime crashes like:\n",
    "- ValueError: numpy.dtype size changed\n",
    "- cannot import name 'Inf' from numpy\n",
    "- binary incompatibility\n",
    "- C-extension import errors\n",
    "\n",
    "Colab's system NumPy/Pillow were compiled with different C compilers / binary formats than the new PyTorch wheels you install.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "If you install new PyTorch without matching these libs ‚Üí Python crashes.\n",
    "\n",
    "‚úÖ Why NumPy + Pillow breaking your environment matters\n",
    "\n",
    "These two packages are special:\n",
    "\n",
    "NumPy \n",
    "- The entire scientific stack depends on it\n",
    "- Has compiled C and Avx2 routines\n",
    "- ABI changes frequently\n",
    "- If one pkg links to NumPy 2.0 but you have 1.26 ‚Üí crash\n",
    "\n",
    "Pillow\n",
    "\n",
    "- Image decoding\n",
    "- Has C libraries (libjpeg, zlib)\n",
    "- PyTorch vision uses Pillow\n",
    "- Mismatch breaks dataset loading\n",
    "\n",
    "‚úÖ Why Unsloth specifically cares\n",
    "\n",
    "Unsloth uses:\n",
    "\n",
    "PyTorch\n",
    "Triton\n",
    "bitsandbytes\n",
    "tokenizers\n",
    "datasets (pyarrow)\n",
    "\n",
    "This stack is extremely sensitive to:\n",
    "\n",
    "- Python version\n",
    "- NumPy ABI\n",
    "- Pillow ABI\n",
    "- CUDA runtime\n",
    "\n",
    "Re-installing NumPy & Pillow ensures everything is built with the same ABI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5490360b-f787-41fb-a5d0-cc3c9562fb44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 numpy==2.3.4 pandas==2.3.3 pyarrow==22.0.0 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445d65cf-5d7e-4ac6-aba8-ea3bc63b6e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets                  4.3.0\n",
      "huggingface-hub           0.34.4\n",
      "numpy                     2.3.4\n",
      "pandas                    2.3.3\n",
      "pyarrow                   22.0.0\n",
      "tokenizers                0.22.1\n",
      "tqdm                      4.67.1\n",
      "transformers              4.56.2\n",
      "trl                       0.22.2\n",
      "unsloth                   2025.11.2\n",
      "unsloth_zoo               2025.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E \"datasets|pandas|tqdm|numpy|pyarrow|transformers|tokenizers|trl|unsloth|unsloth_zoo|huggingface-hub|datasets|numpy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1277b61f-f0c5-4d71-851e-94b8c08bbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db8016aa-48d5-4f1f-9d89-b9d2081a10e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe3b0a8945b452db9759d531eb3c616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51802737-18c7-4b3e-9edc-7debd0ac77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Starting dataset loading process and inspection...\n",
      "\n",
      "-> Loading and selecting 15% randomly from databricks/databricks-dolly-15k (train)\n",
      "\n",
      "-> Loading and selecting 1% randomly from HuggingFaceH4/ultrachat_200k (train_sft)\n",
      "\n",
      "-> Loading and selecting 40% randomly from open-thoughts/open-thoughts-114k (train)\n",
      "\n",
      "-> Loading and selecting 5% randomly from tatsu-lab/alpaca (train)\n",
      "\n",
      "-> Loading and selecting 30% randomly from google/boolq (train)\n",
      "\n",
      "-> Loading and selecting 8% randomly from lmsys/chatbot_arena_conversations (train)\n",
      "\n",
      "üß† Applying OpenThoughts formatter...\n",
      "\n",
      "üß† Applying gsm8k formatter...\n",
      "\n",
      "üß† Applying LMSYS winner extraction logic...\n",
      "\n",
      "üß† Applying dolly formatter...\n",
      "\n",
      "üß† Applying ultrachat formatter...\n",
      "\n",
      "üß† Formatting math formatter...\n",
      "\n",
      "üß† Formatting alpaca formatter...\n",
      "\n",
      "üß† Formatting boolq formatter...\n",
      "\n",
      "üß† Formatting arc formatter...\n",
      "\n",
      "üß† Formatting arc formatter...\n",
      "üóëÔ∏è Applying quality filter...\n",
      "\n",
      "‚úÖ Final dataset successfully filtered and shuffled.\n",
      "--- Entry 1 ---\n",
      "Message with chat template: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
      "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
      "\n",
      "Your task is to:\n",
      "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
      "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
      "  but clearly distinguish between context-based and general reasoning.\n",
      "\n",
      "Your responses must be:\n",
      "- Structured ‚Äî use clear formatting and logical reasoning.\n",
      "- Contextual ‚Äî rely only on the information available.\n",
      "- Concise ‚Äî eliminate filler words while preserving precision.\n",
      "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
      "<|end|><|start|>user<|message|>Compile a visually appealing list of at least ten distinctive dips and gravies that can be paired with a range of dishes prepared on your electric griddle. Make sure to include both sweet and savory options, as well as dips that cater to different dietary restrictions such as vegan, gluten-free, or low-fat. Additionally, provide brief descriptions of each dip or gravy highlighting its key ingredients, flavor profile, and suggested griddle dishes to accompany it.<|end|><|start|>assistant<|message|>1. Caramelized Onion and Garlic Dip - This savory dip pairs perfectly with breakfast dishes such as eggs, bacon, and pancakes or can be used as a topping for burgers and sandwiches. It is made with caramelized onions, garlic, cream cheese, and sour cream, and has a sweet and tangy flavor.\n",
      " \n",
      "2. Spicy Avocado Dip - This vegan option is perfect for those looking for a healthy dip option. Made with ripe avocados, jalapenos, lime juice, and cilantro, it is a great accompaniment for veggie fajitas or as a topping for grilled chicken or fish.\n",
      "\n",
      "3. Sweet and Sour Dipping Sauce - This tangy sauce is great for dipping crispy chicken tenders or fried shrimp. Made with ketchup, brown sugar, apple cider vinegar, soy sauce, and garlic, it has a sweet and slightly sour flavor that complements fried foods well.\n",
      "\n",
      "4. Bacon and Cheddar Dip - This savory dip pairs well with baked potatoes and grilled meats. Made with chopped bacon, cheddar cheese, sour cream, and Worcestershire sauce, it has a rich and creamy texture with a smoky flavor.\n",
      "\n",
      "5. Chipotle Mayo Dip - This spicy dip is perfect for dipping sweet potato fries or as a topping for burgers or sandwiches. Made with mayonnaise, chipotle chilies, lime juice, and garlic, it has a smoky and spicy flavor.\n",
      "\n",
      "6. Cheesy Queso Dip - This gluten-free dip is great for dipping tortilla chips or as a topping for nachos. Made with Velveeta cheese, diced tomatoes, and green chilies, it has a creamy and cheesy flavor with a slight kick.\n",
      "\n",
      "7. Low-Fat Ranch Dip - This healthy dip is made with Greek yogurt, garlic, and fresh herbs, and is a great pairing for grilled veggies, chicken, or fish.\n",
      "\n",
      "8. Roasted Red Pepper Dip - This vegan dip is a great pairing for grilled vegetables, flatbreads, or grilled chicken. Made with roasted red peppers, garlic, and olive oil, it has a smoky and slightly sweet flavor.\n",
      "\n",
      "9. Cilantro Lime Aioli - This low-fat dip is great for dipping crunchy onion rings or as a topping for grilled chicken or fish. Made with Greek yogurt, lime juice, and fresh cilantro, it has a tangy and slightly spicy flavor.\n",
      "\n",
      "10. Chocolate Ganache - This sweet dip is perfect for dipping fresh fruit or as a topping for pancakes, waffles, or French toast. Made with dark chocolate and heavy cream, it has a rich and decadent chocolate flavor.<|return|>...\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Entry 2 ---\n",
      "Message with chat template: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
      "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
      "\n",
      "Your task is to:\n",
      "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
      "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
      "  but clearly distinguish between context-based and general reasoning.\n",
      "\n",
      "Your responses must be:\n",
      "- Structured ‚Äî use clear formatting and logical reasoning.\n",
      "- Contextual ‚Äî rely only on the information available.\n",
      "- Concise ‚Äî eliminate filler words while preserving precision.\n",
      "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
      "<|end|><|start|>user<|message|>Please write a C++ function that takes an array as input and finds the length of the longest continuous subsequence in the array where each number is greater than the previous one. The function should return an integer which represents the length of the longest increasing subsequence. Please ensure that your function takes into account edge cases where the array is empty or has only one element. Please make sure to use appropriate variable names and comments where necessary for clarity.<|end|><|start|>assistant<|message|>Here's a possible implementation of the function:\n",
      "\n",
      "```c++\n",
      "int longestIncreasingSubsequence(int* arr, int size) {\n",
      "    if (size < 2) { // edge cases\n",
      "        return size; // either 0 or 1\n",
      "    }\n",
      "\n",
      "    int maxLength = 1; // initialize with 1 (the minimum)\n",
      "    int currentLength = 1; // the length of the current subsequence\n",
      "\n",
      "    for (int I = 1; I < size; i++) {\n",
      "        if (arr[i] > arr[i-1]) { // the next element is greater\n",
      "            currentLength++; // extend the subsequence\n",
      "        } else { // the next element is not greater\n",
      "            maxLength = max(maxLength, currentLength); // update the max length\n",
      "            currentLength = 1; // reset the current length\n",
      "        }\n",
      "    }\n",
      "\n",
      "    return max(maxLength, currentLength); // don't forget the last subsequence\n",
      "}\n",
      "```\n",
      "\n",
      "Here's how the function works:\n",
      "\n",
      "1. It first checks if the array is empty or has only one element, in which case it returns the size of the array (0 or 1).\n",
      "2. It initializes two variables: one to keep track of the current length of the increasing subsequence (starts at 1) and one to keep track of the maximum length (also starts at 1, as the first element is always an increasing subsequence).\n",
      "3. It then iterates over the array, comparing each element with the previous one. If the next element is greater, it extends the current subsequence by incrementing the length. If it's not greater, it means the current subsequence ends, so the function updates the maximum length with the current length (if it's greater) and resets the current length to 1, as the next element could be the start of a new subsequence.\n",
      "4. After the loop, the function returns the maximum length between the maximum length found so far and the last current length (which could be a subsequence at the end of the array).\n",
      "\n",
      "Note that this implementation has a time complexity of O(n) and a space complexity of O(1), as it only uses a constant amount of additional memory. Also, the input array is assumed to be sorted in increasing order. If it's not, the function could be modified to sort the array first, but that would increase the time complexity to O(n log n).<|return|>...\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Entry 3 ---\n",
      "Message with chat template: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-11-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.\n",
      "Calls to these tools must go to the commentary channel: 'functions'.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
      "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
      "\n",
      "Your task is to:\n",
      "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
      "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
      "  but clearly distinguish between context-based and general reasoning.\n",
      "\n",
      "Your responses must be:\n",
      "- Structured ‚Äî use clear formatting and logical reasoning.\n",
      "- Contextual ‚Äî rely only on the information available.\n",
      "- Concise ‚Äî eliminate filler words while preserving precision.\n",
      "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
      "<|end|><|start|>user<|message|>Return your final response within \\boxed{}. The number of points that are equidistant from both a circle and its two parallel tangents is\n",
      "(A) 0.\n",
      "(B) 2.\n",
      "(C) 3.\n",
      "(D) 4.\n",
      "(E) Infinitely many.\n",
      "(The 20th Annual American High School Mathematics Examination, 1969)<|end|><|start|>assistant<|message|>Okay, let's see. The problem is asking about the number of points that are equidistant from both a circle and its two parallel tangents. The options are from 0 to infinitely many. Hmm. I need to visualize this first.\n",
      "\n",
      "So, imagine a circle. Let me recall that a tangent to a circle is a line that touches the circle at exactly one point. If there are two parallel tangents, then the circle must be between these two lines, right? Since parallel lines never meet, the distance between them must be twice the radius of the circle. Wait, is that true?\n",
      "\n",
      "Wait, no. If the circle is in between two parallel tangent lines, then the distance between the two lines would be twice the diameter, right? Because the tangent lines are each a distance of the radius away from the center, but on opposite sides. Wait, maybe I need to think more carefully.\n",
      "\n",
      "Let me sketch this in my mind. Suppose the circle has center O and radius r. The two parallel tangent lines must be on opposite sides of the circle. For a circle, the distance from the center to a tangent line is equal to the radius. So, if there are two parallel tangents, one on each side of the circle, then the distance between these two lines is 2r. Because each tangent is r distance away from the center, and since they're parallel, the total distance between them is 2r. Got that.\n",
      "\n",
      "Now, the question is about points equidistant from the circle and both of these two parallel tangents. Wait, equidistant from the circle and both tangents? Wait, the points must be equidistant to the circle and to each of the two tangent lines? Or equidistant to the circle and to both tangents collectively? Hmm, the wording is a bit ambiguous, but I think it's equidistant to the circle and to both of the two parallel tangents. Wait, that might not make sense. Because the two tangents are on opposite sides of the circle, so a point can't be equidistant to both tangents unless it's somewhere between them. But the circle is in between them. So maybe the problem is asking for points that are equidistant to the circle and to each of the two tangents. Wait, that might not be possible. Let me parse the question again.\n",
      "\n",
      "\"The number of points that are equidistant from both a circle and its two parallel tangents is...\"\n",
      "\n",
      "Wait, \"equidistant from both a circle and its two parallel tangents.\" So, the point is equidistant from the circle and from both of the two parallel tangents. So, the point must have equal distance to the circle and to each tangent line? Wait, but the two tangent lines are parallel, so a point equidistant to both tangents would lie on the line midway between them. Since the distance between the tangents is 2r, the midway line is at distance r from each tangent, which is the same as the radius. Wait, so the center of the circle is on this midway line, and the distance from the center to each tangent is r. But the circle itself is at radius r from the center, so points on the circle are at distance zero from the circle (since they are on it) and distance r from the tangent lines. But the problem is asking for points equidistant from the circle and the two tangents. Hmm.\n",
      "\n",
      "Wait, perhaps the points need to be equidistant to the circle and to both tangent lines. So, the distance from the point to the circle is equal to the distance from the point to each tangent line. Wait, but the distance from a point to a circle is the minimal distance to the circle, which would be |distance from point to center minus radius|. Whereas the distance from the point to each tangent line is a fixed value if the point is equidistant to both tangents. Let me formalize this.\n",
      "\n",
      "Let me set up a coordinate system. Let's assume the circle is centered at (0, 0) with radius r. The two parallel tangent lines would then be horizontal lines y = r and y = -r, since the distance from the center to each tangent is r. Wait, but if the circle is centered at (0,0), then the horizontal tangents would be y = r and y = -r. The distance between these two lines is 2r.\n",
      "\n",
      "Now, we need to find points equidistant from the circle and both tangents. Since the two tangents are y = r and y = -r, the set of points equidistant from both tangents would lie on the lines midway between them. The midway line between y = r and y = -r is y = 0, which is the x-axis. So, any point equidistant from both tangents must lie on the x-axis.\n",
      "\n",
      "But we also need these points to be equidistant from the circle. The distance from a point (x, 0) on the x-axis to the circle would be the minimal distance from (x, 0) to any point on the circle. The minimal distance is |distance from (x,0) to the center (0,0) minus the radius|. The distance from (x,0) to the center is |x|. So, the minimal distance to the circle is ||x| - r|.\n",
      "\n",
      "On the other hand, the distance from (x,0) to each tangent line y = r and y = -r is r, since the vertical distance from (x,0) to y = r is r, and similarly to y = -r is also r. Wait, but if the point is on the x-axis, then the distance to both tangents is r. So, the distance from any point on the x-axis to the two tangents is always r. Therefore, we need points on the x-axis where the distance to the circle is also r.\n",
      "\n",
      "So, set ||x| - r| = r. Let's solve this equation.\n",
      "\n",
      "Case 1: |x| - r = r => |x| = 2r. So, x = 2r or x = -2r.\n",
      "\n",
      "Case 2: -(|x| - r) = r => -|x| + r = r => -|x| = 0 => |x| = 0. So, x = 0.\n",
      "\n",
      "Therefore, the points on the x-axis that are equidistant from the circle and the two tangents are at (2r, 0), (-2r, 0), and (0,0). So, three points. Therefore, the answer is C) 3.\n",
      "\n",
      "Wait, but let me verify. The point (0,0) is the center. The distance from (0,0) to the circle is the radius r (since the center is inside the circle, the minimal distance to the circle would be r - 0 = r). The distance from (0,0) to each tangent is r. So, yes, (0,0) is equidistant to the circle and the tangents.\n",
      "\n",
      "The points (2r, 0) and (-2r, 0) are outside the circle. The distance from (2r, 0) to the circle is |2r - r| = r. The distance to each tangent is r. Similarly for (-2r, 0). So those three points satisfy the condition. Therefore, the answer should be 3, which is option C.\n",
      "\n",
      "But let me check again. Maybe there are more points. Wait, the problem says \"points equidistant from both a circle and its two parallel tangents\". So, the points must be equidistant to the circle and to both tangents. But in our analysis, we restricted to points on the x-axis, since they have to be equidistant to both tangents. Because the two tangents are parallel and on opposite sides, the set of points equidistant to both is the midline between them, which is the x-axis. Therefore, all candidate points must lie on the x-axis. Then, on the x-axis, we found three points: center and two points at distance 2r from the center.\n",
      "\n",
      "But wait, let's think if there are points not on the x-axis. Wait, but if a point is not on the x-axis, then its distance to the two tangents would not be equal. For example, take a point above the x-axis. Its distance to the upper tangent y = r would be less than its distance to the lower tangent y = -r. Similarly, a point below the x-axis would be closer to the lower tangent. Therefore, the only points equidistant to both tangents are on the x-axis. So, indeed, the only possible points are on the x-axis. Therefore, we only need to consider points on the x-axis. And there we found three points. So, the answer should be C) 3.\n",
      "\n",
      "But wait, let's consider the point (0,0). The distance from (0,0) to the circle is r (since it's the center, the minimal distance to the circle is the radius). The distance from (0,0) to each tangent is r. So, yes, (0,0) is equidistant to the circle and the tangents.\n",
      "\n",
      "The points (2r, 0) and (-2r, 0): distance to the circle is |2r - r| = r, and distance to each tangent is r. So, those two points work too.\n",
      "\n",
      "Is there any other point on the x-axis where ||x| - r| = r? Let's check. Solving ||x| - r| = r:\n",
      "\n",
      "If |x| >= r, then |x| - r = r => |x| = 2r, so x = ¬±2r.\n",
      "\n",
      "If |x| < r, then -( |x| - r ) = r => -|x| + r = r => |x| = 0 => x = 0.\n",
      "\n",
      "Therefore, only three solutions. So, only three points. So, answer is C) 3.\n",
      "\n",
      "But wait, let me think differently. Suppose the circle is not between the two parallel tangents. Wait, but if the two lines are parallel tangents to the circle, then they must be on opposite sides of the circle. Otherwise, if they are on the same side, the lines would have to be coinciding to both be tangent, which is not possible for parallel lines. So, the two parallel tangents must be on opposite sides, separated by 2r.\n",
      "\n",
      "Alternatively, if the circle is, say, between two vertical parallel tangents, then similar reasoning applies. The set of points equidistant to both tangents would be the vertical line halfway between them, which would pass through the center of the circle, and same thing: three points. So, regardless of orientation, the answer is three.\n",
      "\n",
      "Wait, but maybe in 3D, but the problem is in plane. So, yes, three points.\n",
      "\n",
      "Wait, but maybe I missed something. Let me think again.\n",
      "\n",
      "Suppose the circle is centered at (0,0) with radius r. The two parallel tangents are y = r and y = -r.\n",
      "\n",
      "Set of points equidistant to both tangents: y = 0.\n",
      "\n",
      "On y=0, find points where distance to circle is equal to distance to tangent (which is r). The distance from a point (x,0) to the circle is the minimal distance to the circle, which is |sqrt(x¬≤ + 0¬≤) - r| = | |x| - r |.\n",
      "\n",
      "Set this equal to r: | |x| - r | = r.\n",
      "\n",
      "Solutions:\n",
      "\n",
      "Case 1: |x| - r = r => |x| = 2r => x = ¬±2r.\n",
      "\n",
      "Case 2: -(|x| - r) = r => -|x| + r = r => |x| = 0 => x=0.\n",
      "\n",
      "Thus, three points: (0,0), (2r,0), (-2r,0). Therefore, 3 points. So, answer is C.\n",
      "\n",
      "But let me check with a specific example. Let r = 1. Then the circle is x¬≤ + y¬≤ = 1. The tangents are y=1 and y=-1.\n",
      "\n",
      "Points equidistant to both tangents lie on y=0. On y=0, the distance to the circle is | |x| - 1 |. Set this equal to 1.\n",
      "\n",
      "Solutions:\n",
      "\n",
      "| |x| - 1 | = 1.\n",
      "\n",
      "Case 1: |x| - 1 = 1 => |x| = 2 => x=2 or x=-2.\n",
      "\n",
      "Case 2: -(|x| -1 ) =1 => -|x| +1 =1 => |x|=0 => x=0.\n",
      "\n",
      "Thus, points (0,0), (2,0), (-2,0). Indeed three points. So, in this case, three points. Therefore, the answer is C)3.\n",
      "\n",
      "But wait, let me check if (0,0) is actually equidistant to the circle and the tangents. The distance from (0,0) to the circle is the radius, which is 1. The distance from (0,0) to the tangents y=1 and y=-1 is 1. So, yes, it is equidistant.\n",
      "\n",
      "For (2,0): distance to circle is |2 -1|=1, and distance to the tangents is 1. Similarly for (-2,0). So, all three points satisfy.\n",
      "\n",
      "Therefore, the answer is C.\n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{C}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To determine the number of points equidistant from both a circle and its two parallel tangents, we start by considering a circle with center \\(O\\) and radius \\(r\\). The two parallel tangents are on opposite sides of the circle, each at a distance \\(r\\) from the center. The distance between these two tangents is \\(2r\\).\n",
      "\n",
      "The set of points equidistant from both tangents lies on the line midway between them, which is the line passing through the center of the circle. For a circle centered at \\((0, 0)\\) with radius \\(r\\), the tangents are \\(y = r\\) and \\(y = -r\\), and the midway line is \\(y = 0\\) (the x-axis).\n",
      "\n",
      "Next, we need to find points on the x-axis that are also equidistant from the circle. The distance from a point \\((x, 0)\\) on the x-axis to the circle is given by \\(||x| - r|\\). This distance must equal the distance from the point to the tangents, which is \\(r\\).\n",
      "\n",
      "Solving the equation \\(||x| - r| = r\\):\n",
      "\n",
      "1. **Case 1**: \\(|x| - r = r\\)\n",
      "   - \\(|x| = 2r\\)\n",
      "   - Solutions: \\(x = 2r\\) or \\(x = -2r\\)\n",
      "\n",
      "2. **Case 2**: \\(- (|x| - r) = r\\)\n",
      "   - \\(-|x| + r = r\\)\n",
      "   - \\(|x| = 0\\)\n",
      "   - Solution: \\(x = 0\\)\n",
      "\n",
      "Thus, the points \\((2r, 0)\\), \\((-2r, 0)\\), and \\((0, 0)\\) are equidistant from both the circle and the two parallel tangents. These points are verified by checking their distances to the circle and the tangents.\n",
      "\n",
      "Therefore, the number of such points is \\(\\boxed{C}\\).<|return|>...\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "üìä Analyzing Sources in Final Dataset...\n",
      "--------------------------------------------------\n",
      "Final Dataset Size: 29692 entries\n",
      "Total Unique Sources Found: 10\n",
      "--------------------------------------------------\n",
      "Count and Percentage of Data Entries per Source:\n",
      "              Count Percentage\n",
      "OpenThoughts   9987     33.64%\n",
      "GSM8K          7473     25.17%\n",
      "sharegpt4      3582     12.06%\n",
      "boolq          1682      5.66%\n",
      "lmsys_conv     1537      5.18%\n",
      "alpaca         1529      5.15%\n",
      "dolly          1303      4.39%\n",
      "ultrachat      1197      4.03%\n",
      "math            746      2.51%\n",
      "arc             656      2.21%\n",
      "--------------------------------------------------\n",
      "\n",
      "üéâ SUCCESS! Final SFT dataset created: train_sft_final.jsonl\n",
      "Total samples after filtering: 29692\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2Ô∏è‚É£ data format function for different Datasets\n",
    "# ================================================\n",
    "\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "INSPECT_ON = False\n",
    "\n",
    "# Prints ONE random entry\n",
    "def inspect_random_entry(ds_name, ds_object, force_disp = False):\n",
    "\n",
    "    if not force_disp:\n",
    "        if not INSPECT_ON:\n",
    "            return \n",
    "    \n",
    "    \"\"\"Randomly selects and prints one raw sample from the dataset for observation.\"\"\"\n",
    "    print(f\"\\n--- INSPECTION: {ds_name} (ONE RANDOM RAW SAMPLE) ---\")\n",
    "    random_index = random.randint(0, len(ds_object) - 1)\n",
    "    sample = ds_object[random_index]\n",
    "    print(f\"Sample Index: {random_index} Raw Data:\")\n",
    "    print(json.dumps(sample, indent=2, default=str))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "# Prints parts of the formatted datasets\n",
    "def inspect_formatted_sft(ds_name, ds_object, n = 2, disp_len = 50, force_disp = False):\n",
    "\n",
    "    if not force_disp:\n",
    "        if not INSPECT_ON:\n",
    "            return \n",
    "    \n",
    "    for i, entry in enumerate(ds_object):\n",
    "        if i > n:\n",
    "            break\n",
    "        print(f\"--- Entry {i+1} ---\")\n",
    "        print(f\"Instruction: {entry['instruction'][:disp_len]}...\") # Truncating for readability\n",
    "        print(f\"Response: {entry['response'][:disp_len]}\")\n",
    "        print(f\"Source: {entry['source']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "#  EXTRACTION FUNCTIONS. Convert various of datasets to  { \"instruction\": \"xxx\", \"response\": \"xxx\", \"source\": \"xxx\" }\n",
    "def extract_lmsys_winner(example):\n",
    "    \"\"\"\n",
    "    Identifies the winning conversation (model_a or model_b) and extracts the \n",
    "    first instruction/response pair from that conversation list.\n",
    "    \"\"\"\n",
    "    winner_key = example.get('winner')\n",
    "    \n",
    "    if winner_key == 'model_a':\n",
    "        conv_list = example.get('conversation_a')\n",
    "    elif winner_key == 'model_b':\n",
    "        conv_list = example.get('conversation_b')\n",
    "    else: # Handles 'tie', 'tie (a/b)', or missing winner by defaulting to A\n",
    "        conv_list = example.get('conversation_a')\n",
    "        \n",
    "    if not conv_list or len(conv_list) < 2:\n",
    "        return {\"instruction\": None, \"response\": None}\n",
    "\n",
    "    instruction = conv_list[0]['content'] if conv_list[0]['role'] == 'user' else None\n",
    "    response = conv_list[1]['content'] if conv_list[1]['role'] == 'assistant' else None\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"lmsys_conv\"}\n",
    "\n",
    "\n",
    "def format_ultrachat(example):\n",
    "    instruction, response = None, None\n",
    "    \n",
    "    user_messages = [m['content'] for m in example['messages'] if m['role'] == 'user']\n",
    "    model_messages = [m['content'] for m in example['messages'] if m['role'] == 'assistant']\n",
    "    instruction = user_messages[0] if user_messages else None\n",
    "    response = model_messages[0] if model_messages else None\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"ultrachat\"}\n",
    "\n",
    "\n",
    "def format_dolly(example):\n",
    "    \"\"\"\n",
    "    Formats a single Dolly 15k entry, integrating 'context' into the 'instruction'\n",
    "    when available. Assumes the primary keys are 'instruction', 'context', and 'response'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core fields for Dolly (will be None/empty string if not present)\n",
    "    core_instruction = example.get(\"instruction\", \"\")\n",
    "    context = example.get(\"context\", \"\")\n",
    "    response = example.get(\"response\", \"\")\n",
    "    \n",
    "    # Initialize the instruction to be returned\n",
    "    final_instruction = \"\"\n",
    "\n",
    "    # --- 1. Build the Instruction ---\n",
    "    \n",
    "    # Check if context exists and is not just whitespace\n",
    "    if isinstance(context, str) and context.strip():\n",
    "        # Prepend the context to the instruction clearly\n",
    "        final_instruction = (\n",
    "            f\"Context: {context.strip()}\\n\\n\"\n",
    "            f\"Instruction: {core_instruction.strip()}\"\n",
    "        )\n",
    "    elif isinstance(core_instruction, str) and core_instruction.strip():\n",
    "        # If no context, just use the core instruction\n",
    "        final_instruction = core_instruction.strip()\n",
    "    # Note: If both are empty, final_instruction remains \"\" (which will be filtered later).\n",
    "\n",
    "    # --- 2. Keep the Response and Source ---\n",
    "    \n",
    "    if isinstance(response, str):\n",
    "        final_response = response.strip()\n",
    "    else:\n",
    "        final_response = response # Keep None/other type for filtering\n",
    "\n",
    "    return {\n",
    "        \"instruction\": final_instruction, \n",
    "        \"response\": final_response, \n",
    "        \"source\": \"dolly\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "import re \n",
    "# Note: You'd need to ensure 'import re' is at the top of your main script \n",
    "# or use pd.Series as you did before, but the pure Python approach is cleaner.\n",
    "\n",
    "def format_GSM8K(example):\n",
    "    \"\"\"\n",
    "    Formats a single GSM8K entry by extracting the question and cleaning \n",
    "    the step-by-step answer (solution) by removing calculator tokens and \n",
    "    the final answer marker.\n",
    "    \"\"\"\n",
    "    \n",
    "    # GSM8K uses 'question' for instruction and 'answer' for response\n",
    "    instruction = example.get(\"question\")\n",
    "    response = example.get(\"answer\")\n",
    "\n",
    "    if isinstance(response, str):\n",
    "        # --- FINAL FIXES FOR GSM8K using pure Python re module ---\n",
    "        \n",
    "        # 1. Remove GSM8K Calculator Tokens (e.g., <<16+20=36>>)\n",
    "        # The re.sub is much cleaner than the pd.Series chaining.\n",
    "        response = re.sub(r'<<.*?>>', '', response).strip()\n",
    "\n",
    "        # 2. Remove the GSM8K Final Answer Marker (e.g., #### 40 or #### 12.5)\n",
    "        # Using [^\\n]+ to catch any numerical/decimal/fractional answer following the marker\n",
    "        response = re.sub(r'####\\s*[^\\n]+', '', response).strip()\n",
    "    \n",
    "    # The 'instruction' only needs to be the question\n",
    "    if isinstance(instruction, str):\n",
    "        instruction = instruction.strip()\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"GSM8K\"}\n",
    "\n",
    "def format_OpenThoughts(example):\n",
    "    instruction, response = None, None\n",
    "\n",
    "    # Check for the 'conversations' key typical of OpenThoughts structure\n",
    "    if 'conversations' in example and isinstance(example['conversations'], list):\n",
    "        # The user's instruction is the 'value' of the first 'user' turn\n",
    "        # 1. Extract Instruction (User's turn)\n",
    "        user_turn = next((c for c in example['conversations'] if c['from'] == 'user'), None)\n",
    "        if user_turn:\n",
    "            instruction = user_turn.get('value')\n",
    "\n",
    "        # 2. Extract Response (Assistant's turn)\n",
    "        assistant_turn = next((c for c in example['conversations'] if c['from'] == 'assistant'), None)\n",
    "        if assistant_turn:\n",
    "            response = assistant_turn.get('value')\n",
    "    \n",
    "\n",
    "    if isinstance(response, str):\n",
    "        # Apply the CoT cleaning logic\n",
    "        response = response.replace(\"<|begin_of_thought|>\", \"\").replace(\"<|end_of_thought|>\", \"\").strip()\n",
    "        response = response.replace(\"<|begin_of_solution|>\", \"\").replace(\"<|end_of_solution|>\", \"\").strip()\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"OpenThoughts\"}    \n",
    "    \n",
    "def format_math(example):\n",
    "    \n",
    "    return {\"instruction\": example[\"problem\"], \"response\": example[\"solution\"], \"source\": \"math\"}\n",
    "\n",
    "\n",
    "def format_alpaca(example):\n",
    "    \n",
    "    # Base instruction\n",
    "    inst = example.get(\"instruction\", \"\").strip()\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    out = example.get(\"output\") or example.get(\"response\") or example.get(\"answer\") or \"\"\n",
    "\n",
    "    # If input exists and is non-empty, append it with a label\n",
    "    if isinstance(inp, str) and inp.strip():\n",
    "        inst = f\"{inst}\\n\\nInput:\\n{inp.strip()}\"\n",
    "\n",
    "    # final cleaning\n",
    "    inst = inst.strip()\n",
    "    out = out.strip()\n",
    "\n",
    "    return {\"instruction\": inst, \"response\": out, \"source\": \"alpaca\"}\n",
    "\n",
    "\n",
    "def format_sharegpt(example):\n",
    "    \n",
    "    if \"conversations\" not in example or len(example[\"conversations\"]) < 2:\n",
    "        return {\"instruction\": None, \"response\": None}\n",
    "    convs = example[\"conversations\"]\n",
    "    user_turns = [c[\"value\"] for c in convs if c[\"from\"] == \"human\"]\n",
    "    assistant_turns = [c[\"value\"] for c in convs if c[\"from\"] == \"gpt\"]\n",
    "    \n",
    "    return {\n",
    "        \"instruction\": user_turns[0] if user_turns else None,\n",
    "        \"response\": assistant_turns[0] if assistant_turns else None,\n",
    "    }\n",
    "    \n",
    "def format_boolq(example):\n",
    "    \n",
    "    q = example.get(\"question\", \"\").strip()\n",
    "    p = example.get(\"passage\", \"\").strip()\n",
    "    a = example.get(\"answer\", None)\n",
    "\n",
    "    # Convert boolean answer to natural language\n",
    "    if a is True:\n",
    "        answer_text = \"Yes\"\n",
    "    elif a is False:\n",
    "        answer_text = \"No\"\n",
    "    else:\n",
    "        answer_text = \"\"\n",
    "\n",
    "    # Build structured instruction\n",
    "    if p:\n",
    "        instruction = f\"Question: {q}\\n\\nContext:\\n{p}\"\n",
    "    else:\n",
    "        instruction = f\"Question: {q}\"\n",
    "\n",
    "    return {\"instruction\": instruction.strip(), \"response\": answer_text.strip(), \"source\": \"boolq\"}\n",
    "\n",
    "def format_arc(sample):\n",
    "    \"\"\"\n",
    "    Converts a single sample from the allenai/ai2_arc dataset into \n",
    "    the Supervised Fine-Tuning (SFT) instruction-response format.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A single dictionary sample from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary in the SFT format {\"instruction\": ..., \"response\": ...}.\n",
    "    \"\"\"\n",
    "    # 1. Extract the main question and the answer key\n",
    "    question = sample['question']\n",
    "    answer_key = sample['answerKey']\n",
    "    \n",
    "    # 2. Extract the choices and labels\n",
    "    choices_text = sample['choices']['text']\n",
    "    choices_label = sample['choices']['label']\n",
    "    \n",
    "    # 3. Create the formatted list of choices (e.g., \"A. pulley.\\nB. lever.\\n...\")\n",
    "    formatted_choices = []\n",
    "    # Use zip to pair the labels (A, B, C, D) with the corresponding text\n",
    "    for label, text in zip(choices_label, choices_text):\n",
    "        formatted_choices.append(f\"{label}. {text}\")\n",
    "        \n",
    "    # Join the choices with newlines to make them easy for the model to read\n",
    "    choices_block = \"\\n\".join(formatted_choices)\n",
    "    \n",
    "    # 4. Construct the full 'instruction' prompt\n",
    "    instruction = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Choices:\\n{choices_block}\\n\\n\"\n",
    "        \"Please select the letter (A, B, C, or D) that represents the correct answer.\"\n",
    "    )\n",
    "    \n",
    "    # 5. Determine the correct 'response'\n",
    "    \n",
    "    # Find the index of the answer key (e.g., 'B' is at index 1)\n",
    "    answer_index = choices_label.index(answer_key)\n",
    "    \n",
    "    # Get the text corresponding to the answer key\n",
    "    correct_answer_text = choices_text[answer_index]\n",
    "    \n",
    "    # The response should include the key and the full text for a complete answer\n",
    "    response = f\"Correct Answer: {answer_key}. {correct_answer_text}\"\n",
    "    \n",
    "    # 6. Return the final SFT dictionary\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"response\": response,\n",
    "        \"source\": \"arc\"\n",
    "    }\n",
    "\n",
    "def format_hotpot(example):\n",
    "    context_text = \"\\n\".join(example[\"context\"][\"sentences\"][0]) if \"context\" in example and isinstance(example[\"context\"], dict) else \"\"\n",
    "    response = f\"Answer: {example['answer']}\\n\\nContext:\\n{context_text}\"\n",
    "    return {\"instruction\": example[\"question\"], \"response\": response}\n",
    "\n",
    "\n",
    "def format_gpt4(example):\n",
    "    instruction, response = None, None\n",
    "\n",
    "    # Check for the 'conversations' key typical of OpenThoughts structure\n",
    "    if 'items' in example and isinstance(example['items'], list):\n",
    "        # The user's instruction is the 'value' of the first 'user' turn\n",
    "        # 1. Extract Instruction (User's turn)\n",
    "        user_turn = next((c for c in example['items'] if c['from'] == 'human'), None)\n",
    "        if user_turn:\n",
    "            instruction = user_turn.get('value')\n",
    "\n",
    "        # 2. Extract Response (Assistant's turn)\n",
    "        assistant_turn = next((c for c in example['items'] if c['from'] == 'gpt'), None)\n",
    "        if assistant_turn:\n",
    "            response = assistant_turn.get('value')\n",
    "    \n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"sharegpt4\"}    \n",
    "    \n",
    "\n",
    "# ================================================\n",
    "# 3Ô∏è‚É£ load data from different Datasets\n",
    "# ================================================\n",
    "\n",
    "\n",
    "print(\"üì• Starting dataset loading process and inspection...\")\n",
    "\n",
    "\n",
    "# Set a consistent seed for reproducible random sampling\n",
    "SEED = 42 \n",
    "\n",
    "def load_and_random_select(dataset_name, split_name, percentage, config_name=None):\n",
    "    \"\"\"Loads a split, shuffles it, and selects the specified percentage of entries.\"\"\"\n",
    "    print(f\"\\n-> Loading and selecting {percentage*100:.0f}% randomly from {dataset_name} ({split_name})\")\n",
    "    \n",
    "    # 1. Load the full split\n",
    "    if config_name:\n",
    "        ds_full = load_dataset(dataset_name, config_name, split=split_name)\n",
    "    else:\n",
    "        ds_full = load_dataset(dataset_name, split=split_name)\n",
    "    \n",
    "    # Calculate the number of entries to select\n",
    "    num_to_select = int(len(ds_full) * percentage)\n",
    "    \n",
    "    # 2. Shuffle the dataset and select the required number of entries\n",
    "    # The 'seed' makes the random selection reproducible\n",
    "    ds_random = ds_full.shuffle(seed=SEED).select(range(num_to_select))\n",
    "    \n",
    "    return ds_random\n",
    "\n",
    "\n",
    "#Conversational tone / flow  Ultrachat, Dolly 45 %  Teaches helpful, polite phrasing and structure\n",
    "\n",
    "#dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:80%]\")\n",
    "dolly = load_and_random_select(\"databricks/databricks-dolly-15k\", \"train\", 0.15)\n",
    "inspect_random_entry(\"databricks/databricks-dolly-15k\", dolly)\n",
    "\n",
    "#ultrachat = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:10%]\")\n",
    "ultrachat = load_and_random_select(\"HuggingFaceH4/ultrachat_200k\", \"train_sft\", 0.01)\n",
    "inspect_random_entry(\"HuggingFaceH4/ultrachat_200k\", ultrachat)\n",
    "\n",
    "# Structured reasoning (CoT) Open-Thoughts, GSM8K, MATH 35 % Injects step-by-step clarity and logical progression\n",
    "\n",
    "#cot_thoughts = load_dataset(\"open-thoughts/open-thoughts-114k\", split=\"train[:21%]\")\n",
    "cot_thoughts = load_and_random_select(\"open-thoughts/open-thoughts-114k\", \"train\", 0.40)\n",
    "inspect_random_entry(\"open-thoughts/open-thoughts-114k\", cot_thoughts)\n",
    "\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\") \n",
    "inspect_random_entry(\"openai/gsm8k\", gsm8k)\n",
    "\n",
    "math_ds = load_dataset(\"HuggingFaceH4/math\", split=\"train\")\n",
    "inspect_random_entry(\"HuggingFaceH4/math\", math_ds)\n",
    "\n",
    "# Instruction diversity / broad generality Alpaca, ARC, BoolQ 15 % Keeps model generalist and able to follow any prompt\n",
    "#alpaca = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:15%]\")\n",
    "alpaca = load_and_random_select(\"tatsu-lab/alpaca\", \"train\", 0.05)\n",
    "inspect_random_entry(\"tatsu-lab/alpaca\", alpaca)\n",
    "\n",
    "#boolq = load_dataset(\"google/boolq\", split=\"train[:30%]\")\n",
    "boolq = load_and_random_select(\"google/boolq\", \"train\", 0.30)\n",
    "inspect_random_entry(\"google/boolq\", boolq)\n",
    "\n",
    "arc = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"train\")\n",
    "inspect_random_entry(\"allenai/ai2_arc\", arc)\n",
    "\n",
    "# Human-preferred style LMSYS winners 5 % ‚ÄúStyle anchor‚Äù for alignment and empathy\n",
    "#lmsys_conv_raw = load_dataset(\"lmsys/chatbot_arena_conversations\", split=\"train[:15%]\") \n",
    "lmsys_conv_raw = load_and_random_select(\"lmsys/chatbot_arena_conversations\", \"train\", 0.08)\n",
    "inspect_random_entry(\"lmsys/chatbot_arena_conversations\", lmsys_conv_raw)\n",
    "\n",
    "\n",
    "# lmsys/sharegpt4-dataset (subset)\n",
    "#sharegpt = load_dataset(\"anon8231489123/ShareGPT_Vicuna_unfiltered\", split=\"train[:3%]\")\n",
    "#sharegpt = load_dataset(\"LDJnr/ShareGPT-processed\", split=\"train[:5%]\")\n",
    "#sharegpt = load_dataset(\"lmsys/sharegpt4-dataset\", split=\"train[:5%]\")\n",
    "#inspect_random_entry(\"lmsys/sharegpt4-dataset\", sharegpt)\n",
    "\n",
    "sharegpt4 = load_dataset(\n",
    "    \"openchat/openchat_sharegpt4_dataset\",\n",
    "    data_files=\"sharegpt_gpt4.json\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "inspect_random_entry(\"openchat/openchat_sharegpt4_dataset/sharegpt_gpt4.json\", sharegpt4, force_disp = False)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 4Ô∏è‚É£ format datasets to instruction:response for SFT\n",
    "# ================================================\n",
    "\n",
    "\n",
    "print(\"\\nüß† Applying OpenThoughts formatter...\")\n",
    "cot_thoughts_formatted = cot_thoughts.map(format_OpenThoughts, remove_columns=cot_thoughts.column_names)\n",
    "inspect_formatted_sft(\"cot_thoughts_formatted\", cot_thoughts_formatted)\n",
    "\n",
    "print(\"\\nüß† Applying gsm8k formatter...\")\n",
    "gsm8k_formatted = gsm8k.map(format_GSM8K, remove_columns=gsm8k.column_names)\n",
    "inspect_formatted_sft(\"gsm8k_formatted\", gsm8k_formatted, 2, 200)\n",
    "\n",
    "print(\"\\nüß† Applying LMSYS winner extraction logic...\")\n",
    "lmsys_conv_formatted = lmsys_conv_raw.map(extract_lmsys_winner, remove_columns=lmsys_conv_raw.column_names)\n",
    "inspect_formatted_sft(\"lmsys_conv_formatted\", lmsys_conv_formatted)\n",
    "\n",
    "print(\"\\nüß† Applying dolly formatter...\")\n",
    "dolly_formatted = dolly.map(format_dolly, remove_columns=dolly.column_names)\n",
    "inspect_formatted_sft(\"dolly_formatted\", dolly_formatted)\n",
    "\n",
    "print(\"\\nüß† Applying ultrachat formatter...\")\n",
    "ultrachat_formatted = ultrachat.map(format_ultrachat, remove_columns=ultrachat.column_names)\n",
    "inspect_formatted_sft(\"ultrachat_formatted\", ultrachat_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting math formatter...\")\n",
    "math_formatted = math_ds.map(format_math, remove_columns=math_ds.column_names)\n",
    "inspect_formatted_sft(\"math_formatted\", math_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting alpaca formatter...\")\n",
    "alpaca_formatted = alpaca.map(format_alpaca, remove_columns=alpaca.column_names)\n",
    "inspect_formatted_sft(\"alpaca_formatted\", alpaca_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting boolq formatter...\")\n",
    "boolq_formatted = boolq.map(format_boolq, remove_columns=boolq.column_names)\n",
    "inspect_formatted_sft(\"boolq_formatted\", boolq_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting arc formatter...\")\n",
    "arc_formatted = arc.map(format_arc, remove_columns=arc.column_names)\n",
    "inspect_formatted_sft(\"arc_formatted\", arc_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting arc formatter...\")\n",
    "gpt4_formatted = sharegpt4.map(format_gpt4, remove_columns=sharegpt4.column_names)\n",
    "inspect_formatted_sft(\"gpt4_formatted\", gpt4_formatted, force_disp = False)\n",
    "\n",
    "# ================================================\n",
    "# 5Ô∏è‚É£  Merge datasets together\n",
    "# ================================================\n",
    "\n",
    "\n",
    "formatted_datasets = []\n",
    "\n",
    "formatted_datasets.extend([\n",
    "    # Training Chain-of-Thought (CoT) and structured output.\n",
    "    cot_thoughts_formatted,\n",
    "\n",
    "    # Injecting the human-preferred response style (the \"winner's\" style) directly from real-world usage data.\n",
    "    lmsys_conv_formatted,\n",
    "    \n",
    "    # Teaches the model to follow multi-step arithmetic and symbolic logic precisely. \n",
    "    # Foundation for numerical and multi-step reasoning.\n",
    "    #gsm8k_formatted,\n",
    "\n",
    "    # Natural Tone & Diversity. output is conversational, polite, and natural (human-written style)\n",
    "    dolly_formatted,\n",
    "\n",
    "    # Provides a massive volume of clean, multi-turn dialogues. Your SFT extraction\n",
    "    # flattens these into single turns, but the model still learns the density of\n",
    "    # conversation‚Äîthe expectation of clear, concise, and structured responses in a\n",
    "    # chat environment.\n",
    "    ultrachat_formatted,\n",
    "    \n",
    "    #math_formatted,\n",
    "\n",
    "    # instruction-following dataset \n",
    "    alpaca_formatted,\n",
    "\n",
    "    # designed for Question Answering (QA) tasks\n",
    "    # Implicit Reasoning: Answering BoolQ often requires more complex reasoning and\n",
    "    # inference than simply locating a specific keyword or phrase. \n",
    "    boolq_formatted,\n",
    "\n",
    "    # AI2 ARC (AI2 Reasoning Challenge) is a benchmark dataset created by the Allen\n",
    "    # Institute for AI to evaluate AI systems on grade-school level science reasoning.\n",
    "    arc_formatted,\n",
    "\n",
    "    # High-quality instruction‚Äìresponse pairs\n",
    "    # GPT-4-level responses (high reasoning, coherence, and helpfulness)\n",
    "    gpt4_formatted,\n",
    "])\n",
    "\n",
    "\n",
    "mixed = concatenate_datasets(formatted_datasets).shuffle(seed=42)\n",
    "\n",
    "# ================================================\n",
    "# 6Ô∏è‚É£  merged datasets filter\n",
    "# ================================================\n",
    "\n",
    "print(\"üóëÔ∏è Applying quality filter...\")\n",
    "\n",
    "def filter_null_or_empty(example):\n",
    "    inst = example.get(\"instruction\")\n",
    "    resp = example.get(\"response\")\n",
    "    \n",
    "    if not isinstance(inst, str) or not isinstance(resp, str):\n",
    "        return False\n",
    "    if len(inst.strip()) < 5 or len(resp.strip()) < 2:\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "mixed = mixed.filter(filter_null_or_empty)\n",
    "\n",
    "\n",
    "MAX_SAMPLES = 38000 \n",
    "\n",
    "# Shuffle the dataset first (use a seed for reproducibility)\n",
    "shuffled_dataset = mixed.shuffle(seed=42)\n",
    "\n",
    "# Then, select the first N samples from the now-shuffled dataset\n",
    "dataset_random = shuffled_dataset.select(range(min(MAX_SAMPLES, len(shuffled_dataset))))\n",
    "\n",
    "# take all of gsmk8 math\n",
    "# combined_dataset = concatenate_datasets([dataset_random, gpt4_formatted, gsm8k_formatted, math_formatted ])\n",
    "combined_dataset = concatenate_datasets([dataset_random, gsm8k_formatted, math_formatted ])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "MAX_SEQ_LEN = 4096\n",
    "\n",
    "# Define your custom system prompt\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
    "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
    "\n",
    "Your task is to:\n",
    "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
    "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
    "  but clearly distinguish between context-based and general reasoning.\n",
    "\n",
    "Your responses must be:\n",
    "- Structured ‚Äî use clear formatting and logical reasoning.\n",
    "- Contextual ‚Äî rely only on the information available.\n",
    "- Concise ‚Äî eliminate filler words while preserving precision.\n",
    "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Just load the tokenizer ‚Äî no GPU or model weights needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #\"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\",\n",
    "    \n",
    "     # Use Fast tokenizer\n",
    "    use_fast=True,     \n",
    "    \n",
    "    # Required for Unsloth custom templates\n",
    "    trust_remote_code=True,  \n",
    ")\n",
    "\n",
    "\n",
    "# Add customized prompt and apply chat template, then use tokenizer compute the length to token\n",
    "# Only accept length <= 4096 (max_seq_length)\n",
    "def is_length_within_limit(example):\n",
    "    # Create a Harmony-format conversation\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "    wrapped = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "    )\n",
    "    #print(wrapped)\n",
    "    token_count = len(wrapped[\"input_ids\"]) if isinstance(wrapped, dict) else len(wrapped)\n",
    "    return token_count <= MAX_SEQ_LEN\n",
    "\n",
    "\n",
    "\n",
    "# Use the .filter() method which returns a new Dataset object\n",
    "filtered_dataset = combined_dataset.filter(\n",
    "    is_length_within_limit,\n",
    "    num_proc=4 # Use multiple processes to speed up tokenization/filtering\n",
    ")\n",
    "\n",
    "# .shuffle() on the Dataset object\n",
    "final_dataset_random = filtered_dataset.shuffle(seed=42)\n",
    "\n",
    "print(\"\\n‚úÖ Final dataset successfully filtered and shuffled.\")\n",
    "\n",
    "\n",
    "# Prints parts of the datasets with chat template\n",
    "def inspect_formatted_sft(ds_name, ds_object, n = 2, force_disp = False):\n",
    "\n",
    "    if not force_disp:\n",
    "        if not INSPECT_ON:\n",
    "            return \n",
    "    \n",
    "    for i, entry in enumerate(ds_object):\n",
    "        if i > n:\n",
    "            break\n",
    "        print(f\"--- Entry {i+1} ---\")\n",
    "\n",
    "            # Create a Harmony-format conversation\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": entry.get(\"instruction\", \"\")},\n",
    "            {\"role\": \"assistant\", \"content\": entry.get(\"response\", \"\")},\n",
    "        ]\n",
    "        wrapped = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"Message with chat template: {wrapped}...\") \n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "inspect_formatted_sft(\"final_dataset_random\", final_dataset_random, force_disp = True)\n",
    "\n",
    "# ================================================\n",
    "# 7Ô∏è‚É£  Analysis datasets 1. percentage of each source 2. max length(token) of each source\n",
    "# ================================================\n",
    "\n",
    "\n",
    "# Analysis: Getting the Source Statistics\n",
    "print(\"\\nüìä Analyzing Sources in Final Dataset...\")\n",
    "\n",
    "# Extract the 'source' column into a list\n",
    "sources_list = final_dataset_random['source']\n",
    "\n",
    "# Get the total size of the final dataset\n",
    "final_dataset_size = len(final_dataset_random)\n",
    "\n",
    "# Convert the list to a Pandas Series and count the unique values (This gives the 'Count' column)\n",
    "source_counts = pd.Series(sources_list).value_counts()\n",
    "\n",
    "# Calculate the total number of unique sources\n",
    "total_unique_sources = len(source_counts)\n",
    "\n",
    "# Create a DataFrame for summary and add the Percentage column \n",
    "# Convert the Series to a DataFrame\n",
    "summary_df = source_counts.rename('Count').to_frame()\n",
    "\n",
    "# Calculate the percentage\n",
    "summary_df['Percentage'] = (summary_df['Count'] / final_dataset_size) * 100\n",
    "\n",
    "# Format the Percentage column for clean display\n",
    "summary_df['Percentage'] = summary_df['Percentage'].map('{:.2f}%'.format)\n",
    "\n",
    "# Printing the Summary\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final Dataset Size: {final_dataset_size} entries\")\n",
    "print(f\"Total Unique Sources Found: {total_unique_sources}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Count and Percentage of Data Entries per Source:\")\n",
    "print(summary_df.to_string()) # Print the DataFrame with both Count and Percentage\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Save as JSONL for Unsloth\n",
    "df = pd.DataFrame(final_dataset_random)\n",
    "df.to_json(\"train_sft_final.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"\\nüéâ SUCCESS! Final SFT dataset created: train_sft_final.jsonl\")\n",
    "print(f\"Total samples after filtering: {len(final_dataset_random)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c550bff-0143-4cfe-ab14-887fb1f1a880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Token Length (Instruction + Response) per Source:\n",
      "-------------------------------------------------------\n",
      "source\n",
      "OpenThoughts    3872\n",
      "sharegpt4       3728\n",
      "dolly           3337\n",
      "math            2445\n",
      "ultrachat       2366\n",
      "lmsys_conv      1256\n",
      "boolq            580\n",
      "alpaca           444\n",
      "GSM8K            409\n",
      "arc              235\n",
      "Name: total_tokens, dtype: int64\n",
      "Count of Entries with Token Length > 4096 per Source:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Alternatively, a cleaner map that keeps only source and tokens\n",
    "def prepare_data_for_analysis(example, tokenizer):\n",
    "    combined_text = example['instruction'] + \" \" + example['response']\n",
    "    tokenized = tokenizer(combined_text, truncation=False, return_length=True)\n",
    "    return {\n",
    "        'source': example['source'], # Assuming you have a 'source' column\n",
    "        'total_tokens': tokenized['length'][0]\n",
    "    }\n",
    "\n",
    "analysis_dataset = final_dataset_random.map(\n",
    "    lambda x: prepare_data_for_analysis(x, tokenizer),\n",
    "    batched=False, # Process one example at a time\n",
    "    num_proc=4,\n",
    "    remove_columns=final_dataset_random.column_names # Keep only the two new columns\n",
    ")\n",
    "\n",
    "# Convert the analysis dataset to a Pandas DataFrame\n",
    "analysis_df = analysis_dataset.to_pandas()\n",
    "\n",
    "# Group by the 'source' column and find the maximum 'total_tokens'\n",
    "max_lengths_by_source = analysis_df.groupby('source')['total_tokens'].max().sort_values(ascending=False)\n",
    "\n",
    "## Output the result\n",
    "print(\"Maximum Token Length (Instruction + Response) per Source:\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(max_lengths_by_source)\n",
    "\n",
    "\n",
    "# Convert to Pandas for aggregation\n",
    "analysis_df = analysis_dataset.to_pandas()\n",
    "\n",
    "# Filter for entries where token length is greater than the threshold\n",
    "large_entries_df = analysis_df[analysis_df['total_tokens'] > MAX_SEQ_LEN]\n",
    "\n",
    "#  Group by 'source' and count the number of entries\n",
    "count_by_source = large_entries_df.groupby('source').size().sort_values(ascending=False)\n",
    "\n",
    "# Print the final result\n",
    "print(f\"Count of Entries with Token Length > {MAX_SEQ_LEN} per Source:\")\n",
    "print(count_by_source)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb77e2-9b61-4c38-a07e-877c39b44649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
