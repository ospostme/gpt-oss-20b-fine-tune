{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a2cb024-d891-45b9-a022-fb05bc6cdf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/home/zeus/miniconda3/envs/cloudspace/bin/python'}\n",
       "datasets                  4.3.0\n",
       "huggingface-hub           0.34.4\n",
       "numpy                     2.3.4\n",
       "nvidia-cuda-cupti-cu12    12.8.90\n",
       "nvidia-cuda-nvrtc-cu12    12.8.93\n",
       "nvidia-cuda-runtime-cu12  12.8.90\n",
       "pandas                    2.3.3\n",
       "pyarrow                   22.0.0\n",
       "pytorch-lightning         2.6.0\n",
       "tokenizers                0.22.1\n",
       "torch                     2.8.0+cu128\n",
       "torchao                   0.15.0\n",
       "torchmetrics              1.7.4\n",
       "torchvision               0.23.0+cu128\n",
       "tqdm                      4.67.1\n",
       "transformers              4.56.2\n",
       "triton                    3.4.0\n",
       "trl                       0.22.2\n",
       "unsloth                   2025.12.9\n",
       "unsloth_zoo               2025.12.7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "print({sys.executable})\n",
    "!{sys.executable} -m pip list | grep -E \"torch|cuda|datasets|pandas|tqdm|numpy|pyarrow|transformers|tokenizers|trl|unsloth|unsloth_zoo|huggingface-hub|datasets|numpy|triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d63591-ba51-4f4b-95ba-1639aea094c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triton                    3.4.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!{sys.executable} -m pip list  | grep triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bfed80-3836-49cb-a511-236b74eb81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install --upgrade --no-deps bitsandbytes transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 tqdm==4.67.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa02244-705c-4c45-b5f1-7d9dbf90e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade  uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760799f8-74c2-4499-895c-939336c2939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install \"triton>=3.4.0\" bitsandbytes \"transformers==4.56.2\" \\\n",
    "#\"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "#\"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "#        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de872fe-713f-4107-bf23-9b6722f0771c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pre-warmed imports\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Pre-warmed imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4297b7a-1a52-4b3a-a664-c3f805b31ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytorch-lightning         2.6.0\n",
       "torch                     2.8.0+cu128\n",
       "torchao                   0.15.0\n",
       "torchmetrics              1.7.4\n",
       "torchvision               0.23.0+cu128\n",
       "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
       "pytorch-lightning         2.6.0\n",
       "torch                     2.8.0+cu128\n",
       "torchao                   0.15.0\n",
       "torchmetrics              1.7.4\n",
       "torchvision               0.23.0+cu128\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip list | grep torch\n",
    "!{sys.executable} -m uv pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f0164d-06d7-4586-9968-5e54bf22d3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Current Python executable:\n",
       "/home/zeus/miniconda3/envs/cloudspace/bin/python\n",
       "\n",
       "Active env name: cloudspace\n",
       "\n",
       "Kernel spec info:\n",
       "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
       "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
       "0.00s - to python to disable frozen modules.\n",
       "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
       "Available kernels:\n",
       "  python3    /home/zeus/miniconda3/envs/cloudspace/share/jupyter/kernels/python3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Current Python executable:\")\n",
    "print(sys.executable)\n",
    "\n",
    "kernel_name = Path(sys.prefix).name\n",
    "print(\"\\nActive env name:\", kernel_name)\n",
    "\n",
    "print(\"\\nKernel spec info:\")\n",
    "!jupyter kernelspec list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f317a94-7a59-4990-b4e1-c94ac7a74d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
       "Package                   Version\n",
       "------------------------- ------------\n",
       "absl-py                   2.3.1\n",
       "accelerate                1.12.0\n",
       "aiohappyeyeballs          2.6.1\n",
       "aiohttp                   3.13.2\n",
       "aiosignal                 1.4.0\n",
       "annotated-doc             0.0.4\n",
       "annotated-types           0.7.0\n",
       "anyio                     4.12.0\n",
       "argon2-cffi               25.1.0\n",
       "argon2-cffi-bindings      25.1.0\n",
       "arrow                     1.4.0\n",
       "asttokens                 3.0.1\n",
       "async-lru                 2.0.5\n",
       "attrs                     25.4.0\n",
       "babel                     2.17.0\n",
       "backoff                   2.2.1\n",
       "beautifulsoup4            4.14.3\n",
       "bitsandbytes              0.49.0\n",
       "bleach                    6.3.0\n",
       "boto3                     1.42.3\n",
       "botocore                  1.42.3\n",
       "cachetools                6.2.2\n",
       "certifi                   2025.11.12\n",
       "cffi                      2.0.0\n",
       "charset-normalizer        3.4.4\n",
       "click                     8.3.1\n",
       "comm                      0.2.3\n",
       "contourpy                 1.3.3\n",
       "cut-cross-entropy         25.1.1\n",
       "cycler                    0.12.1\n",
       "datasets                  4.3.0\n",
       "debugpy                   1.8.17\n",
       "decorator                 5.2.1\n",
       "defusedxml                0.7.1\n",
       "diffusers                 0.36.0\n",
       "dill                      0.4.0\n",
       "docker                    7.1.0\n",
       "docstring-parser          0.17.0\n",
       "executing                 2.2.1\n",
       "fastapi                   0.123.9\n",
       "fastjsonschema            2.21.2\n",
       "filelock                  3.20.0\n",
       "fonttools                 4.61.0\n",
       "fqdn                      1.5.1\n",
       "frozenlist                1.8.0\n",
       "fsspec                    2025.9.0\n",
       "google-auth               2.41.1\n",
       "google-auth-oauthlib      1.2.3\n",
       "grpcio                    1.76.0\n",
       "h11                       0.16.0\n",
       "hf-transfer               0.1.9\n",
       "hf-xet                    1.2.0\n",
       "httpcore                  1.0.9\n",
       "httptools                 0.7.1\n",
       "httpx                     0.28.1\n",
       "huggingface-hub           0.34.4\n",
       "idna                      3.11\n",
       "importlib-metadata        8.7.1\n",
       "ipykernel                 6.26.0\n",
       "ipython                   8.17.2\n",
       "ipywidgets                8.1.1\n",
       "isoduration               20.11.0\n",
       "jedi                      0.19.2\n",
       "jinja2                    3.1.6\n",
       "jmespath                  1.0.1\n",
       "joblib                    1.5.2\n",
       "json5                     0.12.1\n",
       "jsonpointer               3.0.0\n",
       "jsonschema                4.25.1\n",
       "jsonschema-specifications 2025.9.1\n",
       "jupyter-client            8.6.3\n",
       "jupyter-core              5.9.1\n",
       "jupyter-events            0.12.0\n",
       "jupyter-lsp               2.3.0\n",
       "jupyter-server            2.17.0\n",
       "jupyter-server-terminals  0.5.3\n",
       "jupyterlab                4.2.0\n",
       "jupyterlab-pygments       0.3.0\n",
       "jupyterlab-server         2.28.0\n",
       "jupyterlab-widgets        3.0.16\n",
       "kiwisolver                1.4.9\n",
       "lark                      1.3.1\n",
       "lightning                 2.6.0\n",
       "lightning-cloud           0.5.70\n",
       "lightning-sdk             2025.11.13\n",
       "lightning-utilities       0.15.2\n",
       "litai                     0.0.10\n",
       "litdata                   0.2.58\n",
       "litserve                  0.2.16\n",
       "markdown                  3.10\n",
       "markdown-it-py            4.0.0\n",
       "markupsafe                3.0.3\n",
       "matplotlib                3.8.2\n",
       "matplotlib-inline         0.2.1\n",
       "mdurl                     0.1.2\n",
       "mistune                   3.1.4\n",
       "mpmath                    1.3.0\n",
       "msgspec                   0.20.0\n",
       "multidict                 6.7.0\n",
       "multiprocess              0.70.16\n",
       "nbclient                  0.10.2\n",
       "nbconvert                 7.16.6\n",
       "nbformat                  5.10.4\n",
       "nest-asyncio              1.6.0\n",
       "networkx                  3.6\n",
       "notebook-shim             0.2.4\n",
       "numpy                     2.3.4\n",
       "nvidia-cublas-cu12        12.8.4.1\n",
       "nvidia-cuda-cupti-cu12    12.8.90\n",
       "nvidia-cuda-nvrtc-cu12    12.8.93\n",
       "nvidia-cuda-runtime-cu12  12.8.90\n",
       "nvidia-cudnn-cu12         9.10.2.21\n",
       "nvidia-cufft-cu12         11.3.3.83\n",
       "nvidia-cufile-cu12        1.13.1.3\n",
       "nvidia-curand-cu12        10.3.9.90\n",
       "nvidia-cusolver-cu12      11.7.3.90\n",
       "nvidia-cusparse-cu12      12.5.8.93\n",
       "nvidia-cusparselt-cu12    0.7.1\n",
       "nvidia-nccl-cu12          2.27.3\n",
       "nvidia-nvjitlink-cu12     12.8.93\n",
       "nvidia-nvtx-cu12          12.8.90\n",
       "oauthlib                  3.3.1\n",
       "obstore                   0.8.2\n",
       "packaging                 25.0\n",
       "pandas                    2.3.3\n",
       "pandocfilters             1.5.1\n",
       "parso                     0.8.5\n",
       "peft                      0.18.0\n",
       "pexpect                   4.9.0\n",
       "pillow                    12.0.0\n",
       "pip                       25.3\n",
       "platformdirs              4.5.0\n",
       "prometheus-client         0.23.1\n",
       "prompt-toolkit            3.0.52\n",
       "propcache                 0.4.1\n",
       "protobuf                  4.23.4\n",
       "psutil                    7.1.3\n",
       "ptyprocess                0.7.0\n",
       "pure-eval                 0.2.3\n",
       "pyarrow                   22.0.0\n",
       "pyasn1                    0.6.1\n",
       "pyasn1-modules            0.4.2\n",
       "pycparser                 2.23\n",
       "pydantic                  2.12.5\n",
       "pydantic-core             2.41.5\n",
       "pygments                  2.19.2\n",
       "pyjwt                     2.10.1\n",
       "pyparsing                 3.2.5\n",
       "python-dateutil           2.9.0.post0\n",
       "python-dotenv             1.2.1\n",
       "python-json-logger        4.0.0\n",
       "python-multipart          0.0.20\n",
       "pytorch-lightning         2.6.0\n",
       "pytz                      2025.2\n",
       "pyyaml                    6.0.3\n",
       "pyzmq                     27.1.0\n",
       "referencing               0.37.0\n",
       "regex                     2025.11.3\n",
       "requests                  2.32.5\n",
       "requests-oauthlib         2.0.0\n",
       "rfc3339-validator         0.1.4\n",
       "rfc3986-validator         0.1.1\n",
       "rfc3987-syntax            1.1.0\n",
       "rich                      14.2.0\n",
       "rpds-py                   0.30.0\n",
       "rsa                       4.9.1\n",
       "s3transfer                0.16.0\n",
       "safetensors               0.7.0\n",
       "scikit-learn              1.7.2\n",
       "scipy                     1.16.3\n",
       "send2trash                1.8.3\n",
       "sentencepiece             0.2.1\n",
       "setuptools                80.9.0\n",
       "simple-term-menu          1.6.6\n",
       "six                       1.17.0\n",
       "soupsieve                 2.8\n",
       "stack-data                0.6.3\n",
       "starlette                 0.50.0\n",
       "sympy                     1.14.0\n",
       "tensorboard               2.15.1\n",
       "tensorboard-data-server   0.7.2\n",
       "terminado                 0.18.1\n",
       "threadpoolctl             3.6.0\n",
       "tifffile                  2025.10.16\n",
       "tinycss2                  1.4.0\n",
       "tokenizers                0.22.1\n",
       "torch                     2.8.0+cu128\n",
       "torchao                   0.15.0\n",
       "torchmetrics              1.7.4\n",
       "torchvision               0.23.0+cu128\n",
       "tornado                   6.5.2\n",
       "tqdm                      4.67.1\n",
       "traitlets                 5.14.3\n",
       "transformers              4.56.2\n",
       "triton                    3.4.0\n",
       "trl                       0.22.2\n",
       "typeguard                 4.4.4\n",
       "typing-extensions         4.15.0\n",
       "typing-inspection         0.4.2\n",
       "tyro                      1.0.3\n",
       "tzdata                    2025.2\n",
       "unsloth                   2025.12.9\n",
       "unsloth-zoo               2025.12.7\n",
       "uri-template              1.3.0\n",
       "urllib3                   2.5.0\n",
       "uv                        0.9.21\n",
       "uvicorn                   0.38.0\n",
       "uvloop                    0.22.1\n",
       "watchfiles                1.1.1\n",
       "wcwidth                   0.2.14\n",
       "webcolors                 25.10.0\n",
       "webencodings              0.5.1\n",
       "websocket-client          1.9.0\n",
       "websockets                15.0.1\n",
       "werkzeug                  3.1.4\n",
       "wget                      3.2\n",
       "wheel                     0.45.1\n",
       "widgetsnbextension        4.0.15\n",
       "xformers                  0.0.32.post2\n",
       "xxhash                    3.6.0\n",
       "yarl                      1.22.0\n",
       "zipp                      3.23.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!uv pip list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2b1b2f-f831-4dc0-84b7-bce5fa694160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install --upgrade --no-deps \\\n",
    "#    transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f9577-1c75-49c1-9b38-047107be83c9",
   "metadata": {},
   "source": [
    "# Before everything we start,  preapre the envionment \n",
    "\n",
    "## Best Practice to follow : \n",
    "\n",
    "- Use Python 3.10 in your runtime\n",
    "- Install all heavy binary libs from conda\n",
    "- Install pip-only pure python packages AFTER NumPy/Torch are settled\n",
    "- Install optional Jupyter kernel inside the environment\n",
    "- (Optional but recommended) Create a reproducible environment file\n",
    "- Use Docker instead If your server supports Docker + NVIDIA Container Toolkit, this is the most reproducible\n",
    "\n",
    "‚úÖ NEVER mix uv/pip with conda for GPU ML frameworks (PyTorch, CUDA, Triton, bnb, xformers). \n",
    "You will see such kind of installation on jupyter notebook tutorial, that's a different story.\n",
    "\n",
    "‚úÖ !uv pip install only gets the installation faster due to the uv.  It deson't support --dry-run to check if there are any related dependency packages will be updated.\n",
    "\n",
    "‚úÖ How to lock/freeze the enviornment \n",
    "\n",
    "environment.yml (micromamba/conda) is the most solid choice\n",
    "\n",
    "| Method                  | System Packages | Python Packages |  Sub-dependencies | Reproducible? |\n",
    "|-------------------------|-------------------------|-------------------------|---------------------------|---------------|\n",
    "| pip freeze              | ‚ùå                       | ‚úÖ                       | ‚ùå                         | Partial       |\n",
    "| pip-tools / Poetry       | ‚ùå                       | ‚úÖ                       | ‚úÖ                         | ‚úÖ            |\n",
    "| environment.yml  | ‚úÖ           | ‚úÖ                       | ‚úÖ                         | ‚úÖ            |\n",
    "\n",
    "\n",
    "‚úÖ In Jupyter notebook,  pip/uv pip install actually not install packages into the connected kernel python environment. \n",
    "!pip list show different results from terminal (after  activate the env of the kernel) ??\n",
    "\n",
    "if you used !pip inside the notebook, it sometimes runs the system pip, not the pip from the kernel environment. \n",
    "\n",
    "!pip is a shell command. It resolves pip from the PATH in the Jupyter server process.\n",
    "\n",
    "That PATH might point to your system Python (or Jupyter‚Äôs default env) not your kernel‚Äôs env.\n",
    "\n",
    "That PATH might point to your system Python (or Jupyter‚Äôs default env) not your kernel‚Äôs env.\n",
    "\n",
    "!pip in notebooks is not fully reliable; always use {sys.executable} -m pip to be sure.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "!{sys.executable} -m pip list\n",
    "!{sys.executable} -m pip install package-name\n",
    "!{sys.executable} -m uv pip list\n",
    "!{sys.executable} -m uv pip install some-package\n",
    "```\n",
    "\n",
    "‚úÖ You will see a lot `!pip install !uv pip install` in the tutorial in jupyter notebook, if you are using the default system kernel, it's fine.  But not the customized envionment\n",
    "\n",
    "\n",
    "## For strick production ML enviornment\n",
    "\n",
    "### Packages contain large compiled C/CUDA binaries, should comes from Conda/micomamba \n",
    "\n",
    "> ABI compatibility  Application Binary Interface, how compiled components talk to each other.\n",
    "> PyTorch links to NumPy C-API\n",
    "> pandas links to NumPy C-API\n",
    "> PyArrow links to libarrow + NumPy C-API\n",
    "> PyTorch + TorchVision + TorchAudio + CUDA must be built with same compiler, same runtime, same dependencies\n",
    "> *If any one of these comes from pip, you may get a wheel compiled against a different compiler, different NumPy ABI,\n",
    "> different glibc, etc ‚Üí binary mismatch errors*\n",
    "\n",
    "### Conda/micomamba solves dependency versions together\n",
    "\n",
    "conda/mamba/micomamba solves installation as a set so they are fully compatible.\n",
    "\n",
    "pip cannot solve binary dependencies; it installs whatever wheels exist.\n",
    "\n",
    "### Conda binaries are built with compatible compilers\n",
    "\n",
    "conda-forge and pytorch channels use:\n",
    "\n",
    "same clang/gcc\n",
    "\n",
    "same GLIBC\n",
    "\n",
    "same CUDA runtime ABI\n",
    "\n",
    "same Arrow C++ libs\n",
    "\n",
    "same NumPy C API version\n",
    "\n",
    "pip wheels might:\n",
    "\n",
    "be compiled with gcc 9, 10, or 11\n",
    "\n",
    "assume NumPy 1.x ABI instead of 2.x\n",
    "\n",
    "bundle their own incompatible libarrow\n",
    "\n",
    "So mixing pip wheels for heavy packages is dangerous.\n",
    "\n",
    "### packages \n",
    "\n",
    "Which packages must ALWAYS come from conda?\n",
    "\n",
    "(üî• Critical ‚Äî do NOT install via pip/uv)\n",
    "\n",
    "‚úÖ Heavy Compiled Packages\n",
    "Package\tWhy it must be from conda\n",
    "pytorch\tCUDA + C++ + MKL linked libs\n",
    "torchvision, torchaudio\tMust match PyTorch ABI\n",
    "pytorch-cuda\tCUDA runtime libs; must match PyTorch\n",
    "numpy\tCore C-API; other libs depend on it\n",
    "pandas\tCompiled against NumPy C-API\n",
    "pyarrow\tCompiles against Arrow + NumPy C-API\n",
    "scipy (if any)\tHuge C++ build, must match NumPy\n",
    "opencv (cv2)\tMust match system libs / NumPy ABI\n",
    "tqdm (optional)\tOK in pip but stable in conda\n",
    "\n",
    "Which packages can come from pip/uv (safe)\n",
    "\n",
    "Pure Python packages (no compiled code):\n",
    "\n",
    "transformers\n",
    "trl\n",
    "huggingface-hub\n",
    "datasets\n",
    "unsloth\n",
    "unsloth_zoo\n",
    "tqdm\n",
    "tokenizers ‚ùó ‚Üê C++ but HuggingFace wheels are safe with both pip/conda\n",
    "\n",
    "General rule:\n",
    "\n",
    "‚úÖ If it does NOT contain heavy C++/CUDA ‚Üí pip is OK\n",
    "‚ùå If it DOES contain C++/CUDA ‚Üí use conda\n",
    "\n",
    "\n",
    "## commands \n",
    "\n",
    "```cmd\n",
    "\n",
    "\"${SHELL}\" <(curl -L micro.mamba.pm/install.sh)\n",
    "\n",
    "./bin/micromamba shell init -s bash -r ~/micromamba\n",
    "\n",
    "# better yet, restart your terminal!\n",
    "source ~/.bashrc\n",
    "\n",
    "micromamba create -n unsloth -c conda-forge -c nvidia -c pytorch python=3.10.12 pytorch=2.8.0 torchvision torchaudio pytorch-cuda numpy pillow pyarrow pandas  scipy scikit-learn triton==3.4.0\n",
    "\n",
    "This locks the channel sources for the environment. afterward will automatically use  conda-forge ‚Üí nvidia  ‚Üí pytorch  in that order.\n",
    "\n",
    "micromamba activate unsloth\n",
    "  \n",
    "pip install --dry-run --no-deps transformers==4.56.2 tokenizers torchmetrics trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 tqdm==4.67.1\n",
    "\n",
    "micromamba env export > environment.yml\n",
    "\n",
    "pip freeze > requirements.txt\n",
    "\n",
    "python -m ipykernel install --name ml --display-name \"ML (python 3.10)\"\n",
    "\n",
    "```\n",
    "> If you are working on a standard server,  your customized ML stack is done.\n",
    "> BUT in a cloud VM environment,  things will be much more complicated.  As the storage has a lot of underlying tricks.\n",
    "> - Some of the storage locations are overlay filesystem of container image.  A Writable layer on top a read only layer.\n",
    ">   what your saw is combination results of these. Any changes will be lost after next reboot. It's pretty fair with cloud\n",
    ">   environment, reboot/restart bring a fresh new workable bench.\n",
    ">   \n",
    "> - Some of the persistent storage are on a networked overlay filesystem, files are stored on persistent network storage,\n",
    ">   not a local SSD. Take lighting studio as an example:\n",
    ">\n",
    ">   a. Every I/O operation (e.g., imports, reads, writes, temporary cache creation) is slower ‚Äî sometimes by seconds per call.\n",
    ">\n",
    ">   b. When the Studio wakes from ‚Äúsleep,‚Äù it reloads your environment. Your micromamba env may live on a slower layer (overlay),\n",
    ">   and python and libraries are reloaded into memory the first time you import them ‚Äî causing the ‚Äúlong first cell execution.‚Äù\n",
    ">\n",
    ">   Fix: pre-warm import torch, transformers \n",
    ">\n",
    ">   c. IPC lag. In Lightning AI, your Jupyter UI and kernel may sometimes run in separate containers ‚Äî especially after a restart\n",
    ">   or upgrade. If your kernel‚Äôs python path is /teamspace/studios/this_studio/micromamba/envs/unsloth/bin/python but the JupyterLab\n",
    ">   process runs in /opt/jupyter/envs/main, IPC (inter-process communication) may traverse a socket bridge, introducing latency.\n",
    ">\n",
    "\n",
    "## Lightning AI extra setting for the customized micomamba env kernel\n",
    "\n",
    "\n",
    "```cmd\n",
    "#mkdir -p /teamspace/studios/this_studio/.jupyter_kernels\n",
    "#~ python -m ipykernel install --name ml --display-name \"ML (python 3.10)\" --prefix=/teamspace/studios/this_studio/.jupyter_kernels\n",
    "\n",
    "ln -s /teamspace/studios/this_studio/.jupyter_kernels/share/jupyter/kernels/ml  /opt/jupyter/envs/main/share/jupyter/kernels/ml\n",
    "sudo systemctl restart jupyterlab.service\n",
    "\n",
    "```\n",
    "> JUPYTER_PATH is an environment variable that tells Jupyter to look for kernels (and other data files) in extra directories.\n",
    "> But that doesn't work as that can't reach into application container. (maybe, not verified)\n",
    "> Install to /teamspace/studios/this_studio/.local/share/jupyter also does not work, as reload the jupyterlab service will restore\n",
    "> .local/share/jupyter as well.\n",
    "\n",
    "> Lightning AI tips: Manually customized studio start/stop operation. Like start a app, or clean storage before sleeping...\n",
    "\n",
    "```cmd\n",
    "environment.yml  gpt-oss-20b-fine-tune  main.py  micromamba  requirements.txt\n",
    "‚ö° ~ cat  .lightning_studio/on_start.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# This script runs every time your Studio starts, from your home directory.\n",
    "\n",
    "# Logs from previous runs can be found in ~/.lightning_studio/logs/\n",
    "\n",
    "# List files under fast_load that need to load quickly on start (e.g. model checkpoints).\n",
    "#\n",
    "# ! fast_load\n",
    "# <your file here>\n",
    "\n",
    "# Add your startup commands below.\n",
    "#\n",
    "# Example: streamlit run my_app.py\n",
    "# Example: gradio my_app.py\n",
    "echo \"[on_start] starting at $(date)\"\n",
    "echo \"[on_start] done at $(date)\"\n",
    "‚ö° ~ cat  .lightning_studio/on_stop.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# This script runs every time your Studio sleeps, from your home directory.\n",
    "\n",
    "# Logs from previous runs can be found in ~/.lightning_studio/logs/\n",
    "\n",
    "# Add your shutdown commands below.\n",
    "#\n",
    "# Example: docker down my-container\n",
    "# Example: sudo service mysql stop\n",
    "\n",
    "echo \"[on_stop] cleaning...\"\n",
    "rm -rf /teamspace/studios/this_studio/.cache\n",
    "rm -rf /teamspace/studios/this_studio/tmp_checkpoints\n",
    "echo \"[on_stop] done.\"\n",
    "```\n",
    "\n",
    "\n",
    "## Understand the errors\n",
    "\n",
    "### Why cannot import name 'Inf' from 'numpy' ‚Äî what changed in NumPy?\n",
    "\n",
    "NumPy 2.0 removed several legacy aliases ‚Äî np.Inf / np.NINF / np.Infinity / np.NaN (aliases) ‚Äî in favor of the lowercase forms (np.inf, np.nan). If code (or older compiled extensions) still expects np.Inf, import-time errors or attribute errors appear.\n",
    "\n",
    "Actionable: If you see cannot import name 'Inf' it‚Äôs from code or a compiled extension expecting the old alias. Fix options:\n",
    "\n",
    "Downgrade to a NumPy 1.x-compatible version (or use the exact NumPy version the wheel was built for), or\n",
    "\n",
    "Reinstall the dependent extension compiled against the NumPy you intend to use.\n",
    "\n",
    "### Why ValueError: numpy.dtype size changed happens (ABI / binary mismatch)\n",
    "\n",
    "This is not a Python-level logic bug ‚Äî it‚Äôs a binary compatibility problem between NumPy C-API and compiled extensions (Cython, C/C++ extensions such as those inside pyarrow, pandas, tokenizers, Triton kernels). The community docs and many troubleshooting threads explain this is the classic symptom when packages were built against a different NumPy header than currently installed.\n",
    "\n",
    "Actionable (how to fix):\n",
    "\n",
    "Reinstall NumPy first (or install it from conda), then reinstall packages with C-extensions so they compile/bind against that NumPy.\n",
    "\n",
    "Better: create a fresh conda env (Python 3.10/3.11) and install binary packages (numpy/pandas/pyarrow/torch) via conda channels before pip installs.\n",
    "\n",
    "### Which ML components have trouble on Python 3.12?\n",
    "\n",
    "**2026-01-05 not so scary anymore I guess. Could build workable ML env with python 3.12** \n",
    "\n",
    "Triton: long-standing issues/PRs show Triton did not support 3.12 for a while; many projects depending on Triton (custom GPU kernels) are fragile on 3.12. \n",
    "GitHub\n",
    "+1\n",
    "\n",
    "PyTorch: historically lagged on official wheel support for 3.12; you must check the official PyTorch wheel index for current support (the PyTorch community threads discuss this). If the runtime doesn‚Äôt have appropriate PyTorch wheels for 3.12, installs fail or fall back to incompatible builds. \n",
    "\n",
    "Bitsandbytes / tokenizers / other wheels: bitsandbytes requires specific Python/PyTorch/CUDA combinations; while PyPI may list Python 3.9+, there are reported issues and source-build workarounds for 3.12. Tokenizers and some Hugging Face pieces also saw 3.12-related \n",
    "\n",
    "Important nuance: Some package maintainers do publish 3.12 wheels over time. But uptake is uneven: not guaranteed for every compiled dependency you depend on. That‚Äôs why the conservative recommendation is to use Python 3.10 or 3.11 for ML work today.\n",
    "\n",
    "\n",
    "## Understand package manager \n",
    "\n",
    "### What's the difference between mamba and conda\n",
    "\n",
    "`mamba` = conda, but much faster.\n",
    "It uses the same package repositories, same commands, same environment format, but solves dependencies using a much faster backend (libsolv, in C++). It is a drop-in replacement for conda.\n",
    "\n",
    "1. Speed (the biggest difference)\n",
    "\n",
    "`conda` uses a Python-based dependency solver ‚Üí slow, especially for large ML stacks.\n",
    "\n",
    "`mamba` uses libsolv (C++), the same solver used by Fedora/openSUSE ‚Üí much faster dependency resolution.\n",
    "\n",
    "‚úÖ For real ML projects:\n",
    "Creating large environments with PyTorch, numpy, cudatoolkit, transformers, pyarrow, etc. will take minutes with conda and seconds with mamba.\n",
    "\n",
    "2. Compatibility\n",
    "\n",
    "`mamba` and conda use the same package format, the same channels, and the same environment specs.\n",
    "\n",
    "Any conda command can be replaced with mamba\n",
    "\n",
    "3. Dependency Solving Behavior\n",
    "\n",
    "Both use the same rules, but:\n",
    "\n",
    "`conda` may occasionally get stuck in long solve loops.\n",
    "\n",
    "`mamba` handles big/complex dependency graphs quickly and reliably due to libsolv.\n",
    "\n",
    "4. Installation Footprint\n",
    "\n",
    "`mamba` comes from conda-forge as:\n",
    "\n",
    "`mamba` (standalone replacement for conda commands)\n",
    "\n",
    "`micromamba` (even smaller, can bootstrap whole envs without installing conda)\n",
    "\n",
    "For remote GPU servers or Docker images, people often prefer micromamba to keep containers small.\n",
    "\n",
    "\n",
    "## micromamba\n",
    "\n",
    "`micromamba` is better for fresh ML environments, cloud servers, and production\n",
    "\n",
    "‚≠ê Reason 1 ‚Äî Pure, zero-bloat installation\n",
    "\n",
    "Conda installs hundreds of MB of packages even before you create an environment.\n",
    "Micromamba installs nothing except itself (a 2‚Äì3 MB binary).\n",
    "Perfect for:\n",
    "- **Cloud VMs**\n",
    "- **Docker images**\n",
    "- Remote GPU servers\n",
    "- Clean reproducible ML environments\n",
    "\n",
    "‚≠ê Reason 2 ‚Äî Safer + more reproducible\n",
    "\n",
    "Micromamba:\n",
    "\n",
    "- **Does not auto-update*8\n",
    "- **Does not auto-install extra conda garbage*8\n",
    "- Does not pollute your system\n",
    "- Works entirely inside a single folder (purely isolated)\n",
    "- This **makes your ML environment exactly reproducible**, which is critical for:\n",
    "\n",
    "‚úÖ CUDA correctness\n",
    "‚úÖ Compiled libs\n",
    "‚úÖ PyTorch environments\n",
    "\n",
    "‚≠ê Reason 3 ‚Äî Faster than both conda and mamba\n",
    "\n",
    "Micromamba has:\n",
    "\n",
    "- Instant startup\n",
    "- Faster solver for environment creation\n",
    "- Minimal overhead\n",
    "\n",
    "| Task                                         | Best choice    | Why                               |\n",
    "| -------------------------------------------- | -------------- | --------------------------------- |\n",
    "| Existing conda setup                         | **mamba**      | Drop-in replacement, easy         |\n",
    "| Fresh minimal system (Ubuntu VM, GPU server) | **micromamba** | Fast, small, reproducible         |\n",
    "| Docker images                                | **micromamba** | The industry standard             |\n",
    "| Long-term ML env stability                   | **micromamba** | No surprise updates               |\n",
    "| Local development with GUI tools             | mamba          | Sometimes easier on Windows/macOS |\n",
    "\n",
    "\n",
    "\n",
    "### what is conda-forge ?\n",
    "\n",
    "`conda-forge` is a community-driven collection of conda packages, completely separate from the commercial ‚ÄúAnaconda, Inc.‚Äù channels.\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "‚úÖ A *community-maintained, open-source package repository*\n",
    "‚úÖ Much **larger and more up-to-date** than the default Anaconda channel\n",
    "‚úÖ The **standard source for modern ML, scientific, and data engineering stacks**\n",
    "\n",
    "It‚Äôs basically the PyPI of the conda world‚Äîbut with compiled (pre-built) binaries.\n",
    "\n",
    "#### Why it's important\n",
    "\n",
    "1. Most modern ML packages ship to conda-forge first\n",
    "\n",
    "Examples:\n",
    "\n",
    "PyTorch\n",
    "HuggingFace transformers\n",
    "NumPy\n",
    "SciPy\n",
    "PyArrow\n",
    "Pandas\n",
    "JAX\n",
    "FastAPI\n",
    "UV, ripgrep, and many system-level tools\n",
    "\n",
    "Many packages don‚Äôt even exist in the ‚Äúdefaults‚Äù channel.\n",
    "\n",
    "2. More up-to-date than Anaconda‚Äôs default channel\n",
    "\n",
    "`conda-forge` maintains fast release cycles.\n",
    "\n",
    "Example:\n",
    "\n",
    "PyArrow in conda-forge releases within days.\n",
    "SciPy, NumPy, JAX usually appear in conda-forge first.\n",
    "\n",
    "3. Better dependency solving\n",
    "\n",
    "`conda-forge` tries to keep its entire ecosystem compatible:\n",
    "\n",
    "All packages pinned to consistent versions\n",
    "All use modern compilers/libraries\n",
    "Strong automated build infrastructure\n",
    "This dramatically reduces ‚Äúbinary mismatch‚Äù errors.\n",
    "\n",
    "4. The preferred channel for mamba\n",
    "\n",
    "`mamba` itself is published and maintained via conda-forge.\n",
    "\n",
    "Most mamba users run their entire environment using:\n",
    "\n",
    "`channels`:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - pytorch\n",
    "\n",
    "### what is uv\n",
    "\n",
    "`uv` is essentially a Python tool/environment manager. Think of it as a modern hybrid between:\n",
    "\n",
    "- pip (for installing Python packages)\n",
    "- venv / virtualenv (for creating isolated environments)\n",
    "- pipx (for installing CLI tools in isolated venvs)\n",
    "\n",
    "Modern, high-performance package manager and installer for Python, written in Rust. It's designed to be a significantly faster, all-in-one replacement for multiple traditional Python tools, including pip, pip-tools, and virtualenv.\n",
    "\n",
    "-  Blazing Fast Speed: Due to its implementation in Rust, uv is dramatically faster than pip‚Äîbenchmarks show it can be 10-100x faster for package installation and dependency resolution, especially when leveraging its built-in global cache.\n",
    "\n",
    "-  Drop-in pip and pip-tools Replacement: It offers a compatibility layer, meaning you can often swap out commands like pip install with uv pip install and immediately benefit from the speed improvements.\n",
    "\n",
    "-  *Integrated Environment Management: It eliminates the need for a separate virtualenv or venv tool. uv can create, manage, and synchronize virtual environments directly, which is also significantly faster.*\n",
    "\n",
    "-  Advanced Dependency Resolution: uv is designed with a more robust dependency resolver to better handle complex dependency conflicts and provide clearer error messages\n",
    "\n",
    "-  Universal Tooling: It aims to replace the functionality of numerous other tools like pipx (for installing command-line tools) and even offers features for Python version management (similar to pyenv).\n",
    "\n",
    "-  Reproducibility: It natively supports the creation of lockfiles (uv.lock or a requirements.txt equivalent via uv pip compile), which pin the exact versions of all transitive dependencies to ensure consistent environments across all machines.\n",
    "\n",
    "> **One thing uv doesn't have is --dry-run in pip**\n",
    "> Which actually pretty useful\n",
    "\n",
    "#### Core features of uv\n",
    "\n",
    "Create a Project/Env\n",
    "    **uv init <project-name>**\n",
    "    **Initializes a new project with a virtual environment and a pyproject.toml file.**\n",
    "Create a Venv\n",
    "    uv venv\n",
    "    Creates a virtual environment in the current directory (.venv).\n",
    "Install Packages\n",
    "    uv pip install <package-name>\n",
    "    Use the pip compatible interface for installing packages.\n",
    "Add a Dependency\n",
    "    uv add <package-name>\n",
    "    Adds a package to your pyproject.toml and updates the lockfile/environment.\n",
    "Compile Dependencies\n",
    "    uv pip compile requirements.in -o requirements.txt\n",
    "    uv pip compile pyproject.toml -o uv.lock\n",
    "    Generates a lockfile (requirements.txt) with pinned versions.\n",
    "Sync Environment\n",
    "    uv pip sync requirements.txt \n",
    "    Synchronizes the virtual environment with the exact versions in the lockfile.\n",
    "    Replacement for pip install -r requirements.txt, but faster\n",
    "Sync Environment\n",
    "    **uv sync**\n",
    "    The uv sync command (without specifying a file) assumes you are in a project directory and automatically looks for a generated lockfile (like uv.lock or requirements.txt) to synchronize the environment with the source of truth defined by the pyproject.toml file.\n",
    "Run a Script/Command\n",
    "    uv run python script.py\n",
    "    Executes a command within the project's virtual environment *(related with current dir)*.\n",
    "\n",
    "## Why Colab scripts reinstall numpy and pillow ? \n",
    "\n",
    "Because Google Colab already ships preinstalled versions of NumPy & Pillow that are ABI-incompatible with newer PyTorch wheels.\n",
    "\n",
    "This causes runtime crashes like:\n",
    "- ValueError: numpy.dtype size changed\n",
    "- cannot import name 'Inf' from numpy\n",
    "- binary incompatibility\n",
    "- C-extension import errors\n",
    "\n",
    "Colab's system NumPy/Pillow were compiled with different C compilers / binary formats than the new PyTorch wheels you install.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "If you install new PyTorch without matching these libs ‚Üí Python crashes.\n",
    "\n",
    "‚úÖ Why NumPy + Pillow breaking your environment matters\n",
    "\n",
    "These two packages are special:\n",
    "\n",
    "NumPy \n",
    "- The entire scientific stack depends on it\n",
    "- Has compiled C and Avx2 routines\n",
    "- ABI changes frequently\n",
    "- If one pkg links to NumPy 2.0 but you have 1.26 ‚Üí crash\n",
    "\n",
    "Pillow\n",
    "\n",
    "- Image decoding\n",
    "- Has C libraries (libjpeg, zlib)\n",
    "- PyTorch vision uses Pillow\n",
    "- Mismatch breaks dataset loading\n",
    "\n",
    "‚úÖ Why Unsloth specifically cares\n",
    "\n",
    "Unsloth uses:\n",
    "\n",
    "PyTorch\n",
    "Triton\n",
    "bitsandbytes\n",
    "tokenizers\n",
    "datasets (pyarrow)\n",
    "\n",
    "This stack is extremely sensitive to:\n",
    "\n",
    "- Python version\n",
    "- NumPy ABI\n",
    "- Pillow ABI\n",
    "- CUDA runtime\n",
    "\n",
    "Re-installing NumPy & Pillow ensures everything is built with the same ABI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5490360b-f787-41fb-a5d0-cc3c9562fb44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 numpy==2.3.4 pandas==2.3.3 pyarrow==22.0.0 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445d65cf-5d7e-4ac6-aba8-ea3bc63b6e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets                  4.3.0\n",
       "huggingface-hub           0.34.4\n",
       "numpy                     2.3.4\n",
       "pandas                    2.3.3\n",
       "pyarrow                   22.0.0\n",
       "tokenizers                0.22.1\n",
       "tqdm                      4.67.1\n",
       "transformers              4.56.2\n",
       "trl                       0.22.2\n",
       "unsloth                   2025.12.9\n",
       "unsloth_zoo               2025.12.7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip list | grep -E \"datasets|pandas|tqdm|numpy|pyarrow|transformers|tokenizers|trl|unsloth|unsloth_zoo|huggingface-hub|datasets|numpy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbabafe-f5c6-4739-8d52-93172d3b50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install scipy==1.16.3  scikit-learn==1.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1277b61f-f0c5-4d71-851e-94b8c08bbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db8016aa-48d5-4f1f-9d89-b9d2081a10e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f35801720946ac864e1cb64b31840b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51802737-18c7-4b3e-9edc-7debd0ac77eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "üì• Starting dataset loading process and inspection...\n",
       "\n",
       "-> Loading and selecting 15% randomly from databricks/databricks-dolly-15k (train)\n",
       "\n",
       "-> Loading and selecting 1% randomly from HuggingFaceH4/ultrachat_200k (train_sft)\n",
       "\n",
       "-> Loading and selecting 40% randomly from open-thoughts/open-thoughts-114k (train)\n",
       "\n",
       "-> Loading and selecting 5% randomly from tatsu-lab/alpaca (train)\n",
       "\n",
       "-> Loading and selecting 30% randomly from google/boolq (train)\n",
       "\n",
       "-> Loading and selecting 8% randomly from lmsys/chatbot_arena_conversations (train)\n",
       "\n",
       "üß† Applying OpenThoughts formatter...\n",
       "\n",
       "üß† Applying gsm8k formatter...\n",
       "\n",
       "üß† Applying LMSYS winner extraction logic...\n",
       "\n",
       "üß† Applying dolly formatter...\n",
       "\n",
       "üß† Applying ultrachat formatter...\n",
       "\n",
       "üß† Formatting math formatter...\n",
       "\n",
       "üß† Formatting alpaca formatter...\n",
       "\n",
       "üß† Formatting boolq formatter...\n",
       "\n",
       "üß† Formatting arc formatter...\n",
       "\n",
       "üß† Formatting arc formatter...\n",
       "üóëÔ∏è Applying quality filter...\n",
       "\n",
       "‚úÖ Final dataset successfully filtered and shuffled.\n",
       "--- Entry 1 ---\n",
       "Message with chat template: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
       "Knowledge cutoff: 2024-06\n",
       "Current date: 2025-12-31\n",
       "\n",
       "Reasoning: medium\n",
       "\n",
       "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
       "\n",
       "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
       "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
       "\n",
       "Your task is to:\n",
       "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
       "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
       "  but clearly distinguish between context-based and general reasoning.\n",
       "\n",
       "Your responses must be:\n",
       "- Structured ‚Äî use clear formatting and logical reasoning.\n",
       "- Contextual ‚Äî rely only on the information available.\n",
       "- Concise ‚Äî eliminate filler words while preserving precision.\n",
       "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
       "<|end|><|start|>user<|message|>What is the least spoken language in the world?\n",
       "A: kindergarten class\n",
       "B: mimesis\n",
       "C: sign language\n",
       "D: rhetoric\n",
       "E: late latin<|end|><|start|>assistant<|channel|>final<|message|>Okay, let's try to figure out the answer to this question. The question is asking for the least spoken language in the world, and the options are A through E: kindergarten class, mimesis, sign language, rhetoric, and Late Latin. Hmm, interesting.\n",
       "\n",
       "First, I need to understand each of these options properly. Let me go through them one by one.\n",
       "\n",
       "Starting with option A: kindergarten class. Wait, a kindergarten class isn't a language. It's a place or a group of young students. Maybe this is a trick question, but I don't think \"kindergarten class\" refers to any language. Unless it's a mistranslation or misunderstanding. Maybe in some contexts, but I don't recall any language by that name. So probably, this option is a distractor.\n",
       "\n",
       "Option B: mimesis. Mimesis is a term I've heard in literature or art, referring to imitation or representation. Like how art mimics life. But is that a language? No, mimesis is a concept, not a language. Unless there's a specific sign language or constructed language called mimesis, but I don't think so. Maybe it's a red herring.\n",
       "\n",
       "Option C: sign language. Sign languages are actual languages used by deaf communities. They have their own grammar and structure. There are many sign languages around the world, like American Sign Language, British Sign Language, etc. But the question is about the least spoken. However, sign languages are used by millions of people globally. So even though they're not \"spoken\" in the traditional sense, they are widely used. So maybe not the least spoken, depending on interpretation. But the question says \"spoken,\" which might imply vocal language, but sign languages aren't spoken. So perhaps this is a trick in the wording. If \"spoken\" is literal, then sign language isn't spoken at all, so technically it's the least? But that might be a play on words. But I need to check other options.\n",
       "\n",
       "Option D: rhetoric. Rhetoric is the art of persuasive speaking or writing. It's a field of study, not a language itself. So again, not a language. Unless there's a language named Rhetoric, but I don't think that's the case. So this is probably another distractor.\n",
       "\n",
       "Option E: Late Latin. Latin is a classical language that evolved into Late Latin, which was used in the Late Antiquity period. Eventually, it developed into the Romance languages. But Late Latin itself is considered extinct or no longer spoken as a native language. However, Ecclesiastical Latin is used by the Catholic Church. But in terms of native speakers, there are none. So Late Latin would be a dead language. If that's the case, then it's not spoken at all except maybe in limited contexts. But how does that compare to the other options?\n",
       "\n",
       "Wait, but let's think again. The question is about the \"least spoken\" language. If a language is extinct or has no native speakers, then it's not spoken, making it the least. But among the options, which is a real language that's not spoken? Let's confirm:\n",
       "\n",
       "A: Not a language.\n",
       "\n",
       "B: Not a language.\n",
       "\n",
       "C: Sign language is a language but not spoken; it's signed. However, many people use it. So if the question is about spoken (vocal) languages, sign language wouldn't count, but the question might be using \"spoken\" more generally. But the options include it as a possible answer.\n",
       "\n",
       "D: Not a language.\n",
       "\n",
       "E: Late Latin is a historical form of Latin, not spoken natively anymore. So if the question is looking for a language with the fewest speakers, Late Latin might be the answer because it's extinct. But how does that compare to sign language? For example, if a language has zero speakers, that's the least. But sign languages have millions of users. So if Late Latin is considered a dead language, then it's not spoken, making it the least. But maybe there's a nuance here.\n",
       "\n",
       "Alternatively, could \"kindergarten class\" be a language? Maybe a constructed language or a joke answer. But I don't think so. Similarly, \"rhetoric\" and \"mimesis\" are concepts, not languages. So the real contenders are C and E.\n",
       "\n",
       "But the question is about the least spoken. If \"spoken\" refers to verbal communication, then sign language isn't spoken, so technically it's not spoken at all, making it the least. But that's a bit of a wordplay. Alternatively, if the question is which has the fewest speakers, then Late Latin would have zero or very few (like scholars who can read/write it but don't use it daily). So in that case, E would be the answer.\n",
       "\n",
       "Wait, but some people do study Late Latin and might use it in academic or religious contexts. But as a living, daily spoken language, it's extinct. So compared to sign language, which has active users, Late Latin has none. Therefore, the least spoken language among the options would be Late Latin. So answer E.\n",
       "\n",
       "But I need to check if any other options are actual languages. Let me verify each term again.\n",
       "\n",
       "- Kindergarten class: Not a language.\n",
       "\n",
       "- Mimesis: A concept, not a language.\n",
       "\n",
       "- Sign language: A legitimate category of languages, but not spoken.\n",
       "\n",
       "- Rhetoric: A discipline, not a language.\n",
       "\n",
       "- Late Latin: A historical stage of Latin, no native speakers.\n",
       "\n",
       "Therefore, the answer is E: Late Latin, as it's no longer spoken, making it the least spoken, whereas sign languages are actively used, even if not vocally.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "The question asks for the least spoken language among the given options. Here‚Äôs the systematic breakdown:\n",
       "\n",
       "1. **Option A: Kindergarten Class**  \n",
       "   Not a language. It refers to a group of students or an educational setting, not a linguistic system. **Eliminated.**\n",
       "\n",
       "2. **Option B: Mimesis**  \n",
       "   A concept in art and literature (imitation of reality), not a language. **Eliminated.**\n",
       "\n",
       "3. **Option C: Sign Language**  \n",
       "   A legitimate category of languages (e.g., ASL, BSL) used by deaf communities. While not \"spoken\" vocally, they are actively used by millions globally. **Not the least spoken.**\n",
       "\n",
       "4. **Option D: Rhetoric**  \n",
       "   A field of study focused on persuasive communication, not a language. **Eliminated.**\n",
       "\n",
       "5. **Option E: Late Latin**  \n",
       "   A historical form of Latin from Late Antiquity. It evolved into Romance languages and has no native speakers today. Though studied academically or used ceremonially (e.g., in the Catholic Church), it is extinct as a living, daily spoken language. **Zero active native speakers.**\n",
       "\n",
       "**Conclusion:**  \n",
       "The term \"spoken\" here likely refers to vocal communication. However, even if interpreted broadly, Late Latin has no living speakers, while sign languages are actively used. Thus, **Late Latin (E)** is the least spoken language among the options.\n",
       "\n",
       "**Answer:** E: Late Latin<|end|>...\n",
       "--------------------------------------------------\n",
       "\n",
       "\n",
       "--- Entry 2 ---\n",
       "Message with chat template: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
       "Knowledge cutoff: 2024-06\n",
       "Current date: 2025-12-31\n",
       "\n",
       "Reasoning: medium\n",
       "\n",
       "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
       "\n",
       "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
       "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
       "\n",
       "Your task is to:\n",
       "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
       "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
       "  but clearly distinguish between context-based and general reasoning.\n",
       "\n",
       "Your responses must be:\n",
       "- Structured ‚Äî use clear formatting and logical reasoning.\n",
       "- Contextual ‚Äî rely only on the information available.\n",
       "- Concise ‚Äî eliminate filler words while preserving precision.\n",
       "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
       "<|end|><|start|>user<|message|>how to climb a tree?<|end|><|start|>assistant<|channel|>final<|message|>Climbing a tree can be a fun and challenging activity, but it's important to climb safely. Here are some tips on how to climb a tree:\n",
       "\n",
       "1.   Choose a safe tree: Look for a tree that is sturdy and has strong branches that can support your weight. Avoid trees with dead or rotten branches, as they can be dangerous.\n",
       "2.   Get the right gear: Wear sturdy climbing shoes, a helmet, gloves, and a harness. A harness will help you stay securely in place and prevent you from falling.\n",
       "3.   Get a good hold: Start by finding a branch that is strong enough to support your weight. Use your hands and feet to find a good grip on the branch.\n",
       "4.   Move up the tree: Use your hands and feet to move up the tree, using your legs to push off and your arms to pull yourself up. If you're having trouble, you can try using your hands to find a better grip on the branch or your feet to push off with more force.\n",
       "5.   Take breaks: Climbing can be physically demanding, so it's important to take breaks as needed. You can rest on a branch or on the ground if you need to.\n",
       "6.   Follow safety guidelines: Always follow safety guidelines when climbing trees, such as avoiding standing under a tree that is being climbed and avoiding climbing during high winds or heavy rain.\n",
       "\n",
       "Remember to always climb at your own pace and to stop climbing if you're feeling tired or uncomfortable.<|end|>...\n",
       "--------------------------------------------------\n",
       "\n",
       "\n",
       "--- Entry 3 ---\n",
       "Message with chat template: <|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
       "Knowledge cutoff: 2024-06\n",
       "Current date: 2025-12-31\n",
       "\n",
       "Reasoning: medium\n",
       "\n",
       "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
       "\n",
       "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
       "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
       "\n",
       "Your task is to:\n",
       "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
       "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
       "  but clearly distinguish between context-based and general reasoning.\n",
       "\n",
       "Your responses must be:\n",
       "- Structured ‚Äî use clear formatting and logical reasoning.\n",
       "- Contextual ‚Äî rely only on the information available.\n",
       "- Concise ‚Äî eliminate filler words while preserving precision.\n",
       "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
       "<|end|><|start|>user<|message|>Jack, who weighs 60 pounds, wants to play on the see-saw with his cousin Anna, who weighs 40 pounds. How many 4-pound rocks does Jack need to hold to make their weights equal?<|end|><|start|>assistant<|channel|>final<|message|>First find the total difference in weight between the two people: 60 pounds - 40 pounds = 20 pounds\n",
       "Then divide the weight difference by the weight of each rock to find the number of rocks need to even the weights: 20 pounds / 4 pound/rock = 5 rocks<|end|>...\n",
       "--------------------------------------------------\n",
       "\n",
       "\n",
       "\n",
       "üìä Analyzing Sources in Final Dataset...\n",
       "--------------------------------------------------\n",
       "Final Dataset Size: 30118 entries\n",
       "Total Unique Sources Found: 10\n",
       "--------------------------------------------------\n",
       "Count and Percentage of Data Entries per Source:\n",
       "              Count Percentage\n",
       "OpenThoughts  10413     34.57%\n",
       "GSM8K          7473     24.81%\n",
       "sharegpt4      3582     11.89%\n",
       "boolq          1682      5.58%\n",
       "lmsys_conv     1537      5.10%\n",
       "alpaca         1529      5.08%\n",
       "dolly          1303      4.33%\n",
       "ultrachat      1197      3.97%\n",
       "math            746      2.48%\n",
       "arc             656      2.18%\n",
       "--------------------------------------------------\n",
       "\n",
       "üéâ SUCCESS! Final SFT dataset created: train_sft_final.json\n",
       "Total samples after filtering: 30118\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2Ô∏è‚É£ data format function for different Datasets\n",
    "# ================================================\n",
    "\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "INSPECT_ON = False\n",
    "\n",
    "# Prints ONE random entry\n",
    "def inspect_random_entry(ds_name, ds_object, force_disp = False):\n",
    "\n",
    "    if not force_disp:\n",
    "        if not INSPECT_ON:\n",
    "            return \n",
    "    \n",
    "    \"\"\"Randomly selects and prints one raw sample from the dataset for observation.\"\"\"\n",
    "    print(f\"\\n--- INSPECTION: {ds_name} (ONE RANDOM RAW SAMPLE) ---\")\n",
    "    random_index = random.randint(0, len(ds_object) - 1)\n",
    "    sample = ds_object[random_index]\n",
    "    print(f\"Sample Index: {random_index} Raw Data:\")\n",
    "    print(json.dumps(sample, indent=2, default=str))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "# Prints parts of the formatted datasets\n",
    "def inspect_formatted_sft(ds_name, ds_object, n = 2, disp_len = 50, force_disp = False):\n",
    "\n",
    "    if not force_disp:\n",
    "        if not INSPECT_ON:\n",
    "            return \n",
    "    \n",
    "    for i, entry in enumerate(ds_object):\n",
    "        if i > n:\n",
    "            break\n",
    "        print(f\"--- Entry {i+1} ---\")\n",
    "        print(f\"Instruction: {entry['instruction'][:disp_len]}...\") # Truncating for readability\n",
    "        print(f\"Response: {entry['response'][:disp_len]}\")\n",
    "        print(f\"Source: {entry['source']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "#  EXTRACTION FUNCTIONS. Convert various of datasets to  { \"instruction\": \"xxx\", \"response\": \"xxx\", \"source\": \"xxx\" }\n",
    "def extract_lmsys_winner(example):\n",
    "    \"\"\"\n",
    "    Identifies the winning conversation (model_a or model_b) and extracts the \n",
    "    first instruction/response pair from that conversation list.\n",
    "    \"\"\"\n",
    "    winner_key = example.get('winner')\n",
    "    \n",
    "    if winner_key == 'model_a':\n",
    "        conv_list = example.get('conversation_a')\n",
    "    elif winner_key == 'model_b':\n",
    "        conv_list = example.get('conversation_b')\n",
    "    else: # Handles 'tie', 'tie (a/b)', or missing winner by defaulting to A\n",
    "        conv_list = example.get('conversation_a')\n",
    "        \n",
    "    if not conv_list or len(conv_list) < 2:\n",
    "        return {\"instruction\": None, \"response\": None}\n",
    "\n",
    "    instruction = conv_list[0]['content'] if conv_list[0]['role'] == 'user' else None\n",
    "    response = conv_list[1]['content'] if conv_list[1]['role'] == 'assistant' else None\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"lmsys_conv\"}\n",
    "\n",
    "\n",
    "def format_ultrachat(example):\n",
    "    instruction, response = None, None\n",
    "    \n",
    "    user_messages = [m['content'] for m in example['messages'] if m['role'] == 'user']\n",
    "    model_messages = [m['content'] for m in example['messages'] if m['role'] == 'assistant']\n",
    "    instruction = user_messages[0] if user_messages else None\n",
    "    response = model_messages[0] if model_messages else None\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"ultrachat\"}\n",
    "\n",
    "\n",
    "def format_dolly(example):\n",
    "    \"\"\"\n",
    "    Formats a single Dolly 15k entry, integrating 'context' into the 'instruction'\n",
    "    when available. Assumes the primary keys are 'instruction', 'context', and 'response'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core fields for Dolly (will be None/empty string if not present)\n",
    "    core_instruction = example.get(\"instruction\", \"\")\n",
    "    context = example.get(\"context\", \"\")\n",
    "    response = example.get(\"response\", \"\")\n",
    "    \n",
    "    # Initialize the instruction to be returned\n",
    "    final_instruction = \"\"\n",
    "\n",
    "    # --- 1. Build the Instruction ---\n",
    "    \n",
    "    # Check if context exists and is not just whitespace\n",
    "    if isinstance(context, str) and context.strip():\n",
    "        # Prepend the context to the instruction clearly\n",
    "        final_instruction = (\n",
    "            f\"Context: {context.strip()}\\n\\n\"\n",
    "            f\"Instruction: {core_instruction.strip()}\"\n",
    "        )\n",
    "    elif isinstance(core_instruction, str) and core_instruction.strip():\n",
    "        # If no context, just use the core instruction\n",
    "        final_instruction = core_instruction.strip()\n",
    "    # Note: If both are empty, final_instruction remains \"\" (which will be filtered later).\n",
    "\n",
    "    # --- 2. Keep the Response and Source ---\n",
    "    \n",
    "    if isinstance(response, str):\n",
    "        final_response = response.strip()\n",
    "    else:\n",
    "        final_response = response # Keep None/other type for filtering\n",
    "\n",
    "    return {\n",
    "        \"instruction\": final_instruction, \n",
    "        \"response\": final_response, \n",
    "        \"source\": \"dolly\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "import re \n",
    "# Note: You'd need to ensure 'import re' is at the top of your main script \n",
    "# or use pd.Series as you did before, but the pure Python approach is cleaner.\n",
    "\n",
    "def format_GSM8K(example):\n",
    "    \"\"\"\n",
    "    Formats a single GSM8K entry by extracting the question and cleaning \n",
    "    the step-by-step answer (solution) by removing calculator tokens and \n",
    "    the final answer marker.\n",
    "    \"\"\"\n",
    "    \n",
    "    # GSM8K uses 'question' for instruction and 'answer' for response\n",
    "    instruction = example.get(\"question\")\n",
    "    response = example.get(\"answer\")\n",
    "\n",
    "    if isinstance(response, str):\n",
    "        # --- FINAL FIXES FOR GSM8K using pure Python re module ---\n",
    "        \n",
    "        # 1. Remove GSM8K Calculator Tokens (e.g., <<16+20=36>>)\n",
    "        # The re.sub is much cleaner than the pd.Series chaining.\n",
    "        response = re.sub(r'<<.*?>>', '', response).strip()\n",
    "\n",
    "        # 2. Remove the GSM8K Final Answer Marker (e.g., #### 40 or #### 12.5)\n",
    "        # Using [^\\n]+ to catch any numerical/decimal/fractional answer following the marker\n",
    "        response = re.sub(r'####\\s*[^\\n]+', '', response).strip()\n",
    "    \n",
    "    # The 'instruction' only needs to be the question\n",
    "    if isinstance(instruction, str):\n",
    "        instruction = instruction.strip()\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"GSM8K\"}\n",
    "\n",
    "def format_OpenThoughts(example):\n",
    "    instruction, response = None, None\n",
    "\n",
    "    # Check for the 'conversations' key typical of OpenThoughts structure\n",
    "    if 'conversations' in example and isinstance(example['conversations'], list):\n",
    "        # The user's instruction is the 'value' of the first 'user' turn\n",
    "        # 1. Extract Instruction (User's turn)\n",
    "        user_turn = next((c for c in example['conversations'] if c['from'] == 'user'), None)\n",
    "        if user_turn:\n",
    "            instruction = user_turn.get('value')\n",
    "\n",
    "        # 2. Extract Response (Assistant's turn)\n",
    "        assistant_turn = next((c for c in example['conversations'] if c['from'] == 'assistant'), None)\n",
    "        if assistant_turn:\n",
    "            response = assistant_turn.get('value')\n",
    "    \n",
    "\n",
    "    if isinstance(response, str):\n",
    "        # Apply the CoT cleaning logic\n",
    "        response = response.replace(\"<|begin_of_thought|>\", \"\").replace(\"<|end_of_thought|>\", \"\").strip()\n",
    "        response = response.replace(\"<|begin_of_solution|>\", \"\").replace(\"<|end_of_solution|>\", \"\").strip()\n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"OpenThoughts\"}    \n",
    "    \n",
    "def format_math(example):\n",
    "    \n",
    "    return {\"instruction\": example[\"problem\"], \"response\": example[\"solution\"], \"source\": \"math\"}\n",
    "\n",
    "\n",
    "def format_alpaca(example):\n",
    "    \n",
    "    # Base instruction\n",
    "    inst = example.get(\"instruction\", \"\").strip()\n",
    "    inp = example.get(\"input\", \"\")\n",
    "    out = example.get(\"output\") or example.get(\"response\") or example.get(\"answer\") or \"\"\n",
    "\n",
    "    # If input exists and is non-empty, append it with a label\n",
    "    if isinstance(inp, str) and inp.strip():\n",
    "        inst = f\"{inst}\\n\\nInput:\\n{inp.strip()}\"\n",
    "\n",
    "    # final cleaning\n",
    "    inst = inst.strip()\n",
    "    out = out.strip()\n",
    "\n",
    "    return {\"instruction\": inst, \"response\": out, \"source\": \"alpaca\"}\n",
    "\n",
    "\n",
    "def format_sharegpt(example):\n",
    "    \n",
    "    if \"conversations\" not in example or len(example[\"conversations\"]) < 2:\n",
    "        return {\"instruction\": None, \"response\": None}\n",
    "    convs = example[\"conversations\"]\n",
    "    user_turns = [c[\"value\"] for c in convs if c[\"from\"] == \"human\"]\n",
    "    assistant_turns = [c[\"value\"] for c in convs if c[\"from\"] == \"gpt\"]\n",
    "    \n",
    "    return {\n",
    "        \"instruction\": user_turns[0] if user_turns else None,\n",
    "        \"response\": assistant_turns[0] if assistant_turns else None,\n",
    "    }\n",
    "    \n",
    "def format_boolq(example):\n",
    "    \n",
    "    q = example.get(\"question\", \"\").strip()\n",
    "    p = example.get(\"passage\", \"\").strip()\n",
    "    a = example.get(\"answer\", None)\n",
    "\n",
    "    # Convert boolean answer to natural language\n",
    "    if a is True:\n",
    "        answer_text = \"Yes\"\n",
    "    elif a is False:\n",
    "        answer_text = \"No\"\n",
    "    else:\n",
    "        answer_text = \"\"\n",
    "\n",
    "    # Build structured instruction\n",
    "    if p:\n",
    "        instruction = f\"Question: {q}\\n\\nContext:\\n{p}\"\n",
    "    else:\n",
    "        instruction = f\"Question: {q}\"\n",
    "\n",
    "    return {\"instruction\": instruction.strip(), \"response\": answer_text.strip(), \"source\": \"boolq\"}\n",
    "\n",
    "def format_arc(sample):\n",
    "    \"\"\"\n",
    "    Converts a single sample from the allenai/ai2_arc dataset into \n",
    "    the Supervised Fine-Tuning (SFT) instruction-response format.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): A single dictionary sample from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary in the SFT format {\"instruction\": ..., \"response\": ...}.\n",
    "    \"\"\"\n",
    "    # 1. Extract the main question and the answer key\n",
    "    question = sample['question']\n",
    "    answer_key = sample['answerKey']\n",
    "    \n",
    "    # 2. Extract the choices and labels\n",
    "    choices_text = sample['choices']['text']\n",
    "    choices_label = sample['choices']['label']\n",
    "    \n",
    "    # 3. Create the formatted list of choices (e.g., \"A. pulley.\\nB. lever.\\n...\")\n",
    "    formatted_choices = []\n",
    "    # Use zip to pair the labels (A, B, C, D) with the corresponding text\n",
    "    for label, text in zip(choices_label, choices_text):\n",
    "        formatted_choices.append(f\"{label}. {text}\")\n",
    "        \n",
    "    # Join the choices with newlines to make them easy for the model to read\n",
    "    choices_block = \"\\n\".join(formatted_choices)\n",
    "    \n",
    "    # 4. Construct the full 'instruction' prompt\n",
    "    instruction = (\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Choices:\\n{choices_block}\\n\\n\"\n",
    "        \"Please select the letter (A, B, C, or D) that represents the correct answer.\"\n",
    "    )\n",
    "    \n",
    "    # 5. Determine the correct 'response'\n",
    "    \n",
    "    # Find the index of the answer key (e.g., 'B' is at index 1)\n",
    "    answer_index = choices_label.index(answer_key)\n",
    "    \n",
    "    # Get the text corresponding to the answer key\n",
    "    correct_answer_text = choices_text[answer_index]\n",
    "    \n",
    "    # The response should include the key and the full text for a complete answer\n",
    "    response = f\"Correct Answer: {answer_key}. {correct_answer_text}\"\n",
    "    \n",
    "    # 6. Return the final SFT dictionary\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"response\": response,\n",
    "        \"source\": \"arc\"\n",
    "    }\n",
    "\n",
    "def format_hotpot(example):\n",
    "    context_text = \"\\n\".join(example[\"context\"][\"sentences\"][0]) if \"context\" in example and isinstance(example[\"context\"], dict) else \"\"\n",
    "    response = f\"Answer: {example['answer']}\\n\\nContext:\\n{context_text}\"\n",
    "    return {\"instruction\": example[\"question\"], \"response\": response}\n",
    "\n",
    "\n",
    "def format_gpt4(example):\n",
    "    instruction, response = None, None\n",
    "\n",
    "    # Check for the 'conversations' key typical of OpenThoughts structure\n",
    "    if 'items' in example and isinstance(example['items'], list):\n",
    "        # The user's instruction is the 'value' of the first 'user' turn\n",
    "        # 1. Extract Instruction (User's turn)\n",
    "        user_turn = next((c for c in example['items'] if c['from'] == 'human'), None)\n",
    "        if user_turn:\n",
    "            instruction = user_turn.get('value')\n",
    "\n",
    "        # 2. Extract Response (Assistant's turn)\n",
    "        assistant_turn = next((c for c in example['items'] if c['from'] == 'gpt'), None)\n",
    "        if assistant_turn:\n",
    "            response = assistant_turn.get('value')\n",
    "    \n",
    "    \n",
    "    return {\"instruction\": instruction, \"response\": response, \"source\": \"sharegpt4\"}    \n",
    "    \n",
    "\n",
    "# ================================================\n",
    "# 3Ô∏è‚É£ load data from different Datasets\n",
    "# ================================================\n",
    "\n",
    "\n",
    "print(\"üì• Starting dataset loading process and inspection...\")\n",
    "\n",
    "\n",
    "# Set a consistent seed for reproducible random sampling\n",
    "SEED = 42 \n",
    "\n",
    "def load_and_random_select(dataset_name, split_name, percentage, config_name=None):\n",
    "    \"\"\"Loads a split, shuffles it, and selects the specified percentage of entries.\"\"\"\n",
    "    print(f\"\\n-> Loading and selecting {percentage*100:.0f}% randomly from {dataset_name} ({split_name})\")\n",
    "    \n",
    "    # 1. Load the full split\n",
    "    if config_name:\n",
    "        ds_full = load_dataset(dataset_name, config_name, split=split_name)\n",
    "    else:\n",
    "        ds_full = load_dataset(dataset_name, split=split_name)\n",
    "    \n",
    "    # Calculate the number of entries to select\n",
    "    num_to_select = int(len(ds_full) * percentage)\n",
    "    \n",
    "    # 2. Shuffle the dataset and select the required number of entries\n",
    "    # The 'seed' makes the random selection reproducible\n",
    "    ds_random = ds_full.shuffle(seed=SEED).select(range(num_to_select))\n",
    "    \n",
    "    return ds_random\n",
    "\n",
    "\n",
    "#Conversational tone / flow  Ultrachat, Dolly 45 %  Teaches helpful, polite phrasing and structure\n",
    "\n",
    "#dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:80%]\")\n",
    "dolly = load_and_random_select(\"databricks/databricks-dolly-15k\", \"train\", 0.15)\n",
    "inspect_random_entry(\"databricks/databricks-dolly-15k\", dolly)\n",
    "\n",
    "#ultrachat = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:10%]\")\n",
    "ultrachat = load_and_random_select(\"HuggingFaceH4/ultrachat_200k\", \"train_sft\", 0.01)\n",
    "inspect_random_entry(\"HuggingFaceH4/ultrachat_200k\", ultrachat)\n",
    "\n",
    "# Structured reasoning (CoT) Open-Thoughts, GSM8K, MATH 35 % Injects step-by-step clarity and logical progression\n",
    "\n",
    "#cot_thoughts = load_dataset(\"open-thoughts/open-thoughts-114k\", split=\"train[:21%]\")\n",
    "cot_thoughts = load_and_random_select(\"open-thoughts/open-thoughts-114k\", \"train\", 0.40)\n",
    "inspect_random_entry(\"open-thoughts/open-thoughts-114k\", cot_thoughts)\n",
    "\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\") \n",
    "inspect_random_entry(\"openai/gsm8k\", gsm8k)\n",
    "\n",
    "math_ds = load_dataset(\"HuggingFaceH4/math\", split=\"train\")\n",
    "inspect_random_entry(\"HuggingFaceH4/math\", math_ds)\n",
    "\n",
    "# Instruction diversity / broad generality Alpaca, ARC, BoolQ 15 % Keeps model generalist and able to follow any prompt\n",
    "#alpaca = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:15%]\")\n",
    "alpaca = load_and_random_select(\"tatsu-lab/alpaca\", \"train\", 0.05)\n",
    "inspect_random_entry(\"tatsu-lab/alpaca\", alpaca)\n",
    "\n",
    "#boolq = load_dataset(\"google/boolq\", split=\"train[:30%]\")\n",
    "boolq = load_and_random_select(\"google/boolq\", \"train\", 0.30)\n",
    "inspect_random_entry(\"google/boolq\", boolq)\n",
    "\n",
    "arc = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"train\")\n",
    "inspect_random_entry(\"allenai/ai2_arc\", arc)\n",
    "\n",
    "# Human-preferred style LMSYS winners 5 % ‚ÄúStyle anchor‚Äù for alignment and empathy\n",
    "#lmsys_conv_raw = load_dataset(\"lmsys/chatbot_arena_conversations\", split=\"train[:15%]\") \n",
    "lmsys_conv_raw = load_and_random_select(\"lmsys/chatbot_arena_conversations\", \"train\", 0.08)\n",
    "inspect_random_entry(\"lmsys/chatbot_arena_conversations\", lmsys_conv_raw)\n",
    "\n",
    "\n",
    "# lmsys/sharegpt4-dataset (subset)\n",
    "#sharegpt = load_dataset(\"anon8231489123/ShareGPT_Vicuna_unfiltered\", split=\"train[:3%]\")\n",
    "#sharegpt = load_dataset(\"LDJnr/ShareGPT-processed\", split=\"train[:5%]\")\n",
    "#sharegpt = load_dataset(\"lmsys/sharegpt4-dataset\", split=\"train[:5%]\")\n",
    "#inspect_random_entry(\"lmsys/sharegpt4-dataset\", sharegpt)\n",
    "\n",
    "sharegpt4 = load_dataset(\n",
    "    \"openchat/openchat_sharegpt4_dataset\",\n",
    "    data_files=\"sharegpt_gpt4.json\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "inspect_random_entry(\"openchat/openchat_sharegpt4_dataset/sharegpt_gpt4.json\", sharegpt4, force_disp = False)\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 4Ô∏è‚É£ format datasets to instruction:response for SFT\n",
    "# ================================================\n",
    "\n",
    "\n",
    "print(\"\\nüß† Applying OpenThoughts formatter...\")\n",
    "cot_thoughts_formatted = cot_thoughts.map(format_OpenThoughts, remove_columns=cot_thoughts.column_names)\n",
    "inspect_formatted_sft(\"cot_thoughts_formatted\", cot_thoughts_formatted)\n",
    "\n",
    "print(\"\\nüß† Applying gsm8k formatter...\")\n",
    "gsm8k_formatted = gsm8k.map(format_GSM8K, remove_columns=gsm8k.column_names)\n",
    "inspect_formatted_sft(\"gsm8k_formatted\", gsm8k_formatted, 2, 200)\n",
    "\n",
    "print(\"\\nüß† Applying LMSYS winner extraction logic...\")\n",
    "lmsys_conv_formatted = lmsys_conv_raw.map(extract_lmsys_winner, remove_columns=lmsys_conv_raw.column_names)\n",
    "inspect_formatted_sft(\"lmsys_conv_formatted\", lmsys_conv_formatted)\n",
    "\n",
    "print(\"\\nüß† Applying dolly formatter...\")\n",
    "dolly_formatted = dolly.map(format_dolly, remove_columns=dolly.column_names)\n",
    "inspect_formatted_sft(\"dolly_formatted\", dolly_formatted)\n",
    "\n",
    "print(\"\\nüß† Applying ultrachat formatter...\")\n",
    "ultrachat_formatted = ultrachat.map(format_ultrachat, remove_columns=ultrachat.column_names)\n",
    "inspect_formatted_sft(\"ultrachat_formatted\", ultrachat_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting math formatter...\")\n",
    "math_formatted = math_ds.map(format_math, remove_columns=math_ds.column_names)\n",
    "inspect_formatted_sft(\"math_formatted\", math_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting alpaca formatter...\")\n",
    "alpaca_formatted = alpaca.map(format_alpaca, remove_columns=alpaca.column_names)\n",
    "inspect_formatted_sft(\"alpaca_formatted\", alpaca_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting boolq formatter...\")\n",
    "boolq_formatted = boolq.map(format_boolq, remove_columns=boolq.column_names)\n",
    "inspect_formatted_sft(\"boolq_formatted\", boolq_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting arc formatter...\")\n",
    "arc_formatted = arc.map(format_arc, remove_columns=arc.column_names)\n",
    "inspect_formatted_sft(\"arc_formatted\", arc_formatted)\n",
    "\n",
    "print(\"\\nüß† Formatting arc formatter...\")\n",
    "gpt4_formatted = sharegpt4.map(format_gpt4, remove_columns=sharegpt4.column_names)\n",
    "inspect_formatted_sft(\"gpt4_formatted\", gpt4_formatted, force_disp = False)\n",
    "\n",
    "# ================================================\n",
    "# 5Ô∏è‚É£  Merge datasets together\n",
    "# ================================================\n",
    "\n",
    "\n",
    "formatted_datasets = []\n",
    "\n",
    "formatted_datasets.extend([\n",
    "    # Training Chain-of-Thought (CoT) and structured output.\n",
    "    cot_thoughts_formatted,\n",
    "\n",
    "    # Injecting the human-preferred response style (the \"winner's\" style) directly from real-world usage data.\n",
    "    lmsys_conv_formatted,\n",
    "    \n",
    "    # Teaches the model to follow multi-step arithmetic and symbolic logic precisely. \n",
    "    # Foundation for numerical and multi-step reasoning.\n",
    "    #gsm8k_formatted,\n",
    "\n",
    "    # Natural Tone & Diversity. output is conversational, polite, and natural (human-written style)\n",
    "    dolly_formatted,\n",
    "\n",
    "    # Provides a massive volume of clean, multi-turn dialogues. Your SFT extraction\n",
    "    # flattens these into single turns, but the model still learns the density of\n",
    "    # conversation‚Äîthe expectation of clear, concise, and structured responses in a\n",
    "    # chat environment.\n",
    "    ultrachat_formatted,\n",
    "    \n",
    "    #math_formatted,\n",
    "\n",
    "    # instruction-following dataset \n",
    "    alpaca_formatted,\n",
    "\n",
    "    # designed for Question Answering (QA) tasks\n",
    "    # Implicit Reasoning: Answering BoolQ often requires more complex reasoning and\n",
    "    # inference than simply locating a specific keyword or phrase. \n",
    "    boolq_formatted,\n",
    "\n",
    "    # AI2 ARC (AI2 Reasoning Challenge) is a benchmark dataset created by the Allen\n",
    "    # Institute for AI to evaluate AI systems on grade-school level science reasoning.\n",
    "    arc_formatted,\n",
    "\n",
    "    # High-quality instruction‚Äìresponse pairs\n",
    "    # GPT-4-level responses (high reasoning, coherence, and helpfulness)\n",
    "    gpt4_formatted,\n",
    "])\n",
    "\n",
    "\n",
    "mixed = concatenate_datasets(formatted_datasets).shuffle(seed=42)\n",
    "\n",
    "# ================================================\n",
    "# 6Ô∏è‚É£  merged datasets filter\n",
    "# ================================================\n",
    "\n",
    "print(\"üóëÔ∏è Applying quality filter...\")\n",
    "\n",
    "def filter_null_or_empty(example):\n",
    "    inst = example.get(\"instruction\")\n",
    "    resp = example.get(\"response\")\n",
    "    \n",
    "    if not isinstance(inst, str) or not isinstance(resp, str):\n",
    "        return False\n",
    "    if len(inst.strip()) < 5 or len(resp.strip()) < 2:\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "mixed = mixed.filter(filter_null_or_empty)\n",
    "\n",
    "\n",
    "MAX_SAMPLES = 38000 \n",
    "\n",
    "# Shuffle the dataset first (use a seed for reproducibility)\n",
    "shuffled_dataset = mixed.shuffle(seed=42)\n",
    "\n",
    "# Then, select the first N samples from the now-shuffled dataset\n",
    "dataset_random = shuffled_dataset.select(range(min(MAX_SAMPLES, len(shuffled_dataset))))\n",
    "\n",
    "# take all of gsmk8 math\n",
    "# combined_dataset = concatenate_datasets([dataset_random, gpt4_formatted, gsm8k_formatted, math_formatted ])\n",
    "combined_dataset = concatenate_datasets([dataset_random, gsm8k_formatted, math_formatted ])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "MAX_SEQ_LEN = 4096\n",
    "\n",
    "# Define your custom system prompt\n",
    "CUSTOM_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a highly professional, concise technical expert across modern computing domains ‚Äî \n",
    "including software architecture, cloud infrastructure, data systems, machine learning, and applied AI.\n",
    "\n",
    "Your task is to:\n",
    "- Answer the user‚Äôs question using the provided CONTEXT as your primary source.\n",
    "- If the CONTEXT does not contain enough information, use your own knowledge,\n",
    "  but clearly distinguish between context-based and general reasoning.\n",
    "\n",
    "Your responses must be:\n",
    "- Structured ‚Äî use clear formatting and logical reasoning.\n",
    "- Contextual ‚Äî rely only on the information available.\n",
    "- Concise ‚Äî eliminate filler words while preserving precision.\n",
    "- Aligned with industry best practices ‚Äî modern, reproducible, and standards-based.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Just load the tokenizer ‚Äî no GPU or model weights needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #\"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gpt-oss-20b\",\n",
    "    \n",
    "     # Use Fast tokenizer\n",
    "    use_fast=True,     \n",
    "    \n",
    "    # Required for Unsloth custom templates\n",
    "    trust_remote_code=True,  \n",
    ")\n",
    "\n",
    "\n",
    "# Add customized prompt and apply chat template, then use tokenizer compute the length to token\n",
    "# Only accept length <= 4096 (max_seq_length)\n",
    "def is_length_within_limit(example):\n",
    "    # Create a Harmony-format conversation\n",
    "    messages = [\n",
    "        # {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": example.get(\"instruction\", \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": example.get(\"response\", \"\")},\n",
    "    ]\n",
    "    wrapped = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        tokenize=True,\n",
    "    )\n",
    "    #print(wrapped)\n",
    "    token_count = len(wrapped[\"input_ids\"]) if isinstance(wrapped, dict) else len(wrapped)\n",
    "    return token_count <= MAX_SEQ_LEN\n",
    "\n",
    "\n",
    "\n",
    "# Use the .filter() method which returns a new Dataset object\n",
    "filtered_dataset = combined_dataset.filter(\n",
    "    is_length_within_limit,\n",
    "    num_proc=4 # Use multiple processes to speed up tokenization/filtering\n",
    ")\n",
    "\n",
    "# .shuffle() on the Dataset object\n",
    "final_dataset_random = filtered_dataset.shuffle(seed=42)\n",
    "\n",
    "print(\"\\n‚úÖ Final dataset successfully filtered and shuffled.\")\n",
    "\n",
    "\n",
    "# Prints parts of the datasets with chat template\n",
    "def inspect_formatted_sft(ds_name, ds_object, n = 2, force_disp = False):\n",
    "\n",
    "    if not force_disp:\n",
    "        if not INSPECT_ON:\n",
    "            return \n",
    "    \n",
    "    for i, entry in enumerate(ds_object):\n",
    "        if i > n:\n",
    "            break\n",
    "        print(f\"--- Entry {i+1} ---\")\n",
    "\n",
    "            # Create a Harmony-format conversation\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": CUSTOM_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": entry.get(\"instruction\", \"\")},\n",
    "            {\"role\": \"assistant\", \"content\": entry.get(\"response\", \"\")},\n",
    "        ]\n",
    "        wrapped = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"Message with chat template: {wrapped}...\") \n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "inspect_formatted_sft(\"final_dataset_random\", final_dataset_random, force_disp = True)\n",
    "\n",
    "# ================================================\n",
    "# 7Ô∏è‚É£  Analysis datasets 1. percentage of each source 2. max length(token) of each source\n",
    "# ================================================\n",
    "\n",
    "\n",
    "# Analysis: Getting the Source Statistics\n",
    "print(\"\\nüìä Analyzing Sources in Final Dataset...\")\n",
    "\n",
    "# Extract the 'source' column into a list\n",
    "sources_list = final_dataset_random['source']\n",
    "\n",
    "# Get the total size of the final dataset\n",
    "final_dataset_size = len(final_dataset_random)\n",
    "\n",
    "# Convert the list to a Pandas Series and count the unique values (This gives the 'Count' column)\n",
    "source_counts = pd.Series(sources_list).value_counts()\n",
    "\n",
    "# Calculate the total number of unique sources\n",
    "total_unique_sources = len(source_counts)\n",
    "\n",
    "# Create a DataFrame for summary and add the Percentage column \n",
    "# Convert the Series to a DataFrame\n",
    "summary_df = source_counts.rename('Count').to_frame()\n",
    "\n",
    "# Calculate the percentage\n",
    "summary_df['Percentage'] = (summary_df['Count'] / final_dataset_size) * 100\n",
    "\n",
    "# Format the Percentage column for clean display\n",
    "summary_df['Percentage'] = summary_df['Percentage'].map('{:.2f}%'.format)\n",
    "\n",
    "# Printing the Summary\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final Dataset Size: {final_dataset_size} entries\")\n",
    "print(f\"Total Unique Sources Found: {total_unique_sources}\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Count and Percentage of Data Entries per Source:\")\n",
    "print(summary_df.to_string()) # Print the DataFrame with both Count and Percentage\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Save as JSONL for Unsloth\n",
    "df = pd.DataFrame(final_dataset_random)\n",
    "df.to_json(\"train_sft_final.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"\\nüéâ SUCCESS! Final SFT dataset created: train_sft_final.json\")\n",
    "print(f\"Total samples after filtering: {len(final_dataset_random)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c550bff-0143-4cfe-ab14-887fb1f1a880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Maximum Token Length (Instruction + Response) per Source:\n",
       "-------------------------------------------------------\n",
       "source\n",
       "OpenThoughts    4025\n",
       "sharegpt4       3728\n",
       "dolly           3337\n",
       "math            2445\n",
       "ultrachat       2366\n",
       "lmsys_conv      1256\n",
       "boolq            580\n",
       "alpaca           444\n",
       "GSM8K            409\n",
       "arc              235\n",
       "Name: total_tokens, dtype: int64\n",
       "Count of Entries with Token Length > 4096 per Source:\n",
       "Series([], dtype: int64)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Alternatively, a cleaner map that keeps only source and tokens\n",
    "def prepare_data_for_analysis(example, tokenizer):\n",
    "    combined_text = example['instruction'] + \" \" + example['response']\n",
    "    tokenized = tokenizer(combined_text, truncation=False, return_length=True)\n",
    "    return {\n",
    "        'source': example['source'], # Assuming you have a 'source' column\n",
    "        'total_tokens': tokenized['length'][0]\n",
    "    }\n",
    "\n",
    "analysis_dataset = final_dataset_random.map(\n",
    "    lambda x: prepare_data_for_analysis(x, tokenizer),\n",
    "    batched=False, # Process one example at a time\n",
    "    num_proc=4,\n",
    "    remove_columns=final_dataset_random.column_names # Keep only the two new columns\n",
    ")\n",
    "\n",
    "# Convert the analysis dataset to a Pandas DataFrame\n",
    "analysis_df = analysis_dataset.to_pandas()\n",
    "\n",
    "# Group by the 'source' column and find the maximum 'total_tokens'\n",
    "max_lengths_by_source = analysis_df.groupby('source')['total_tokens'].max().sort_values(ascending=False)\n",
    "\n",
    "## Output the result\n",
    "print(\"Maximum Token Length (Instruction + Response) per Source:\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(max_lengths_by_source)\n",
    "\n",
    "\n",
    "# Convert to Pandas for aggregation\n",
    "analysis_df = analysis_dataset.to_pandas()\n",
    "\n",
    "# Filter for entries where token length is greater than the threshold\n",
    "large_entries_df = analysis_df[analysis_df['total_tokens'] > MAX_SEQ_LEN]\n",
    "\n",
    "#  Group by 'source' and count the number of entries\n",
    "count_by_source = large_entries_df.groupby('source').size().sort_values(ascending=False)\n",
    "\n",
    "# Print the final result\n",
    "print(f\"Count of Entries with Token Length > {MAX_SEQ_LEN} per Source:\")\n",
    "print(count_by_source)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb77e2-9b61-4c38-a07e-877c39b44649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
