{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed153c43-9c31-4051-95a1-8e1d2055f55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: /home/ospost/micromamba/envs/ML\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m                                          \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 0.15ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================\n",
    "# 1ï¸âƒ£ Env Prepare Install Required Packages\n",
    "# ================================================\n",
    "\n",
    "#%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5da945-1efe-4050-8fbd-b35cab526c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ospost/micromamba/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2026.1.2: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    GRID A100D-40C. Num GPUs = 1. Max memory: 39.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1. CUDA: 8.0. CUDA Toolkit: 12.9. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB 4-bit layers found: 1632\n",
      "Sample BNB layer: model.layers.0.self_attn.q_proj\n",
      "Model compute dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from collections import Counter\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/gpt-oss-20b\",\n",
    "#    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "bnb_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, bnb.nn.Linear4bit):\n",
    "        bnb_layers.append(name)\n",
    "\n",
    "print(f\"BNB 4-bit layers found: {len(bnb_layers)}\")\n",
    "print(\"Sample BNB layer:\", bnb_layers[0] if bnb_layers else \"âŒ NONE\")\n",
    "print(\"Model compute dtype:\", model.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1667548e-9bbc-4947-8a27-9dbef6178f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    checkpoint_path=\"ospost/gpt-oss-20b-sft-adapter\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7462d3c3-3ac6-465a-be7c-4690d33829dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA parameters found: 192\n",
      "Sample LoRA param: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "LoRA dtypes: Counter({torch.float32: 192})\n",
      "Trainable params count: 192\n",
      "Sample trainable param: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n"
     ]
    }
   ],
   "source": [
    "assert any(\"lora_\" in n for n,_ in model.named_parameters())\n",
    "\n",
    "lora_params = [\n",
    "    (n, p) for n, p in model.named_parameters() if \"lora_\" in n\n",
    "]\n",
    "\n",
    "print(f\"LoRA parameters found: {len(lora_params)}\")\n",
    "print(\"Sample LoRA param:\", lora_params[0][0] if lora_params else \"âŒ NONE\")\n",
    "\n",
    "print(\"LoRA dtypes:\", Counter(p.dtype for _, p in lora_params))\n",
    "\n",
    "trainable = [\n",
    "    n for n, p in model.named_parameters() if p.requires_grad\n",
    "]\n",
    "\n",
    "print(f\"Trainable params count: {len(trainable)}\")\n",
    "print(\"Sample trainable param:\", trainable[0] if trainable else \"âŒ NONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21f3ac-7001-4cd1-a62c-9cf7d6dfb6a0",
   "metadata": {},
   "source": [
    "(ML) ospost@A100:~/workspace/llama.cpp$ cp -fr build/bin/* ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0171dd2b-f884-4774-9754-91eb6b48cf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ospost/workspace/llama.cpp:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.path.abspath(\"../llama.cpp\") + \":\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "# Optional: verify\n",
    "print(os.environ[\"LD_LIBRARY_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e68033-0bd2-449a-820d-2c258a973a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return code: 1\n",
      "Stdout: usage: ../llama.cpp/llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights]\n",
      "       [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--tensor-type] [--prune-layers] [--keep-split] [--override-kv]\n",
      "       model-f32.gguf [model-quant.gguf] type [nthreads]\n",
      "\n",
      "  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n",
      "  --\n",
      "Stderr: \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "res = subprocess.run([\"../llama.cpp/llama-quantize\", \"--help\"], capture_output=True, text=True)\n",
    "print(\"Return code:\", res.returncode)\n",
    "print(\"Stdout:\", res.stdout[:500])\n",
    "print(\"Stderr:\", res.stderr[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdd46e3-7dd4-4a87-a545-2d64011bc3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizer path: ../llama.cpp/llama-quantize\n",
      "Converter path: ../llama.cpp/convert_hf_to_gguf.py\n"
     ]
    }
   ],
   "source": [
    "from unsloth_zoo.llama_cpp import check_llama_cpp\n",
    "\n",
    "quantizer, converter = check_llama_cpp(\"../llama.cpp\")\n",
    "print(\"Quantizer path:\", quantizer)\n",
    "print(\"Converter path:\", converter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e04f69-304b-4195-b602-d2370f5a95e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/ospost/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00000-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into mxfp4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/ospost/workspace/gpt-oss-20b-fine-tune/gpt-oss-20b-sft-mxfp4`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save_pretrained_merged(\"gpt-oss-20b-sft-mxfp4\", tokenizer, save_method=\"mxfp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994a41ee-1070-43c3-b992-afb121e62026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'merged_model': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -lah merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19e85b25-a180-4ed5-80f2-c65d8a345264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13G\n",
      "drwxrwxr-x 3 ospost ospost 4.0K Jan  9 15:14 .\n",
      "drwxrwxr-x 9 ospost ospost 4.0K Jan  9 15:14 ..\n",
      "drwxrwxr-x 3 ospost ospost 4.0K Jan  9 15:14 .cache\n",
      "-rw-rw-r-- 1 ospost ospost  15K Jan  9 15:14 chat_template.jinja\n",
      "-rw-rw-r-- 1 ospost ospost 1.8K Jan  9 15:14 config.json\n",
      "-rw-rw-r-- 1 ospost ospost 4.5G Jan  9 15:14 model-00000-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ospost ospost 4.5G Jan  9 15:14 model-00001-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ospost ospost 3.9G Jan  9 15:14 model-00002-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ospost ospost  36K Jan  9 15:14 model.safetensors.index.json\n",
      "-rw-rw-r-- 1 ospost ospost  446 Jan  9 15:14 special_tokens_map.json\n",
      "-rw-rw-r-- 1 ospost ospost  20K Jan  9 15:14 tokenizer_config.json\n",
      "-rw-rw-r-- 1 ospost ospost  27M Jan  9 15:14 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lah gpt-oss-20b-sft-mxfp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69820a6-323c-4888-b94a-51d511f1d13b",
   "metadata": {},
   "source": [
    "To save your fine-tuned model, you can export your fine-tuned model both in bf16 format , with our on-demand dequantization of MXFP4 base models using save_method=\"merged_16bit\"or in native MXFP4 Safetensors format using save_method=\"mxfp4\" .\n",
    "\n",
    "The MXFP4 native merge format offers significant performance improvements compared to the bf16 format: it uses up to 75% less disk space, reduces VRAM consumption by 50%, accelerates merging by 5-10x, and enables much faster conversion to GGUF format.\n",
    "\n",
    "Convert the MXFP4 merged model:\n",
    "python3 llama.cpp/convert_hf_to_gguf.py gpt-oss-finetuned-merged/ --outfile gpt-oss-finetuned-mxfp4.gguf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39ef130-9178-491f-abf5-ac55d359f900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENTS.md\t\t       llama-lookup-stats\n",
      "AUTHORS\t\t\t       llama-minicpmv-cli\n",
      "benches\t\t\t       llama-mtmd-cli\n",
      "build\t\t\t       llama-parallel\n",
      "build-xcframework.sh\t       llama-passkey\n",
      "ci\t\t\t       llama-perplexity\n",
      "CLAUDE.md\t\t       llama-q8dot\n",
      "cmake\t\t\t       llama-quantize\n",
      "CMakeLists.txt\t\t       llama-qwen2vl-cli\n",
      "CMakePresets.json\t       llama-retrieval\n",
      "CODEOWNERS\t\t       llama-save-load-state\n",
      "common\t\t\t       llama-server\n",
      "CONTRIBUTING.md\t\t       llama-simple\n",
      "convert_hf_to_gguf.py\t       llama-simple-chat\n",
      "convert_hf_to_gguf_update.py   llama-speculative\n",
      "convert_llama_ggml_to_gguf.py  llama-speculative-simple\n",
      "convert_lora_to_gguf.py        llama-tokenize\n",
      "docs\t\t\t       llama-tts\n",
      "examples\t\t       llama-vdot\n",
      "flake.lock\t\t       Makefile\n",
      "flake.nix\t\t       media\n",
      "ggml\t\t\t       models\n",
      "gguf-py\t\t\t       mypy.ini\n",
      "grammars\t\t       pocs\n",
      "include\t\t\t       poetry.lock\n",
      "libggml-base.so\t\t       pyproject.toml\n",
      "libggml-base.so.0\t       pyrightconfig.json\n",
      "libggml-base.so.0.9.5\t       quantize\n",
      "libggml-cpu.so\t\t       README.md\n",
      "libggml-cpu.so.0\t       requirements\n",
      "libggml-cpu.so.0.9.5\t       requirements.txt\n",
      "libggml-cuda.so\t\t       scripts\n",
      "libggml-cuda.so.0\t       SECURITY.md\n",
      "libggml-cuda.so.0.9.5\t       src\n",
      "libggml.so\t\t       test-alloc\n",
      "libggml.so.0\t\t       test-arg-parser\n",
      "libggml.so.0.9.5\t       test-autorelease\n",
      "libllama.so\t\t       test-backend-ops\n",
      "libllama.so.0\t\t       test-backend-sampler\n",
      "libllama.so.0.0.7684\t       test-barrier\n",
      "libmtmd.so\t\t       test-c\n",
      "libmtmd.so.0\t\t       test-chat\n",
      "libmtmd.so.0.0.7684\t       test-chat-parser\n",
      "LICENSE\t\t\t       test-chat-peg-parser\n",
      "licenses\t\t       test-chat-template\n",
      "llama-batched\t\t       test-gbnf-validator\n",
      "llama-batched-bench\t       test-gguf\n",
      "llama-bench\t\t       test-grammar-integration\n",
      "llama-cli\t\t       test-grammar-parser\n",
      "llama-completion\t       test-json-partial\n",
      "llama-convert-llama2c-to-ggml  test-json-schema-to-grammar\n",
      "llama-cvector-generator        test-llama-grammar\n",
      "llama-debug\t\t       test-log\n",
      "llama-diffusion-cli\t       test-model-load-cancel\n",
      "llama-embedding\t\t       test-mtmd-c-api\n",
      "llama-eval-callback\t       test-opt\n",
      "llama-export-lora\t       test-peg-parser\n",
      "llama-finetune\t\t       test-quantize-fns\n",
      "llama-fit-params\t       test-quantize-perf\n",
      "llama-gemma3-cli\t       test-quantize-stats\n",
      "llama-gen-docs\t\t       test-regex-partial\n",
      "llama-gguf\t\t       test-rope\n",
      "llama-gguf-hash\t\t       tests\n",
      "llama-gguf-split\t       test-sampling\n",
      "llama-idle\t\t       test-state-restore-fragmented\n",
      "llama-imatrix\t\t       test-thread-safety\n",
      "llama-llava-cli\t\t       test-tokenizer-0\n",
      "llama-lookahead\t\t       test-tokenizer-1-bpe\n",
      "llama-lookup\t\t       test-tokenizer-1-spm\n",
      "llama-lookup-create\t       tools\n",
      "llama-lookup-merge\t       vendor\n"
     ]
    }
   ],
   "source": [
    "!ls ../llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899fa61-17c7-431c-b524-9b0b77b32e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to mxfp4 format...\n",
      "Found HuggingFace hub cache directory: /home/ospost/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00000-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 38014.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into mxfp4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/ospost/workspace/gpt-oss-20b-fine-tune/gpt-oss-20b-sft-mxfp4`\n",
      "Unsloth: Converting to GGUF format...\n",
      "Unsloth: GPT-OSS model detected - using special conversion settings\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF  might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF  to ['None'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into  GGUF format.\n",
      "This might take 3 minutes...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.save_pretrained_gguf(\"gpt-oss-20b-sft-mxfp4\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec885344-f832-4eb0-b611-c033f9f1f173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
