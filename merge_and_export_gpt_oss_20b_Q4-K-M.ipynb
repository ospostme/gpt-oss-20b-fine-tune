{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed153c43-9c31-4051-95a1-8e1d2055f55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m11 packages\u001b[0m \u001b[2min 114ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m11 packages\u001b[0m \u001b[2min 0.28ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================================\n",
    "# 1ï¸âƒ£ Env Prepare Install Required Packages\n",
    "# ================================================\n",
    "\n",
    "#%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):    \n",
    "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
    "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo huggingface-hub==0.34.4 datasets==4.3.0 numpy==2.3.4 pandas==2.3.3 pyarrow==22.0.0 tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5da945-1efe-4050-8fbd-b35cab526c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.12.10: Fast Gpt_Oss patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.521 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49586a8c1a664cba8a015e0067fdf18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB 4-bit layers found: 1632\n",
      "Sample BNB layer: model.layers.0.self_attn.q_proj\n",
      "Model compute dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from collections import Counter\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/gpt-oss-20b\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "bnb_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, bnb.nn.Linear4bit):\n",
    "        bnb_layers.append(name)\n",
    "\n",
    "print(f\"BNB 4-bit layers found: {len(bnb_layers)}\")\n",
    "print(\"Sample BNB layer:\", bnb_layers[0] if bnb_layers else \"âŒ NONE\")\n",
    "print(\"Model compute dtype:\", model.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1667548e-9bbc-4947-8a27-9dbef6178f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    checkpoint_path=\"ospost/gpt-oss-20b-sft-adapter\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7462d3c3-3ac6-465a-be7c-4690d33829dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA parameters found: 192\n",
      "Sample LoRA param: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "LoRA dtypes: Counter({torch.float32: 192})\n",
      "Trainable params count: 192\n",
      "Sample trainable param: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n"
     ]
    }
   ],
   "source": [
    "assert any(\"lora_\" in n for n,_ in model.named_parameters())\n",
    "\n",
    "lora_params = [\n",
    "    (n, p) for n, p in model.named_parameters() if \"lora_\" in n\n",
    "]\n",
    "\n",
    "print(f\"LoRA parameters found: {len(lora_params)}\")\n",
    "print(\"Sample LoRA param:\", lora_params[0][0] if lora_params else \"âŒ NONE\")\n",
    "\n",
    "print(\"LoRA dtypes:\", Counter(p.dtype for _, p in lora_params))\n",
    "\n",
    "trainable = [\n",
    "    n for n, p in model.named_parameters() if p.requires_grad\n",
    "]\n",
    "\n",
    "print(f\"Trainable params count: {len(trainable)}\")\n",
    "print(\"Sample trainable param:\", trainable[0] if trainable else \"âŒ NONE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0171dd2b-f884-4774-9754-91eb6b48cf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/gpt-oss-20b-fine-tune/llama.cpp:/opt/amazon/efa/lib:/opt/aws-ofi-nccl/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LD_LIBRARY_PATH\"] = os.path.abspath(\"./llama.cpp\") + \":\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "# Optional: verify\n",
    "print(os.environ[\"LD_LIBRARY_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e68033-0bd2-449a-820d-2c258a973a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return code: 1\n",
      "Stdout: usage: ./llama.cpp/llama-quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] [--imatrix] [--include-weights]\n",
      "       [--exclude-weights] [--output-tensor-type] [--token-embedding-type] [--tensor-type] [--prune-layers] [--keep-split] [--override-kv]\n",
      "       model-f32.gguf [model-quant.gguf] type [nthreads]\n",
      "\n",
      "  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n",
      "  --l\n",
      "Stderr: \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "res = subprocess.run([\"./llama.cpp/llama-quantize\", \"--help\"], capture_output=True, text=True)\n",
    "print(\"Return code:\", res.returncode)\n",
    "print(\"Stdout:\", res.stdout[:500])\n",
    "print(\"Stderr:\", res.stderr[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cdd46e3-7dd4-4a87-a545-2d64011bc3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizer path: ./llama.cpp/llama-quantize\n",
      "Converter path: ./llama.cpp/convert_hf_to_gguf.py\n"
     ]
    }
   ],
   "source": [
    "from unsloth_zoo.llama_cpp import check_llama_cpp\n",
    "\n",
    "quantizer, converter = check_llama_cpp(\"./llama.cpp/\")\n",
    "print(\"Quantizer path:\", quantizer)\n",
    "print(\"Converter path:\", converter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca51dbb-5595-43f5-a77f-6a9e595ef7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to mxfp4 format...\n",
      "Found HuggingFace hub cache directory: /teamspace/studios/this_studio/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7919923df8f4eeebb3258f438c6ac49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00000-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cdb5fd67c8450f9ee013b5c840a789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00000-of-00002.safetensors:   0%|          | 0.00/4.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:10,  5.43s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237102fdd4954f0fbbd60ce4f8f7e57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:11<00:06,  6.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8f6c1573a549d4993fef919c8bb599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into mxfp4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:24<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/teamspace/studios/this_studio/gpt-oss-20b-fine-tune/gpt-oss-20b-sft-q4_k_m`\n",
      "Unsloth: Converting to GGUF format...\n",
      "Unsloth: GPT-OSS model detected - using special conversion settings\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF  might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF  to ['None'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into  GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['gpt-oss-20b.MXFP4.gguf']\n",
      "Unsloth: GPT-OSS model - skipping additional quantizations\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['gpt-oss-20b.MXFP4.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model gpt-oss-20b.MXFP4.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'save_directory': 'gpt-oss-20b-sft-q4_k_m',\n",
       " 'gguf_files': ['gpt-oss-20b.MXFP4.gguf'],\n",
       " 'modelfile_location': '/teamspace/studios/this_studio/gpt-oss-20b-fine-tune/Modelfile',\n",
       " 'want_full_precision': True,\n",
       " 'is_vlm': False,\n",
       " 'fix_bos_token': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\n",
    "    \"gpt-oss-20b-sft-q4_k_m\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6cc08-a059-4f08-b7f1-b002aac178b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah gpt-oss-20b-sft-q4_k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e85b25-a180-4ed5-80f2-c65d8a345264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
